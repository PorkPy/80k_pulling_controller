{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Trainer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PorkPy/80k_pulling_controller/blob/master/Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtEIpTbrfxxB",
        "colab_type": "code",
        "outputId": "3615be07-2f50-40d1-90eb-d5b7354f319f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mzCKBtLA-zOi",
        "colab_type": "code",
        "outputId": "6eaa2136-2d0f-4364-8cd6-32c04a2f2bff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch import nn, optim\n",
        "import random\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from scipy.stats import norm\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "## Set random seed for numpy and Torch\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fd5e6d94530>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xNzK2S2_mdE",
        "colab_type": "code",
        "outputId": "41b909c5-7ad8-4a17-f59c-7e010b1dd73d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jun 12 18:51:13 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    36W / 250W |    731MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5WXvVoh-zOu",
        "colab_type": "text"
      },
      "source": [
        "Check CUDA is available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJf9cG7o-zOw",
        "colab_type": "code",
        "outputId": "5c434845-ca65-4afa-99b3-a1f1ee804851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35dozZZC3Nut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_number():\n",
        "    model_num = '8' # model number to save new models with\n",
        "    return model_num\n",
        "def load_params():\n",
        "    params = '4_v100' # which model params to load.\n",
        "    return params\n",
        "\n",
        "def model_directory():\n",
        "    model_dir = 'model8' # directory for specific model being trained. \n",
        "    return model_dir\n",
        "#name = 'model7_v0' # filename to save test results to."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "O7ZKYszy-zQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def tests(model_name):\n",
        "    \n",
        "    model_name = model_name\n",
        "    model_dir = model_directory()\n",
        "    stats_list = []\n",
        "    pdf = PdfPages(f\"/content/drive/My Drive/PhD/PhD/lstm/{model_dir}/{model_name}/testing_traj_pics_{model_name}.pdf\")\n",
        "    fig = plt.figure()\n",
        "\n",
        "    for traj in range(len(test_batches)):\n",
        "        Xtest, ytest, jim, scaler = get_test_batch(traj)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            model.reset_hidden_state()\n",
        "            test_seq = Xtest[0].reshape(-1,500,4)#.reshape(1,200,4) # input first sequence from trajectory/batch\n",
        "            preds = [] # create a list to store predictions.\n",
        "            for i in range(len(Xtest)-1): # for each sequence i in the trajectory,\n",
        "                y_test_pred = model(test_seq).to(device) # send sequence to model,\n",
        "                pred = torch.flatten(y_test_pred).item() # reshape the model output,\n",
        "                preds.append(pred) # and append to the list of predictions - preds.\n",
        "                new_seq = Xtest[i+1].reshape(-1,500,4)#.reshape(1,200,4) # Change sequence to the next one in the list.\n",
        "                test_seq = torch.cuda.FloatTensor(new_seq).view(1, seq_length, -1) # change sequence to a torch Tensor\n",
        "\n",
        "\n",
        "        preds2 = np.asarray(preds)\n",
        "        preds = scaler.inverse_transform(preds2).reshape(-1,1)\n",
        "\n",
        "        # Mean Absolute Error\n",
        "        MAE_list = []\n",
        "        for i,j in zip(preds, ytest):\n",
        "            error = np.abs(i-j)\n",
        "            MAE_list.append(error)\n",
        "        MAE = float(\"{:.3f}\".format(np.mean(MAE_list)))\n",
        "        #print(\"MAE\",\"{:.3f}\".format(MAE),'N')\n",
        "\n",
        "        # Coefficient of Variance\n",
        "        mean = np.mean(data.iloc[:,-1]) # mean of all dependent variables.\n",
        "        cov_list = []\n",
        "        for i,j in zip(preds, ytest):\n",
        "            sq_dev = (i-j)**2\n",
        "            cov_list.append(sq_dev)    \n",
        "        MSD = np.mean(cov_list) # mean square deviation\n",
        "        RMSD = np.sqrt(MSD) # root mean square deviation\n",
        "        cov = RMSD/mean # coefficient of variance\n",
        "        RMSD = float(\"{:.3f}\".format(RMSD))\n",
        "        cov =  float(\"{:.3f}\".format(cov))\n",
        "        #print(\"COV:\",\"{:.3f}\".format(cov))\n",
        "        \n",
        "    \n",
        "        my_dict = {'Trajectory':traj,\n",
        "                'MAE': MAE, \n",
        "                'RMSD':RMSD,\n",
        "                'cov': cov, # Used to normalise the RMSD accross all the data\n",
        "        }\n",
        "        stats_list.append(my_dict)\n",
        "\n",
        "\n",
        "        # Plot forces\n",
        "        predicted_cases = preds\n",
        "        true_cases = ytest\n",
        "        # Add title and axis names\n",
        "        plt.title(f'Force Trajectory {traj}')\n",
        "        plt.xlabel('Sample num')\n",
        "        plt.ylabel('Force (N)')\n",
        "        #plt.plot(jim,label='Sequence')\n",
        "        plt.plot(true_cases[:,-1], label='Real Force')\n",
        "        plt.plot(predicted_cases[:,-1], label='Predicted Force')\n",
        "        plt.legend();\n",
        "        # save the current figure\n",
        "        pdf.savefig(fig);\n",
        "        # destroy the current figure\n",
        "        plt.clf()\n",
        "\n",
        "    # close the object\n",
        "    # fig = plt.figure()\n",
        "    # plt.plot(train_hist, label=\"Training loss\")\n",
        "    # plt.plot(test_hist, label=\"Test loss\")\n",
        "    # plt.legend();\n",
        "    # pdf.savefig(fig)\n",
        "    # plt.clf\n",
        "    pdf.close()\n",
        "    stats_list = pd.DataFrame(stats_list)\n",
        "    return stats_list\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def stats(stats_list2, model_name):\n",
        "    \n",
        "    #display(stats_list2)\n",
        "    #display(stats_list2['MAE'])\n",
        "\n",
        "    mean_list = {\n",
        "                'MAE' :float(\"{:.3f}\".format(np.mean(stats_list2['MAE']))),\n",
        "                'RMSD':float(\"{:.3f}\".format(np.mean(stats_list2['RMSD']))),\n",
        "                'cov' :float(\"{:.3f}\".format(np.mean(stats_list2['cov'])))\n",
        "    }\n",
        "\n",
        "    std_dev = {\n",
        "                'MAE' :float(\"{:.3f}\".format(np.std(stats_list2['MAE']))),\n",
        "                'RMSD':float(\"{:.3f}\".format(np.std(stats_list2['RMSD']))),\n",
        "                'cov' :float(\"{:.3f}\".format(np.std(stats_list2['cov'])))\n",
        "    }\n",
        "\n",
        "    max_list = {\n",
        "                'MAE' :float(stats_list2['MAE'].max()),\n",
        "                'RMSD':float(stats_list2['RMSD'].max()),\n",
        "                'cov' :float(stats_list2['cov'].max())\n",
        "    }\n",
        "\n",
        "    stats_list2 = stats_list2.append(mean_list, ignore_index=True).fillna('Grand Mean')\n",
        "    stats_list2 = stats_list2.append(std_dev, ignore_index=True).fillna('Standard Dev')\n",
        "    stats_list2 = stats_list2.append(max_list, ignore_index=True).fillna('Max Value')\n",
        "\n",
        "    #display(stats_list2)\n",
        "    model_name = model_name\n",
        "    model_dir = model_directory()\n",
        "\n",
        "    stats_list2.to_csv(f\"/content/drive/My Drive/PhD/PhD/lstm/{model_dir}/{model_name}/lstm_model_metrics_{model_name}.csv\", index=False)\n",
        "    return stats_list2\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def gauss_plot(stats_list2, name, error_type, num):\n",
        "    import matplotlib.pyplot as plt\n",
        "    model_name = name\n",
        "    model_dir = model_directory()\n",
        "\n",
        "    error = error_type\n",
        "    pdf = PdfPages(f\"/content/drive/My Drive/PhD/PhD/lstm/{model_dir}/{model_name}/gauss_pic_{error}.pdf\")\n",
        "    fig = plt.figure()\n",
        "    \n",
        "    # define constants\n",
        "    mu = np.mean(stats_list2.iloc[:-3,num]) \n",
        "    sigma = np.sqrt(np.var(stats_list2.iloc[:-3,num]))\n",
        "    x1 = np.min(stats_list2.iloc[:-3,num])\n",
        "    x2 = np.max(stats_list2.iloc[:-3,num])\n",
        "    # print(mu)\n",
        "    # print(sigma)\n",
        "    # print(x1)\n",
        "    # print(x2)\n",
        "\n",
        "    # calculate the z-transform\n",
        "    z1 = ( x1 - mu ) / sigma\n",
        "    z2 = ( x2 - mu ) / sigma\n",
        "\n",
        "    x = np.arange(z1, z2, 0.001) # range of x in spec\n",
        "    x_all = np.arange(-10, 10, 0.001) # entire range of x, both in and out of spec\n",
        "    # mean = 0, stddev = 1, since Z-transform was calculated\n",
        "    y = norm.pdf(x,0,1)\n",
        "    y2 = norm.pdf(x_all,0,1)\n",
        "\n",
        "    # build the plot\n",
        "    fig, ax = plt.subplots(figsize=(9,6))\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    ax.plot(x_all,y2)\n",
        "\n",
        "    ax.fill_between(x,y,0, alpha=0.3, color='b')\n",
        "    ax.fill_between(x_all,y2,0, alpha=0.1)\n",
        "    ax.set_xlim([-4,4])\n",
        "    ax.set_xlabel('# of Standard Deviations Outside the Mean')\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_title(f'{model_name} {error} Std Dev')\n",
        "\n",
        "    plt.savefig('normal_curve.png', dpi=72, bbox_inches='tight');\n",
        "    plt.grid(True);\n",
        "    #plt.show()\n",
        "\n",
        "    # save the current figure\n",
        "    pdf.savefig(fig);\n",
        "    # destroy the current figure\n",
        "    plt.clf()\n",
        "\n",
        "    # close the object\n",
        "    pdf.close()\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def prob_dist(stats_list2, name, error_type, num):    \n",
        "    model_name = name\n",
        "    model_dir = model_directory()\n",
        "\n",
        "    error = error_type\n",
        "    pdf = PdfPages(f\"/content/drive/My Drive/PhD/PhD/lstm/{model_dir}/{model_name}/prob_dist_pic_{error}.pdf\")\n",
        "    fig = plt.figure()\n",
        "\n",
        "    import seaborn as sns\n",
        "    sns.distplot(stats_list2.iloc[:-3,num], color=\"darkslategrey\");\n",
        "    plt.xlabel(\"Force [newtons]\", labelpad=14);\n",
        "    plt.ylabel(\"Probability of Occurence\", labelpad=14);\n",
        "    plt.title(f\"Probability Distribution of {error}\", fontsize=20);\n",
        "    #plt.show()\n",
        "    # save the current figure\n",
        "    pdf.savefig(fig);\n",
        "    # destroy the current figure\n",
        "    plt.clf()\n",
        "\n",
        "    # close the object\n",
        "    pdf.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2BXoG-jlHcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_runner(name):   \n",
        "    stats_df = tests(name) # Run tests on testing data and save generated plots to Google Drive\n",
        "    stats(stats_df, name) # Record stats and save to Google Drive\n",
        "    for i in range(1,4): # 1 to 3 = the colunms in the stats_list DataFrame\n",
        "        if i ==1:\n",
        "            error_type = 'MAE' # mean absolur error\n",
        "        elif i == 2:\n",
        "            error_type = 'RMSE' # root mean squared error\n",
        "        elif i == 3:\n",
        "            error_type = 'cov' # coefficient of variance\n",
        "\n",
        "        prob_dist(stats_df, name, error_type, i) # Gen prob_dist and save to GD\n",
        "        \n",
        "        gauss_plot(stats_df, name, error_type, i) # Gen Gauss plots and save to GD\n",
        "    print(\"Done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSq64dXK-zO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ForcePredictor(nn.Module):\n",
        "\n",
        "    def __init__(self, n_features, n_hidden, seq_len, n_layers=2, ignore_zero=True):\n",
        "        super(ForcePredictor, self).__init__()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device(\"cuda:0\")\n",
        "            print(\"Running on the GPU\")\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            print(\"Running on CPU\")\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.seq_len = seq_len\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "          input_size=n_features,\n",
        "          hidden_size=n_hidden,\n",
        "          num_layers=n_layers,\n",
        "          dropout=0.5)\n",
        "\n",
        "\n",
        "        self.linear1 = nn.Linear(in_features=n_hidden, out_features=256)\n",
        "        self.linear2 = nn.Linear(in_features=256, out_features=1)\n",
        "        self.linear3 = nn.Linear(in_features=512, out_features=1024)\n",
        "        self.linear4 = nn.Linear(in_features=1024, out_features=1)\n",
        "        #self.linear5 = nn.Linear(in_features=5000, out_features=1)\n",
        "        #self.linear6 = nn.Linear(in_features=10000, out_features=1)\n",
        "\n",
        "    def reset_hidden_state(self):\n",
        "        self.hidden = (\n",
        "            torch.zeros(self.n_layers, self.seq_len, self.n_hidden).to(device),\n",
        "            torch.zeros(self.n_layers, self.seq_len, self.n_hidden).to(device)\n",
        "        )\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        lstm_out, self.hidden = self.lstm(sequences.view(len(sequences), self.seq_len, -1),self.hidden)\n",
        "        last_time_step = lstm_out.view(self.seq_len, len(sequences), self.n_hidden)[-1]\n",
        "        y_pred = self.linear1(last_time_step)\n",
        "        #y_pred = F.leaky_relu(self.linear2(y_pred))\n",
        "        #y_pred = F.leaky_relu(self.linear3(y_pred))\n",
        "        #y_pred = self.linear4(y_pred)\n",
        "        #y_pred = self.linear5(y_pred)\n",
        "        #y_pred = self.linear6(y_pred)\n",
        "\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThRgX0Yo-zPC",
        "colab_type": "code",
        "outputId": "51b50549-7147-4c3a-8a28-5e0e5c4c21b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on CPU\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "894bcCIs-zPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model):\n",
        "   \n",
        "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)#0.0007  \n",
        "    num_epochs = 600 #1600 #600\n",
        "    train_hist = np.zeros(num_epochs)\n",
        "    test_hist = np.zeros(num_epochs)\n",
        "              \n",
        "    # params = load_params() # model num and version num: 4_v100.\n",
        "    # PATH = f\"/content/drive/My Drive/PhD/PhD/lstm/model_params{params}.pt\"     \n",
        "    # checkpoint = torch.load(PATH)\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    # for state in optimizer.state.values():\n",
        "    #     for k, v in state.items():\n",
        "    #         if isinstance(v, torch.Tensor):\n",
        "    #             state[k] = v.cuda()\n",
        "    # num_epoch = checkpoint['epoch']\n",
        "    # loss = checkpoint['loss']\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    \n",
        "\n",
        "    for t in range(num_epochs):\n",
        "        for j in range(len(batches)):\n",
        "            \n",
        "            train_data, train_labels, test_data, test_labels = get_batches(j)\n",
        "            #if t % 100 == 0:#100 is good so far\n",
        "            model.reset_hidden_state() # Requiered because he hiddent state was forgetting too early. \n",
        "            y_pred = model(train_data)\n",
        "            loss = loss_fn(y_pred.float(), train_labels)\n",
        "\n",
        "            if test_data is not None:\n",
        "                with torch.no_grad():\n",
        "                    y_test_pred = model(test_data)\n",
        "                    test_loss = loss_fn(y_test_pred.float(), test_labels)\n",
        "                test_hist[t] = test_loss.item()\n",
        "\n",
        "                if t % 10 == 0:  \n",
        "                    print(f'Epoch {t} train loss: {loss.item()} test loss: {test_loss.item()}')\n",
        "            elif t % 10 == 0:\n",
        "                  print(f'Epoch {t} train loss: {loss.item()}')\n",
        "\n",
        "            train_hist[t] = loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            model = model.to(device)\n",
        "            optimizer.step()\n",
        "            #print(model.linear4.weight.data) # Check weights are being updated.\n",
        "\n",
        "        ## Periodically save model and show training and testing loss\n",
        "        if t % 100 == 0:\n",
        "            print('Saving model', '\\n')\n",
        "            model_num = model_number()\n",
        "            model_save_name = f'model_params{model_num}_v{t}.pt'\n",
        "            torch.save({\n",
        "                'epoch': num_epochs,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss':loss,},\n",
        "                f\"/content/drive/My Drive/PhD/PhD/lstm/{model_save_name}\" \n",
        "            )\n",
        "            name = f'model{model_num}_v{t}'\n",
        "            test_runner(name)\n",
        "            model.train()\n",
        "            \n",
        "\n",
        "    return model.eval(), train_hist, test_hist, optimizer, t, loss_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOH4bUQb-zPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"Saving Model\")\n",
        "# torch.save({\n",
        "# 'epoch': num_epochs,\n",
        "# 'model_state_dict': model.state_dict(),\n",
        "# 'optimizer_state_dict': optimizer.state_dict(),\n",
        "# 'loss': loss,\n",
        "# },\"/home/ur10pc/Desktop/robot_data2/80k_data/LSTM_params/lstm_params.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtXzj7tF-zPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/PorkPy/LSTM-Force-Predictor/master/80k_data/mean_force_data.csv'\n",
        "\n",
        "data = pd.read_csv(url)\n",
        "main_seq = data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "IGY4CFZ9-zP4",
        "colab_type": "code",
        "outputId": "9d0993b7-bb55-4713-b6b2-20ee38c40484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "n=1000\n",
        "batchesx = [data[i:i + n] for i in range(0, len(data), n)]\n",
        "print(len(batchesx))\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(batchesx)\n",
        "\n",
        "########################\n",
        "# high_force_list = []\n",
        "# sub_list0 = [4,6,7,13,17,20,8,14,23,27,37,42,59,60,21,29,40,49,57,62,66,70,72,73,74]\n",
        "# sub_list = [4,6,7,13,17,20,21,29,40,49,57,62,66,70,72,73,74] \n",
        "# for i in sub_list:\n",
        "#     high_force_list.append(batchesx[i])\n",
        "#################################################\n",
        "clean_list = [2,4,5,6,8,9,13,14,15,16,17,18,19,20,22,24,26,28,30,\n",
        "              35,36,37,38,39,40,43,44,45,48,49,55,56,59,60,63,\n",
        "              65,66,67,68,73,74,78] # all traj - traj that start off high.\n",
        "\n",
        "#high_force_a = [4,6,13,17,20,40,49,66,73,74,76] # large forces only\n",
        "high_force_b = [4,6,13,40,49,73,74,76,8,37,59,60] # large and medium forces\n",
        "test_list = [67,17,20,66,76,1,41,10,64,33]\n",
        "#random.shuffle(high_force_list)\n",
        "#print(\"len clean list\",len(clean_list))\n",
        "#print(\"high force b\",len(high_force_b))\n",
        "#print(len(clean_list)-len(high_force_b))\n",
        "\n",
        "clean_data = [] # create empty list\n",
        "for i in clean_list: \n",
        "    clean_data.append(batchesx[i]) # take the good traj from batchesx and populate clean data.\n",
        "for i in high_force_b:\n",
        "    clean_data.append(batchesx[i]) # append the high force traj from clean_list again to even data.\n",
        "test_data = []\n",
        "for i in test_list:\n",
        "    test_data.append(batchesx[i])\n",
        "\n",
        "## clean_data len = 64\n",
        "batches = clean_data[:50] #high_force_list[:20]\n",
        "val_batches = clean_data[:50] #high_force_list[:20] \n",
        "test_batches = test_data #high_force_list#[7:14]\n",
        "print(len(batches), len(val_batches), len(test_batches))\n",
        "print(type(batches[48]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79\n",
            "50 50 10\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WabAxpcY-zP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_batch(batch_number):\n",
        "    \n",
        "    seq_size = 500\n",
        "\n",
        "    X_test = []\n",
        "\n",
        "    data = test_batches[batch_number].reset_index(drop=True)\n",
        "    data= data[:seq_size]\n",
        "\n",
        "    features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "    features = np.asarray(features)\n",
        "\n",
        "    targets = data.iloc[:,-1]\n",
        "    targets = np.asarray(targets)\n",
        "    targets = targets.reshape(-1,1)\n",
        "    \n",
        "    for i in range(len(features)):           \n",
        "   \n",
        "        X =(features[:i+1])\n",
        "        an_array = np.array(X)\n",
        "        shape = np.shape(X)\n",
        "        temp = np.zeros((seq_size, 4))\n",
        "        temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "        X_test.append(temp)\n",
        "        y_test = targets\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    force_scaler = np.asarray(y_test)\n",
        "    force_scaler = force_scaler.reshape(-1,1)\n",
        "    scaler = scaler.fit(force_scaler)    \n",
        "    #y_test = scaler.transform(y_test)\n",
        "   \n",
        "    \n",
        "    X_test = torch.cuda.FloatTensor(X_test)\n",
        "    #y_test = torch.cuda.FloatTensor(y_test)\n",
        "    #print(data, '\\n')\n",
        "    #print( X_test.shape, y_test.shape)\n",
        "    jim = targets\n",
        "    return(X_test, y_test, jim, scaler)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nehzUS3F-zQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(batch_num):  \n",
        "    \n",
        "    seq_size = 500 # 1000 = full trajectories\n",
        "\n",
        "    # random.seed(batch_num)\n",
        "    # random.shuffle(batches) # ive turned this off to test new cleaned data\n",
        "\n",
        "    # Randomise the fetching of new data to break the corrolation of training.\n",
        "    #print(batch_num)\n",
        "    #print(type(batches[batch_num])) \n",
        "    data = batches[batch_num].reset_index(drop=True)\n",
        "    data= data[:seq_size]\n",
        "    ################################################\n",
        "\n",
        "    X_train = []\n",
        "    X_test = []\n",
        "    \n",
        "    features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "    features = np.asarray(features)\n",
        "\n",
        "    targets = data.iloc[:,-1]\n",
        "    targets = np.asarray(targets)\n",
        "    targets = targets.reshape(-1,1)\n",
        "\n",
        "    for i in range(len(features)):           \n",
        "        \n",
        "        np.random.seed(42)\n",
        "       \n",
        "        X =(features[:i+1])\n",
        "        an_array = np.array(X)\n",
        "        shape = np.shape(X)\n",
        "        temp = np.zeros((seq_size, 4))\n",
        "        temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "        X_train.append(temp)\n",
        "        y_train = targets\n",
        "    \n",
        "    ###############################\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    force_scaler = np.asarray(y_train)\n",
        "    force_scaler = force_scaler.reshape(-1,1)\n",
        "    scaler = scaler.fit(force_scaler)    \n",
        "    y_train = scaler.transform(y_train)\n",
        "    \n",
        "    ################################\n",
        "    data = val_batches[batch_num].reset_index(drop=True)\n",
        "    data= data[:seq_size]\n",
        "\n",
        "    features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "    features = np.asarray(features)\n",
        "\n",
        "    targets = data.iloc[:,-1]\n",
        "    targets = np.asarray(targets)\n",
        "    targets = targets.reshape(-1,1)\n",
        "    \n",
        "    for i in range(len(features)):           \n",
        "   \n",
        "        X =(features[:i+1])\n",
        "        an_array = np.array(X)\n",
        "        shape = np.shape(X)\n",
        "        temp = np.zeros((seq_size, 4))\n",
        "        temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "        X_test.append(temp)\n",
        "        y_test = targets\n",
        "\n",
        " #############################################\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    force_scaler = np.asarray(y_test)\n",
        "    force_scaler = force_scaler.reshape(-1,1)\n",
        "    scaler = scaler.fit(force_scaler)    \n",
        "    y_test = scaler.transform(y_test)\n",
        "\n",
        "###############################################\n",
        "\n",
        "    X_train = torch.cuda.FloatTensor(X_train) # Change data to tensors\n",
        "    y_train = torch.cuda.FloatTensor(y_train)\n",
        "    X_test = torch.cuda.FloatTensor(X_test)\n",
        "    y_test = torch.cuda.FloatTensor(y_test)\n",
        "    \n",
        "    #del targets, features, data, temp, an_array\n",
        "    \n",
        "    #print(data)\n",
        "    #print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "    return(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thk1hwULis6G",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cv2RZ52m-zQE",
        "colab_type": "code",
        "outputId": "7970c408-0048-402e-c8ed-2390aaf48260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "seq_length = 500 # when using zero padding, this seq_length is a bit redundent but still has to match the zero's size.\n",
        "\n",
        "model = ForcePredictor(\n",
        "      n_features=4, \n",
        "      n_hidden=64, \n",
        "      seq_len=seq_length, \n",
        "      n_layers=2\n",
        "    )\n",
        "\n",
        "model, train_hist, test_hist, optimizer, epochs, loss = train_model(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "Epoch 0 train loss: 0.99146568775177 test loss: 0.9915937185287476\n",
            "Epoch 0 train loss: 1.0040357112884521 test loss: 1.0061832666397095\n",
            "Epoch 0 train loss: 0.9921894073486328 test loss: 0.9899007081985474\n",
            "Epoch 0 train loss: 1.0050081014633179 test loss: 1.0002068281173706\n",
            "Epoch 0 train loss: 0.9935280084609985 test loss: 0.9937669038772583\n",
            "Epoch 0 train loss: 0.9910799264907837 test loss: 0.9907001256942749\n",
            "Epoch 0 train loss: 1.0007989406585693 test loss: 1.001639485359192\n",
            "Epoch 0 train loss: 1.0159658193588257 test loss: 1.0156574249267578\n",
            "Epoch 0 train loss: 0.9890984892845154 test loss: 0.9877922534942627\n",
            "Epoch 0 train loss: 1.0123459100723267 test loss: 1.0126564502716064\n",
            "Epoch 0 train loss: 0.9991532564163208 test loss: 1.0006678104400635\n",
            "Epoch 0 train loss: 0.9728888273239136 test loss: 0.973740816116333\n",
            "Epoch 0 train loss: 0.9819363355636597 test loss: 0.983296275138855\n",
            "Epoch 0 train loss: 1.0035016536712646 test loss: 1.0032906532287598\n",
            "Epoch 0 train loss: 0.9641688466072083 test loss: 0.9651229381561279\n",
            "Epoch 0 train loss: 0.9593584537506104 test loss: 0.959987223148346\n",
            "Epoch 0 train loss: 1.0240647792816162 test loss: 1.0246491432189941\n",
            "Epoch 0 train loss: 0.9908165335655212 test loss: 0.9907207489013672\n",
            "Epoch 0 train loss: 0.9985486268997192 test loss: 0.9974972009658813\n",
            "Epoch 0 train loss: 1.0274875164031982 test loss: 1.0262279510498047\n",
            "Epoch 0 train loss: 0.956505298614502 test loss: 0.9593139886856079\n",
            "Epoch 0 train loss: 0.9962294697761536 test loss: 0.9981019496917725\n",
            "Epoch 0 train loss: 0.9882556200027466 test loss: 0.9871701002120972\n",
            "Epoch 0 train loss: 0.9361748695373535 test loss: 0.9361248016357422\n",
            "Epoch 0 train loss: 1.0097923278808594 test loss: 1.0105255842208862\n",
            "Epoch 0 train loss: 0.9804014563560486 test loss: 0.9800260663032532\n",
            "Epoch 0 train loss: 0.9291421175003052 test loss: 0.9278785586357117\n",
            "Epoch 0 train loss: 0.9870291352272034 test loss: 0.9852839708328247\n",
            "Epoch 0 train loss: 1.0365968942642212 test loss: 1.0381985902786255\n",
            "Epoch 0 train loss: 1.0356723070144653 test loss: 1.0361018180847168\n",
            "Epoch 0 train loss: 0.8930490016937256 test loss: 0.8944994211196899\n",
            "Epoch 0 train loss: 0.9919250011444092 test loss: 0.9913140535354614\n",
            "Epoch 0 train loss: 0.9645859003067017 test loss: 0.9619889259338379\n",
            "Epoch 0 train loss: 1.0364018678665161 test loss: 1.0395886898040771\n",
            "Epoch 0 train loss: 0.9176467061042786 test loss: 0.916978120803833\n",
            "Epoch 0 train loss: 1.0753463506698608 test loss: 1.0774484872817993\n",
            "Epoch 0 train loss: 0.9997018575668335 test loss: 0.9970398545265198\n",
            "Epoch 0 train loss: 0.9975079894065857 test loss: 0.9949907660484314\n",
            "Epoch 0 train loss: 0.8639413118362427 test loss: 0.8609499335289001\n",
            "Epoch 0 train loss: 1.0495070219039917 test loss: 1.051725149154663\n",
            "Epoch 0 train loss: 0.9873689413070679 test loss: 0.9874905347824097\n",
            "Epoch 0 train loss: 0.8932487964630127 test loss: 0.894477128982544\n",
            "Epoch 0 train loss: 0.9706205129623413 test loss: 0.9730398654937744\n",
            "Epoch 0 train loss: 0.9906361699104309 test loss: 0.9940146207809448\n",
            "Epoch 0 train loss: 0.997087299823761 test loss: 0.9956189393997192\n",
            "Epoch 0 train loss: 1.0114517211914062 test loss: 1.013992190361023\n",
            "Epoch 0 train loss: 1.0555570125579834 test loss: 1.0540072917938232\n",
            "Epoch 0 train loss: 1.0460606813430786 test loss: 1.0474334955215454\n",
            "Epoch 0 train loss: 0.9734447598457336 test loss: 0.9747585654258728\n",
            "Epoch 0 train loss: 0.9922460317611694 test loss: 0.9940816760063171\n",
            "Saving model \n",
            "\n",
            "Done\n",
            "Epoch 10 train loss: 1.3852037191390991 test loss: 1.3844317197799683\n",
            "Epoch 10 train loss: 0.4262978732585907 test loss: 0.4238065481185913\n",
            "Epoch 10 train loss: 1.2062817811965942 test loss: 1.2028061151504517\n",
            "Epoch 10 train loss: 0.9713297486305237 test loss: 0.9752943515777588\n",
            "Epoch 10 train loss: 0.7554401159286499 test loss: 0.757821261882782\n",
            "Epoch 10 train loss: 0.6817820072174072 test loss: 0.689596951007843\n",
            "Epoch 10 train loss: 0.9507610201835632 test loss: 0.9499160647392273\n",
            "Epoch 10 train loss: 0.8242579698562622 test loss: 0.8207671046257019\n",
            "Epoch 10 train loss: 0.9443617463111877 test loss: 0.9474600553512573\n",
            "Epoch 10 train loss: 0.9996851086616516 test loss: 1.0023040771484375\n",
            "Epoch 10 train loss: 0.9400588274002075 test loss: 0.9396567344665527\n",
            "Epoch 10 train loss: 0.619202733039856 test loss: 0.6175546646118164\n",
            "Epoch 10 train loss: 0.5416125655174255 test loss: 0.5412633419036865\n",
            "Epoch 10 train loss: 1.4652073383331299 test loss: 1.4515703916549683\n",
            "Epoch 10 train loss: 1.0323314666748047 test loss: 1.0350016355514526\n",
            "Epoch 10 train loss: 0.5109714269638062 test loss: 0.5072541832923889\n",
            "Epoch 10 train loss: 1.213300108909607 test loss: 1.2017183303833008\n",
            "Epoch 10 train loss: 0.7658200860023499 test loss: 0.7684330344200134\n",
            "Epoch 10 train loss: 0.8414152264595032 test loss: 0.8420460820198059\n",
            "Epoch 10 train loss: 1.4461930990219116 test loss: 1.4466999769210815\n",
            "Epoch 10 train loss: 0.6450915932655334 test loss: 0.6407937407493591\n",
            "Epoch 10 train loss: 0.9915599822998047 test loss: 0.992712676525116\n",
            "Epoch 10 train loss: 1.1144846677780151 test loss: 1.110475778579712\n",
            "Epoch 10 train loss: 0.7462630867958069 test loss: 0.7418027520179749\n",
            "Epoch 10 train loss: 0.9642786383628845 test loss: 0.9724918007850647\n",
            "Epoch 10 train loss: 0.5131065249443054 test loss: 0.5218715071678162\n",
            "Epoch 10 train loss: 0.9712304472923279 test loss: 0.9689226746559143\n",
            "Epoch 10 train loss: 0.8551788926124573 test loss: 0.8566519021987915\n",
            "Epoch 10 train loss: 1.096435785293579 test loss: 1.0984485149383545\n",
            "Epoch 10 train loss: 0.8457148671150208 test loss: 0.8369550108909607\n",
            "Epoch 10 train loss: 0.5074090361595154 test loss: 0.5090510845184326\n",
            "Epoch 10 train loss: 0.8786328434944153 test loss: 0.8723755478858948\n",
            "Epoch 10 train loss: 0.7998358607292175 test loss: 0.7969751358032227\n",
            "Epoch 10 train loss: 1.4432923793792725 test loss: 1.4438366889953613\n",
            "Epoch 10 train loss: 0.8271609544754028 test loss: 0.821168839931488\n",
            "Epoch 10 train loss: 1.2047795057296753 test loss: 1.1951411962509155\n",
            "Epoch 10 train loss: 1.225938081741333 test loss: 1.2294683456420898\n",
            "Epoch 10 train loss: 1.210382103919983 test loss: 1.2080328464508057\n",
            "Epoch 10 train loss: 0.5871713161468506 test loss: 0.580821692943573\n",
            "Epoch 10 train loss: 1.2976956367492676 test loss: 1.3005579710006714\n",
            "Epoch 10 train loss: 0.8900123834609985 test loss: 0.8946094512939453\n",
            "Epoch 10 train loss: 0.5689272880554199 test loss: 0.5682814121246338\n",
            "Epoch 10 train loss: 0.9431537985801697 test loss: 0.9403991103172302\n",
            "Epoch 10 train loss: 0.995211124420166 test loss: 0.994412362575531\n",
            "Epoch 10 train loss: 0.9201395511627197 test loss: 0.9204334616661072\n",
            "Epoch 10 train loss: 0.9368801712989807 test loss: 0.933594822883606\n",
            "Epoch 10 train loss: 1.1884604692459106 test loss: 1.1960211992263794\n",
            "Epoch 10 train loss: 1.094997525215149 test loss: 1.0944490432739258\n",
            "Epoch 10 train loss: 0.7816892862319946 test loss: 0.7835519313812256\n",
            "Epoch 10 train loss: 1.0421918630599976 test loss: 1.0442310571670532\n",
            "Epoch 20 train loss: 1.101935863494873 test loss: 1.099695086479187\n",
            "Epoch 20 train loss: 0.7945038080215454 test loss: 0.7991696000099182\n",
            "Epoch 20 train loss: 1.0948421955108643 test loss: 1.090589165687561\n",
            "Epoch 20 train loss: 0.969624936580658 test loss: 0.9666599035263062\n",
            "Epoch 20 train loss: 0.9224370121955872 test loss: 0.9199320077896118\n",
            "Epoch 20 train loss: 0.8800567388534546 test loss: 0.8818967938423157\n",
            "Epoch 20 train loss: 0.9529361128807068 test loss: 0.9546059966087341\n",
            "Epoch 20 train loss: 0.9124552011489868 test loss: 0.9117664098739624\n",
            "Epoch 20 train loss: 0.9304993152618408 test loss: 0.923703670501709\n",
            "Epoch 20 train loss: 0.9835473895072937 test loss: 0.9840766191482544\n",
            "Epoch 20 train loss: 0.9454664587974548 test loss: 0.9446316957473755\n",
            "Epoch 20 train loss: 0.7746444344520569 test loss: 0.7757672667503357\n",
            "Epoch 20 train loss: 0.8187580704689026 test loss: 0.8143208026885986\n",
            "Epoch 20 train loss: 0.9563486576080322 test loss: 0.9611163139343262\n",
            "Epoch 20 train loss: 0.9877998232841492 test loss: 0.9850725531578064\n",
            "Epoch 20 train loss: 0.7285236120223999 test loss: 0.7229437828063965\n",
            "Epoch 20 train loss: 1.044979214668274 test loss: 1.0409839153289795\n",
            "Epoch 20 train loss: 0.8414794206619263 test loss: 0.8370475172996521\n",
            "Epoch 20 train loss: 0.9057960510253906 test loss: 0.9011629819869995\n",
            "Epoch 20 train loss: 1.1481742858886719 test loss: 1.1480295658111572\n",
            "Epoch 20 train loss: 0.7517328858375549 test loss: 0.7557048201560974\n",
            "Epoch 20 train loss: 0.974351704120636 test loss: 0.9724075794219971\n",
            "Epoch 20 train loss: 1.0159159898757935 test loss: 1.0194731950759888\n",
            "Epoch 20 train loss: 0.8264589309692383 test loss: 0.8218432068824768\n",
            "Epoch 20 train loss: 0.9466230869293213 test loss: 0.9464940428733826\n",
            "Epoch 20 train loss: 0.6978927850723267 test loss: 0.7008022665977478\n",
            "Epoch 20 train loss: 0.939413845539093 test loss: 0.9408972859382629\n",
            "Epoch 20 train loss: 0.73265141248703 test loss: 0.7292258739471436\n",
            "Epoch 20 train loss: 1.067200779914856 test loss: 1.0700650215148926\n",
            "Epoch 20 train loss: 0.9001016616821289 test loss: 0.9014500975608826\n",
            "Epoch 20 train loss: 0.5965741276741028 test loss: 0.6005062460899353\n",
            "Epoch 20 train loss: 0.9250930547714233 test loss: 0.9274088740348816\n",
            "Epoch 20 train loss: 0.8209180235862732 test loss: 0.8185455799102783\n",
            "Epoch 20 train loss: 1.274833083152771 test loss: 1.2740763425827026\n",
            "Epoch 20 train loss: 0.9631322026252747 test loss: 0.9628782272338867\n",
            "Epoch 20 train loss: 1.1364574432373047 test loss: 1.143525242805481\n",
            "Epoch 20 train loss: 0.8004654049873352 test loss: 0.8062023520469666\n",
            "Epoch 20 train loss: 1.0612518787384033 test loss: 1.0633677244186401\n",
            "Epoch 20 train loss: 0.5917514562606812 test loss: 0.5908486843109131\n",
            "Epoch 20 train loss: 0.7817512154579163 test loss: 0.7864074110984802\n",
            "Epoch 20 train loss: 0.66289222240448 test loss: 0.6600224375724792\n",
            "Epoch 20 train loss: 0.680282473564148 test loss: 0.6847885847091675\n",
            "Epoch 20 train loss: 0.5281423330307007 test loss: 0.5240267515182495\n",
            "Epoch 20 train loss: 0.9077790975570679 test loss: 0.9110319018363953\n",
            "Epoch 20 train loss: 0.887536883354187 test loss: 0.8878470659255981\n",
            "Epoch 20 train loss: 0.906978189945221 test loss: 0.9081116914749146\n",
            "Epoch 20 train loss: 0.8334726095199585 test loss: 0.8341363668441772\n",
            "Epoch 20 train loss: 0.7486300468444824 test loss: 0.7463704347610474\n",
            "Epoch 20 train loss: 0.5547306537628174 test loss: 0.5587062239646912\n",
            "Epoch 20 train loss: 1.0236352682113647 test loss: 1.0261138677597046\n",
            "Epoch 30 train loss: 1.0395678281784058 test loss: 1.0390655994415283\n",
            "Epoch 30 train loss: 0.8504490256309509 test loss: 0.8528998494148254\n",
            "Epoch 30 train loss: 1.1663775444030762 test loss: 1.1712138652801514\n",
            "Epoch 30 train loss: 0.9726720452308655 test loss: 0.9681466817855835\n",
            "Epoch 30 train loss: 0.8978798985481262 test loss: 0.8995810747146606\n",
            "Epoch 30 train loss: 0.7768465876579285 test loss: 0.7790748476982117\n",
            "Epoch 30 train loss: 0.9632015228271484 test loss: 0.961909294128418\n",
            "Epoch 30 train loss: 0.9900644421577454 test loss: 0.9849194288253784\n",
            "Epoch 30 train loss: 0.8920190334320068 test loss: 0.8945015668869019\n",
            "Epoch 30 train loss: 1.1134309768676758 test loss: 1.1128580570220947\n",
            "Epoch 30 train loss: 0.9392069578170776 test loss: 0.9429155588150024\n",
            "Epoch 30 train loss: 0.7991081476211548 test loss: 0.8081677556037903\n",
            "Epoch 30 train loss: 0.847960352897644 test loss: 0.8473212718963623\n",
            "Epoch 30 train loss: 1.0008316040039062 test loss: 0.9986457228660583\n",
            "Epoch 30 train loss: 1.009719967842102 test loss: 1.0021576881408691\n",
            "Epoch 30 train loss: 0.7032663822174072 test loss: 0.7046461701393127\n",
            "Epoch 30 train loss: 1.0759772062301636 test loss: 1.0837653875350952\n",
            "Epoch 30 train loss: 0.822952926158905 test loss: 0.8264824748039246\n",
            "Epoch 30 train loss: 0.9284562468528748 test loss: 0.929253876209259\n",
            "Epoch 30 train loss: 1.2016615867614746 test loss: 1.1958415508270264\n",
            "Epoch 30 train loss: 0.6912501454353333 test loss: 0.691855251789093\n",
            "Epoch 30 train loss: 1.0041799545288086 test loss: 1.0038138628005981\n",
            "Epoch 30 train loss: 1.0360522270202637 test loss: 1.0355745553970337\n",
            "Epoch 30 train loss: 0.736255943775177 test loss: 0.7363773584365845\n",
            "Epoch 30 train loss: 0.9220796227455139 test loss: 0.9219103455543518\n",
            "Epoch 30 train loss: 0.8073034882545471 test loss: 0.8112049698829651\n",
            "Epoch 30 train loss: 0.8924944400787354 test loss: 0.892993688583374\n",
            "Epoch 30 train loss: 0.8405258059501648 test loss: 0.8406533002853394\n",
            "Epoch 30 train loss: 1.145871877670288 test loss: 1.1420761346817017\n",
            "Epoch 30 train loss: 1.1194818019866943 test loss: 1.1274827718734741\n",
            "Epoch 30 train loss: 0.5813819766044617 test loss: 0.5786842703819275\n",
            "Epoch 30 train loss: 0.9097238779067993 test loss: 0.9150341153144836\n",
            "Epoch 30 train loss: 0.836301863193512 test loss: 0.8366950154304504\n",
            "Epoch 30 train loss: 1.2411787509918213 test loss: 1.2480593919754028\n",
            "Epoch 30 train loss: 0.8559429049491882 test loss: 0.8591088056564331\n",
            "Epoch 30 train loss: 1.2315400838851929 test loss: 1.2303153276443481\n",
            "Epoch 30 train loss: 0.9924378991127014 test loss: 0.9938340187072754\n",
            "Epoch 30 train loss: 1.0141236782073975 test loss: 1.0186923742294312\n",
            "Epoch 30 train loss: 0.6262306571006775 test loss: 0.6253241300582886\n",
            "Epoch 30 train loss: 1.0198112726211548 test loss: 1.0238010883331299\n",
            "Epoch 30 train loss: 0.8346682190895081 test loss: 0.8315566778182983\n",
            "Epoch 30 train loss: 0.6482913494110107 test loss: 0.6508811116218567\n",
            "Epoch 30 train loss: 0.7797656059265137 test loss: 0.7801802158355713\n",
            "Epoch 30 train loss: 0.9437886476516724 test loss: 0.9439568519592285\n",
            "Epoch 30 train loss: 0.9255673289299011 test loss: 0.9253696799278259\n",
            "Epoch 30 train loss: 0.897774338722229 test loss: 0.8994749188423157\n",
            "Epoch 30 train loss: 1.07110595703125 test loss: 1.0684418678283691\n",
            "Epoch 30 train loss: 0.9575456976890564 test loss: 0.9677118062973022\n",
            "Epoch 30 train loss: 0.7945778369903564 test loss: 0.7925341129302979\n",
            "Epoch 30 train loss: 0.9830973744392395 test loss: 0.976434051990509\n",
            "Epoch 40 train loss: 1.5719690322875977 test loss: 1.591676115989685\n",
            "Epoch 40 train loss: 1.0003015995025635 test loss: 1.0044827461242676\n",
            "Epoch 40 train loss: 1.2734510898590088 test loss: 1.26942777633667\n",
            "Epoch 40 train loss: 0.8831656575202942 test loss: 0.8787280321121216\n",
            "Epoch 40 train loss: 0.7290038466453552 test loss: 0.7285791635513306\n",
            "Epoch 40 train loss: 0.6267603039741516 test loss: 0.6188317537307739\n",
            "Epoch 40 train loss: 0.581749439239502 test loss: 0.5822440385818481\n",
            "Epoch 40 train loss: 0.6393707394599915 test loss: 0.6394654512405396\n",
            "Epoch 40 train loss: 0.604932963848114 test loss: 0.6046682596206665\n",
            "Epoch 40 train loss: 0.7951398491859436 test loss: 0.7936646938323975\n",
            "Epoch 40 train loss: 0.5510637760162354 test loss: 0.5492967963218689\n",
            "Epoch 40 train loss: 1.0389388799667358 test loss: 1.0365986824035645\n",
            "Epoch 40 train loss: 1.1900094747543335 test loss: 1.1985334157943726\n",
            "Epoch 40 train loss: 1.5779863595962524 test loss: 1.5699779987335205\n",
            "Epoch 40 train loss: 0.6647469401359558 test loss: 0.6632636189460754\n",
            "Epoch 40 train loss: 0.368348091840744 test loss: 0.3688046336174011\n",
            "Epoch 40 train loss: 1.5463427305221558 test loss: 1.5486637353897095\n",
            "Epoch 40 train loss: 1.1565210819244385 test loss: 1.161582112312317\n",
            "Epoch 40 train loss: 1.4723095893859863 test loss: 1.476401925086975\n",
            "Epoch 40 train loss: 0.9069950580596924 test loss: 0.9088204503059387\n",
            "Epoch 40 train loss: 0.8447872996330261 test loss: 0.8304809331893921\n",
            "Epoch 40 train loss: 0.6884537935256958 test loss: 0.6874369978904724\n",
            "Epoch 40 train loss: 0.7018588185310364 test loss: 0.7132195830345154\n",
            "Epoch 40 train loss: 0.7461961507797241 test loss: 0.748650312423706\n",
            "Epoch 40 train loss: 1.0855813026428223 test loss: 1.0835325717926025\n",
            "Epoch 40 train loss: 0.5319619178771973 test loss: 0.525956392288208\n",
            "Epoch 40 train loss: 0.6449615955352783 test loss: 0.6496791243553162\n",
            "Epoch 40 train loss: 1.0250366926193237 test loss: 1.027622103691101\n",
            "Epoch 40 train loss: 1.0997488498687744 test loss: 1.0967174768447876\n",
            "Epoch 40 train loss: 0.9664402604103088 test loss: 0.9607391953468323\n",
            "Epoch 40 train loss: 0.47473573684692383 test loss: 0.4736352860927582\n",
            "Epoch 40 train loss: 1.1111023426055908 test loss: 1.1093138456344604\n",
            "Epoch 40 train loss: 0.91472327709198 test loss: 0.9172021150588989\n",
            "Epoch 40 train loss: 0.9739682078361511 test loss: 0.9766634106636047\n",
            "Epoch 40 train loss: 0.7897139191627502 test loss: 0.7866917848587036\n",
            "Epoch 40 train loss: 1.039598822593689 test loss: 1.0386457443237305\n",
            "Epoch 40 train loss: 0.8341419696807861 test loss: 0.8413712382316589\n",
            "Epoch 40 train loss: 1.0556479692459106 test loss: 1.0536543130874634\n",
            "Epoch 40 train loss: 1.0317962169647217 test loss: 1.0280120372772217\n",
            "Epoch 40 train loss: 0.9636013507843018 test loss: 0.9771886467933655\n",
            "Epoch 40 train loss: 1.0484806299209595 test loss: 1.090171217918396\n",
            "Epoch 40 train loss: 0.49447572231292725 test loss: 0.49390316009521484\n",
            "Epoch 40 train loss: 1.0026850700378418 test loss: 1.0043461322784424\n",
            "Epoch 40 train loss: 0.8925886154174805 test loss: 0.8908582329750061\n",
            "Epoch 40 train loss: 0.6432850360870361 test loss: 0.6482354402542114\n",
            "Epoch 40 train loss: 1.031989574432373 test loss: 1.032531499862671\n",
            "Epoch 40 train loss: 0.9810585379600525 test loss: 0.9849390387535095\n",
            "Epoch 40 train loss: 0.9385184645652771 test loss: 0.9393585920333862\n",
            "Epoch 40 train loss: 0.8731504678726196 test loss: 0.8713839650154114\n",
            "Epoch 40 train loss: 0.5407598614692688 test loss: 0.5455783009529114\n",
            "Epoch 50 train loss: 0.9180639982223511 test loss: 0.9162391424179077\n",
            "Epoch 50 train loss: 0.8872758150100708 test loss: 0.8850793838500977\n",
            "Epoch 50 train loss: 1.2461766004562378 test loss: 1.2516376972198486\n",
            "Epoch 50 train loss: 0.8183614611625671 test loss: 0.8074434995651245\n",
            "Epoch 50 train loss: 0.816516101360321 test loss: 0.8166268467903137\n",
            "Epoch 50 train loss: 0.567274808883667 test loss: 0.5709754824638367\n",
            "Epoch 50 train loss: 0.6000040769577026 test loss: 0.6052752733230591\n",
            "Epoch 50 train loss: 0.8412814140319824 test loss: 0.8159059286117554\n",
            "Epoch 50 train loss: 0.6272233128547668 test loss: 0.6211385726928711\n",
            "Epoch 50 train loss: 0.8991953730583191 test loss: 0.8976649641990662\n",
            "Epoch 50 train loss: 0.6882202625274658 test loss: 0.6956886053085327\n",
            "Epoch 50 train loss: 0.9349297285079956 test loss: 0.9379327893257141\n",
            "Epoch 50 train loss: 1.0978481769561768 test loss: 1.094665288925171\n",
            "Epoch 50 train loss: 0.8337558507919312 test loss: 0.8359381556510925\n",
            "Epoch 50 train loss: 0.9927767515182495 test loss: 0.9942119717597961\n",
            "Epoch 50 train loss: 0.7662372589111328 test loss: 0.767842710018158\n",
            "Epoch 50 train loss: 0.8440703749656677 test loss: 0.8414635062217712\n",
            "Epoch 50 train loss: 0.7228721380233765 test loss: 0.721013069152832\n",
            "Epoch 50 train loss: 1.0734783411026 test loss: 1.0780606269836426\n",
            "Epoch 50 train loss: 0.9665254354476929 test loss: 0.967437207698822\n",
            "Epoch 50 train loss: 0.9239380359649658 test loss: 0.9282243251800537\n",
            "Epoch 50 train loss: 0.8440536260604858 test loss: 0.8431484699249268\n",
            "Epoch 50 train loss: 0.8179672956466675 test loss: 0.8114897608757019\n",
            "Epoch 50 train loss: 0.8760693669319153 test loss: 0.8790861964225769\n",
            "Epoch 50 train loss: 0.9691717028617859 test loss: 0.9705849885940552\n",
            "Epoch 50 train loss: 0.9460256695747375 test loss: 0.9400014877319336\n",
            "Epoch 50 train loss: 0.7848854660987854 test loss: 0.7823219299316406\n",
            "Epoch 50 train loss: 0.9754114747047424 test loss: 0.9717658758163452\n",
            "Epoch 50 train loss: 1.142431616783142 test loss: 1.140945315361023\n",
            "Epoch 50 train loss: 0.9566108584403992 test loss: 0.9490108489990234\n",
            "Epoch 50 train loss: 0.7042627334594727 test loss: 0.7044212818145752\n",
            "Epoch 50 train loss: 1.0641297101974487 test loss: 1.0571507215499878\n",
            "Epoch 50 train loss: 0.8643804788589478 test loss: 0.8642328381538391\n",
            "Epoch 50 train loss: 1.0358504056930542 test loss: 1.0306082963943481\n",
            "Epoch 50 train loss: 0.9106091856956482 test loss: 0.9130120873451233\n",
            "Epoch 50 train loss: 1.0890412330627441 test loss: 1.0886873006820679\n",
            "Epoch 50 train loss: 0.8389289975166321 test loss: 0.8435199856758118\n",
            "Epoch 50 train loss: 0.7843275666236877 test loss: 0.7894010543823242\n",
            "Epoch 50 train loss: 0.8526920080184937 test loss: 0.847861111164093\n",
            "Epoch 50 train loss: 0.8712679743766785 test loss: 0.8734427690505981\n",
            "Epoch 50 train loss: 0.8905432820320129 test loss: 0.8917140960693359\n",
            "Epoch 50 train loss: 0.6525841951370239 test loss: 0.6518186926841736\n",
            "Epoch 50 train loss: 0.8672105669975281 test loss: 0.8647437691688538\n",
            "Epoch 50 train loss: 0.8688486218452454 test loss: 0.8696732521057129\n",
            "Epoch 50 train loss: 0.7092788815498352 test loss: 0.708629310131073\n",
            "Epoch 50 train loss: 0.979332685470581 test loss: 0.9777923822402954\n",
            "Epoch 50 train loss: 0.9406660199165344 test loss: 0.937436044216156\n",
            "Epoch 50 train loss: 0.8490999937057495 test loss: 0.8463795781135559\n",
            "Epoch 50 train loss: 0.8852351307868958 test loss: 0.8755770921707153\n",
            "Epoch 50 train loss: 0.7339802384376526 test loss: 0.7306122779846191\n",
            "Epoch 60 train loss: 1.093604326248169 test loss: 1.096570611000061\n",
            "Epoch 60 train loss: 0.49006959795951843 test loss: 0.4875257909297943\n",
            "Epoch 60 train loss: 1.7768675088882446 test loss: 1.7811099290847778\n",
            "Epoch 60 train loss: 0.380318820476532 test loss: 0.38358521461486816\n",
            "Epoch 60 train loss: 0.7847586870193481 test loss: 0.785951554775238\n",
            "Epoch 60 train loss: 0.6191157102584839 test loss: 0.6334534287452698\n",
            "Epoch 60 train loss: 0.3424236476421356 test loss: 0.34343427419662476\n",
            "Epoch 60 train loss: 0.8431903123855591 test loss: 0.8493782877922058\n",
            "Epoch 60 train loss: 0.6603601574897766 test loss: 0.6540682911872864\n",
            "Epoch 60 train loss: 0.40435075759887695 test loss: 0.41267791390419006\n",
            "Epoch 60 train loss: 0.6966292858123779 test loss: 0.7013287544250488\n",
            "Epoch 60 train loss: 0.9398534297943115 test loss: 0.9350535273551941\n",
            "Epoch 60 train loss: 1.0588529109954834 test loss: 1.053460717201233\n",
            "Epoch 60 train loss: 0.5738244652748108 test loss: 0.5708006620407104\n",
            "Epoch 60 train loss: 0.5933530330657959 test loss: 0.6055481433868408\n",
            "Epoch 60 train loss: 0.5613008141517639 test loss: 0.557841956615448\n",
            "Epoch 60 train loss: 0.9298060536384583 test loss: 0.9297741055488586\n",
            "Epoch 60 train loss: 0.7914434671401978 test loss: 0.783246636390686\n",
            "Epoch 60 train loss: 1.099251627922058 test loss: 1.0929526090621948\n",
            "Epoch 60 train loss: 0.6832733750343323 test loss: 0.6809567213058472\n",
            "Epoch 60 train loss: 0.7576775550842285 test loss: 0.7534189820289612\n",
            "Epoch 60 train loss: 0.6048447489738464 test loss: 0.6140475273132324\n",
            "Epoch 60 train loss: 0.4073493778705597 test loss: 0.4011366069316864\n",
            "Epoch 60 train loss: 0.9154637455940247 test loss: 0.9222000241279602\n",
            "Epoch 60 train loss: 1.0600804090499878 test loss: 1.0650688409805298\n",
            "Epoch 60 train loss: 0.36683595180511475 test loss: 0.36596181988716125\n",
            "Epoch 60 train loss: 0.39379945397377014 test loss: 0.39132922887802124\n",
            "Epoch 60 train loss: 0.4375636577606201 test loss: 0.43660950660705566\n",
            "Epoch 60 train loss: 0.9030919671058655 test loss: 0.902794599533081\n",
            "Epoch 60 train loss: 0.25045499205589294 test loss: 0.2507892847061157\n",
            "Epoch 60 train loss: 0.3520580530166626 test loss: 0.3528795838356018\n",
            "Epoch 60 train loss: 0.8990719318389893 test loss: 0.8944098949432373\n",
            "Epoch 60 train loss: 0.2200319617986679 test loss: 0.21725250780582428\n",
            "Epoch 60 train loss: 1.3654083013534546 test loss: 1.3660107851028442\n",
            "Epoch 60 train loss: 0.5457306504249573 test loss: 0.534731388092041\n",
            "Epoch 60 train loss: 0.8870077133178711 test loss: 0.8871244788169861\n",
            "Epoch 60 train loss: 0.4590337872505188 test loss: 0.45480865240097046\n",
            "Epoch 60 train loss: 0.8121805787086487 test loss: 0.8102606534957886\n",
            "Epoch 60 train loss: 0.39958468079566956 test loss: 0.3963968753814697\n",
            "Epoch 60 train loss: 0.27949392795562744 test loss: 0.2782406806945801\n",
            "Epoch 60 train loss: 0.32353442907333374 test loss: 0.327772855758667\n",
            "Epoch 60 train loss: 0.16729111969470978 test loss: 0.16608254611492157\n",
            "Epoch 60 train loss: 0.5968562960624695 test loss: 0.6012706756591797\n",
            "Epoch 60 train loss: 0.2793245315551758 test loss: 0.27553024888038635\n",
            "Epoch 60 train loss: 0.37493813037872314 test loss: 0.37831762433052063\n",
            "Epoch 60 train loss: 0.8444747924804688 test loss: 0.846278965473175\n",
            "Epoch 60 train loss: 0.18816515803337097 test loss: 0.19050411880016327\n",
            "Epoch 60 train loss: 0.24195851385593414 test loss: 0.2401772290468216\n",
            "Epoch 60 train loss: 0.2836064100265503 test loss: 0.2823520004749298\n",
            "Epoch 60 train loss: 0.7974263429641724 test loss: 0.7970598340034485\n",
            "Epoch 70 train loss: 0.7455783486366272 test loss: 0.7428833246231079\n",
            "Epoch 70 train loss: 0.6947113871574402 test loss: 0.6922157406806946\n",
            "Epoch 70 train loss: 1.5692070722579956 test loss: 1.5683623552322388\n",
            "Epoch 70 train loss: 0.3345431089401245 test loss: 0.3354661464691162\n",
            "Epoch 70 train loss: 0.7438223958015442 test loss: 0.7410374879837036\n",
            "Epoch 70 train loss: 0.3664208650588989 test loss: 0.36833515763282776\n",
            "Epoch 70 train loss: 0.28370168805122375 test loss: 0.28192493319511414\n",
            "Epoch 70 train loss: 1.0253064632415771 test loss: 1.0344945192337036\n",
            "Epoch 70 train loss: 0.6610236167907715 test loss: 0.6562737226486206\n",
            "Epoch 70 train loss: 0.30342110991477966 test loss: 0.3001635670661926\n",
            "Epoch 70 train loss: 0.696179986000061 test loss: 0.6985702514648438\n",
            "Epoch 70 train loss: 0.8509792685508728 test loss: 0.8498502969741821\n",
            "Epoch 70 train loss: 0.9497557878494263 test loss: 0.9558311104774475\n",
            "Epoch 70 train loss: 0.2280576527118683 test loss: 0.227993443608284\n",
            "Epoch 70 train loss: 0.5556179285049438 test loss: 0.5537353157997131\n",
            "Epoch 70 train loss: 0.32537809014320374 test loss: 0.3227594196796417\n",
            "Epoch 70 train loss: 1.1570967435836792 test loss: 1.1683903932571411\n",
            "Epoch 70 train loss: 0.8617841005325317 test loss: 0.8494477868080139\n",
            "Epoch 70 train loss: 1.2359915971755981 test loss: 1.236765742301941\n",
            "Epoch 70 train loss: 1.1642036437988281 test loss: 1.1692395210266113\n",
            "Epoch 70 train loss: 0.4581950604915619 test loss: 0.457293838262558\n",
            "Epoch 70 train loss: 0.5725349187850952 test loss: 0.5805603265762329\n",
            "Epoch 70 train loss: 0.5372974872589111 test loss: 0.5363981127738953\n",
            "Epoch 70 train loss: 0.581723153591156 test loss: 0.5851648449897766\n",
            "Epoch 70 train loss: 0.5271106958389282 test loss: 0.5235516428947449\n",
            "Epoch 70 train loss: 0.33435505628585815 test loss: 0.3384091258049011\n",
            "Epoch 70 train loss: 0.2068983018398285 test loss: 0.20768597722053528\n",
            "Epoch 70 train loss: 0.47682544589042664 test loss: 0.4784482717514038\n",
            "Epoch 70 train loss: 0.8750791549682617 test loss: 0.8789168000221252\n",
            "Epoch 70 train loss: 0.23503440618515015 test loss: 0.2374476194381714\n",
            "Epoch 70 train loss: 0.4272197484970093 test loss: 0.4159681499004364\n",
            "Epoch 70 train loss: 0.6254352331161499 test loss: 0.621483325958252\n",
            "Epoch 70 train loss: 0.1872674971818924 test loss: 0.1852577030658722\n",
            "Epoch 70 train loss: 1.3764386177062988 test loss: 1.3766433000564575\n",
            "Epoch 70 train loss: 0.49061936140060425 test loss: 0.48018455505371094\n",
            "Epoch 70 train loss: 0.8640885353088379 test loss: 0.8710793852806091\n",
            "Epoch 70 train loss: 0.37935298681259155 test loss: 0.3850145936012268\n",
            "Epoch 70 train loss: 0.651000440120697 test loss: 0.6499261856079102\n",
            "Epoch 70 train loss: 0.5065001249313354 test loss: 0.5023095607757568\n",
            "Epoch 70 train loss: 0.2364649474620819 test loss: 0.23548109829425812\n",
            "Epoch 70 train loss: 0.3399706184864044 test loss: 0.3339858651161194\n",
            "Epoch 70 train loss: 0.12138290703296661 test loss: 0.12140163034200668\n",
            "Epoch 70 train loss: 0.7317113280296326 test loss: 0.7313109040260315\n",
            "Epoch 70 train loss: 0.2801101803779602 test loss: 0.2786765396595001\n",
            "Epoch 70 train loss: 0.3028769791126251 test loss: 0.30577605962753296\n",
            "Epoch 70 train loss: 0.5740568041801453 test loss: 0.5779353976249695\n",
            "Epoch 70 train loss: 0.1688905656337738 test loss: 0.17005427181720734\n",
            "Epoch 70 train loss: 0.22124077379703522 test loss: 0.21885181963443756\n",
            "Epoch 70 train loss: 0.33684825897216797 test loss: 0.33106014132499695\n",
            "Epoch 70 train loss: 0.7945837378501892 test loss: 0.7923358082771301\n",
            "Epoch 80 train loss: 0.6159231662750244 test loss: 0.6096305251121521\n",
            "Epoch 80 train loss: 1.3203855752944946 test loss: 1.3095638751983643\n",
            "Epoch 80 train loss: 1.2588659524917603 test loss: 1.25802743434906\n",
            "Epoch 80 train loss: 0.5480429530143738 test loss: 0.5550374984741211\n",
            "Epoch 80 train loss: 0.6667693257331848 test loss: 0.662270188331604\n",
            "Epoch 80 train loss: 0.4562245309352875 test loss: 0.46128517389297485\n",
            "Epoch 80 train loss: 0.27718597650527954 test loss: 0.27373242378234863\n",
            "Epoch 80 train loss: 0.9160038828849792 test loss: 0.9087288975715637\n",
            "Epoch 80 train loss: 0.6701721549034119 test loss: 0.6705392003059387\n",
            "Epoch 80 train loss: 0.43558695912361145 test loss: 0.43454280495643616\n",
            "Epoch 80 train loss: 0.580382227897644 test loss: 0.5763811469078064\n",
            "Epoch 80 train loss: 1.6658737659454346 test loss: 1.6636978387832642\n",
            "Epoch 80 train loss: 1.3061877489089966 test loss: 1.3003782033920288\n",
            "Epoch 80 train loss: 0.5404844284057617 test loss: 0.5414252281188965\n",
            "Epoch 80 train loss: 0.48478683829307556 test loss: 0.47379645705223083\n",
            "Epoch 80 train loss: 0.4463930130004883 test loss: 0.4476161301136017\n",
            "Epoch 80 train loss: 0.8546152710914612 test loss: 0.8530731201171875\n",
            "Epoch 80 train loss: 0.8534786105155945 test loss: 0.8453588485717773\n",
            "Epoch 80 train loss: 0.9690616130828857 test loss: 0.9681183099746704\n",
            "Epoch 80 train loss: 0.7308140993118286 test loss: 0.725674033164978\n",
            "Epoch 80 train loss: 1.178898572921753 test loss: 1.1742368936538696\n",
            "Epoch 80 train loss: 0.7218758463859558 test loss: 0.7247178554534912\n",
            "Epoch 80 train loss: 0.4730703830718994 test loss: 0.4710110127925873\n",
            "Epoch 80 train loss: 1.0852652788162231 test loss: 1.0837503671646118\n",
            "Epoch 80 train loss: 1.0983703136444092 test loss: 1.0995830297470093\n",
            "Epoch 80 train loss: 0.647362232208252 test loss: 0.6568953990936279\n",
            "Epoch 80 train loss: 0.5042212009429932 test loss: 0.49942073225975037\n",
            "Epoch 80 train loss: 0.9436873197555542 test loss: 0.9482446312904358\n",
            "Epoch 80 train loss: 0.731711745262146 test loss: 0.7310089468955994\n",
            "Epoch 80 train loss: 0.3954792022705078 test loss: 0.39407962560653687\n",
            "Epoch 80 train loss: 0.8012219667434692 test loss: 0.8011638522148132\n",
            "Epoch 80 train loss: 1.195014476776123 test loss: 1.1926072835922241\n",
            "Epoch 80 train loss: 0.4110419750213623 test loss: 0.41483089327812195\n",
            "Epoch 80 train loss: 0.8406997919082642 test loss: 0.8466823101043701\n",
            "Epoch 80 train loss: 0.5796703696250916 test loss: 0.5810709595680237\n",
            "Epoch 80 train loss: 0.7425472736358643 test loss: 0.7401632070541382\n",
            "Epoch 80 train loss: 0.5787932276725769 test loss: 0.5834571719169617\n",
            "Epoch 80 train loss: 0.3420586585998535 test loss: 0.3397715389728546\n",
            "Epoch 80 train loss: 0.9516332149505615 test loss: 0.9533383250236511\n",
            "Epoch 80 train loss: 0.48911458253860474 test loss: 0.4903985857963562\n",
            "Epoch 80 train loss: 0.9351125955581665 test loss: 0.9365106821060181\n",
            "Epoch 80 train loss: 0.24702495336532593 test loss: 0.2510300874710083\n",
            "Epoch 80 train loss: 1.297454595565796 test loss: 1.2999788522720337\n",
            "Epoch 80 train loss: 0.4246879518032074 test loss: 0.42989790439605713\n",
            "Epoch 80 train loss: 0.3527117967605591 test loss: 0.3432314097881317\n",
            "Epoch 80 train loss: 1.1008884906768799 test loss: 1.1020898818969727\n",
            "Epoch 80 train loss: 0.35126903653144836 test loss: 0.3506792187690735\n",
            "Epoch 80 train loss: 0.46633827686309814 test loss: 0.4728015661239624\n",
            "Epoch 80 train loss: 0.9177762866020203 test loss: 0.9148516654968262\n",
            "Epoch 80 train loss: 0.255877822637558 test loss: 0.25284069776535034\n",
            "Epoch 90 train loss: 0.883791446685791 test loss: 0.904107928276062\n",
            "Epoch 90 train loss: 1.2464412450790405 test loss: 1.251094937324524\n",
            "Epoch 90 train loss: 1.2460777759552002 test loss: 1.2382490634918213\n",
            "Epoch 90 train loss: 0.6135764718055725 test loss: 0.6095743775367737\n",
            "Epoch 90 train loss: 0.7529157996177673 test loss: 0.7591730356216431\n",
            "Epoch 90 train loss: 0.3826223611831665 test loss: 0.391231894493103\n",
            "Epoch 90 train loss: 0.2503513991832733 test loss: 0.2471107840538025\n",
            "Epoch 90 train loss: 0.8135440945625305 test loss: 0.805165708065033\n",
            "Epoch 90 train loss: 0.5065461993217468 test loss: 0.5022909045219421\n",
            "Epoch 90 train loss: 0.5467114448547363 test loss: 0.5524267554283142\n",
            "Epoch 90 train loss: 0.4807646870613098 test loss: 0.4830816984176636\n",
            "Epoch 90 train loss: 1.3113712072372437 test loss: 1.3130862712860107\n",
            "Epoch 90 train loss: 0.9656559824943542 test loss: 0.963178277015686\n",
            "Epoch 90 train loss: 0.47667935490608215 test loss: 0.47720056772232056\n",
            "Epoch 90 train loss: 0.4466659128665924 test loss: 0.45582908391952515\n",
            "Epoch 90 train loss: 0.28084632754325867 test loss: 0.278862863779068\n",
            "Epoch 90 train loss: 0.8530819416046143 test loss: 0.8625824451446533\n",
            "Epoch 90 train loss: 0.9238317012786865 test loss: 0.9201900362968445\n",
            "Epoch 90 train loss: 0.8329750299453735 test loss: 0.8370262980461121\n",
            "Epoch 90 train loss: 0.8830879330635071 test loss: 0.8852018117904663\n",
            "Epoch 90 train loss: 0.9726083874702454 test loss: 0.9703686833381653\n",
            "Epoch 90 train loss: 0.555213451385498 test loss: 0.5595163106918335\n",
            "Epoch 90 train loss: 0.3176642656326294 test loss: 0.32183295488357544\n",
            "Epoch 90 train loss: 1.0036956071853638 test loss: 0.9822244048118591\n",
            "Epoch 90 train loss: 1.0347338914871216 test loss: 1.038503646850586\n",
            "Epoch 90 train loss: 0.41406509280204773 test loss: 0.4144151210784912\n",
            "Epoch 90 train loss: 0.29988422989845276 test loss: 0.29235583543777466\n",
            "Epoch 90 train loss: 0.7138534188270569 test loss: 0.7224196791648865\n",
            "Epoch 90 train loss: 0.6615843176841736 test loss: 0.6897844672203064\n",
            "Epoch 90 train loss: 0.3416600525379181 test loss: 0.342912495136261\n",
            "Epoch 90 train loss: 0.5394874811172485 test loss: 0.5369072556495667\n",
            "Epoch 90 train loss: 1.0895366668701172 test loss: 1.095505952835083\n",
            "Epoch 90 train loss: 0.22541281580924988 test loss: 0.22682654857635498\n",
            "Epoch 90 train loss: 0.9078489542007446 test loss: 0.9149258136749268\n",
            "Epoch 90 train loss: 0.40122777223587036 test loss: 0.3985506296157837\n",
            "Epoch 90 train loss: 0.6439961194992065 test loss: 0.6520422697067261\n",
            "Epoch 90 train loss: 0.6635286808013916 test loss: 0.6573337912559509\n",
            "Epoch 90 train loss: 0.49995875358581543 test loss: 0.5017115473747253\n",
            "Epoch 90 train loss: 0.8059658408164978 test loss: 0.816795825958252\n",
            "Epoch 90 train loss: 0.45181599259376526 test loss: 0.45723646879196167\n",
            "Epoch 90 train loss: 0.8449021577835083 test loss: 0.8545063138008118\n",
            "Epoch 90 train loss: 0.12130200117826462 test loss: 0.12316595762968063\n",
            "Epoch 90 train loss: 1.427283525466919 test loss: 1.4145807027816772\n",
            "Epoch 90 train loss: 0.5635185837745667 test loss: 0.5636386275291443\n",
            "Epoch 90 train loss: 0.2969723045825958 test loss: 0.29045385122299194\n",
            "Epoch 90 train loss: 1.0219013690948486 test loss: 1.0233101844787598\n",
            "Epoch 90 train loss: 0.3332568407058716 test loss: 0.33313465118408203\n",
            "Epoch 90 train loss: 0.4283389449119568 test loss: 0.43100857734680176\n",
            "Epoch 90 train loss: 0.8256570100784302 test loss: 0.8347637057304382\n",
            "Epoch 90 train loss: 0.21968846023082733 test loss: 0.221822127699852\n",
            "Epoch 100 train loss: 0.613084077835083 test loss: 0.6064781546592712\n",
            "Epoch 100 train loss: 0.8995776772499084 test loss: 0.8937073945999146\n",
            "Epoch 100 train loss: 1.6126189231872559 test loss: 1.6084433794021606\n",
            "Epoch 100 train loss: 0.549064040184021 test loss: 0.5344107151031494\n",
            "Epoch 100 train loss: 0.7513583302497864 test loss: 0.7499136924743652\n",
            "Epoch 100 train loss: 0.47097769379615784 test loss: 0.4784252941608429\n",
            "Epoch 100 train loss: 0.23759739100933075 test loss: 0.23823808133602142\n",
            "Epoch 100 train loss: 0.8634908199310303 test loss: 0.8709157705307007\n",
            "Epoch 100 train loss: 0.5816622376441956 test loss: 0.5768651366233826\n",
            "Epoch 100 train loss: 0.5276894569396973 test loss: 0.5440008044242859\n",
            "Epoch 100 train loss: 0.5379553437232971 test loss: 0.5427815318107605\n",
            "Epoch 100 train loss: 0.9405224919319153 test loss: 0.9386951327323914\n",
            "Epoch 100 train loss: 1.01908278465271 test loss: 1.019363284111023\n",
            "Epoch 100 train loss: 0.9103044271469116 test loss: 0.9037973284721375\n",
            "Epoch 100 train loss: 0.5129427909851074 test loss: 0.5367317199707031\n",
            "Epoch 100 train loss: 0.35915663838386536 test loss: 0.36077213287353516\n",
            "Epoch 100 train loss: 0.9540942907333374 test loss: 0.9632229208946228\n",
            "Epoch 100 train loss: 0.7226120233535767 test loss: 0.729743480682373\n",
            "Epoch 100 train loss: 1.2376899719238281 test loss: 1.2431076765060425\n",
            "Epoch 100 train loss: 0.7271241545677185 test loss: 0.7234137058258057\n",
            "Epoch 100 train loss: 0.9886074662208557 test loss: 0.9923409819602966\n",
            "Epoch 100 train loss: 0.6122448444366455 test loss: 0.6141526103019714\n",
            "Epoch 100 train loss: 0.5329249501228333 test loss: 0.5371906161308289\n",
            "Epoch 100 train loss: 0.9352947473526001 test loss: 0.9297842383384705\n",
            "Epoch 100 train loss: 0.9019960165023804 test loss: 0.9051229357719421\n",
            "Epoch 100 train loss: 0.45153164863586426 test loss: 0.44897231459617615\n",
            "Epoch 100 train loss: 0.34204018115997314 test loss: 0.3390524983406067\n",
            "Epoch 100 train loss: 0.6379368305206299 test loss: 0.6370336413383484\n",
            "Epoch 100 train loss: 0.6610631942749023 test loss: 0.6658706665039062\n",
            "Epoch 100 train loss: 0.33605897426605225 test loss: 0.33657869696617126\n",
            "Epoch 100 train loss: 0.33227694034576416 test loss: 0.32218849658966064\n",
            "Epoch 100 train loss: 0.7923241257667542 test loss: 0.7953271269798279\n",
            "Epoch 100 train loss: 0.23612192273139954 test loss: 0.23394592106342316\n",
            "Epoch 100 train loss: 1.249415636062622 test loss: 1.2481502294540405\n",
            "Epoch 100 train loss: 0.5785993933677673 test loss: 0.5749870538711548\n",
            "Epoch 100 train loss: 0.7181702852249146 test loss: 0.7095434069633484\n",
            "Epoch 100 train loss: 0.6168303489685059 test loss: 0.6168033480644226\n",
            "Epoch 100 train loss: 0.637763500213623 test loss: 0.6374518275260925\n",
            "Epoch 100 train loss: 0.5309600830078125 test loss: 0.526216983795166\n",
            "Epoch 100 train loss: 0.4684118926525116 test loss: 0.46882838010787964\n",
            "Epoch 100 train loss: 0.628015398979187 test loss: 0.6354795098304749\n",
            "Epoch 100 train loss: 0.1649596095085144 test loss: 0.16824546456336975\n",
            "Epoch 100 train loss: 0.8270746469497681 test loss: 0.8196668028831482\n",
            "Epoch 100 train loss: 0.470748633146286 test loss: 0.4732518196105957\n",
            "Epoch 100 train loss: 0.2778238356113434 test loss: 0.27839767932891846\n",
            "Epoch 100 train loss: 0.8970273733139038 test loss: 0.8964979648590088\n",
            "Epoch 100 train loss: 0.2791080176830292 test loss: 0.28415533900260925\n",
            "Epoch 100 train loss: 0.4098835587501526 test loss: 0.40486299991607666\n",
            "Epoch 100 train loss: 0.519992470741272 test loss: 0.5159983038902283\n",
            "Epoch 100 train loss: 0.4944809079170227 test loss: 0.502741277217865\n",
            "Saving model \n",
            "\n",
            "Done\n",
            "Epoch 110 train loss: 0.645207405090332 test loss: 0.6536967158317566\n",
            "Epoch 110 train loss: 0.6028240919113159 test loss: 0.5981555581092834\n",
            "Epoch 110 train loss: 1.4369696378707886 test loss: 1.4279810190200806\n",
            "Epoch 110 train loss: 0.4216429591178894 test loss: 0.4164772927761078\n",
            "Epoch 110 train loss: 0.6847295761108398 test loss: 0.6843976378440857\n",
            "Epoch 110 train loss: 0.3917364180088043 test loss: 0.3920576274394989\n",
            "Epoch 110 train loss: 0.3067038655281067 test loss: 0.30009278655052185\n",
            "Epoch 110 train loss: 1.0120514631271362 test loss: 1.0245498418807983\n",
            "Epoch 110 train loss: 0.4908062815666199 test loss: 0.48748263716697693\n",
            "Epoch 110 train loss: 0.2631233036518097 test loss: 0.2699926793575287\n",
            "Epoch 110 train loss: 0.5097490549087524 test loss: 0.5174311995506287\n",
            "Epoch 110 train loss: 0.8054608106613159 test loss: 0.8018273115158081\n",
            "Epoch 110 train loss: 1.008681058883667 test loss: 0.9998748898506165\n",
            "Epoch 110 train loss: 0.8822229504585266 test loss: 0.885646641254425\n",
            "Epoch 110 train loss: 0.49845463037490845 test loss: 0.4913460314273834\n",
            "Epoch 110 train loss: 0.25002962350845337 test loss: 0.2534176707267761\n",
            "Epoch 110 train loss: 1.0670419931411743 test loss: 1.0741164684295654\n",
            "Epoch 110 train loss: 0.6512566208839417 test loss: 0.6575427651405334\n",
            "Epoch 110 train loss: 1.1848853826522827 test loss: 1.1754019260406494\n",
            "Epoch 110 train loss: 0.6767438054084778 test loss: 0.6675596237182617\n",
            "Epoch 110 train loss: 0.6093964576721191 test loss: 0.6009184718132019\n",
            "Epoch 110 train loss: 0.6062609553337097 test loss: 0.5992938280105591\n",
            "Epoch 110 train loss: 0.37006232142448425 test loss: 0.37253764271736145\n",
            "Epoch 110 train loss: 0.7718217968940735 test loss: 0.7659398913383484\n",
            "Epoch 110 train loss: 0.7227354049682617 test loss: 0.7339383363723755\n",
            "Epoch 110 train loss: 0.31806570291519165 test loss: 0.31993117928504944\n",
            "Epoch 110 train loss: 0.2603716254234314 test loss: 0.2596323490142822\n",
            "Epoch 110 train loss: 0.4687661826610565 test loss: 0.46844208240509033\n",
            "Epoch 110 train loss: 0.6715960502624512 test loss: 0.6731815338134766\n",
            "Epoch 110 train loss: 0.14894938468933105 test loss: 0.14412440359592438\n",
            "Epoch 110 train loss: 0.4343031048774719 test loss: 0.43251314759254456\n",
            "Epoch 110 train loss: 0.5924627780914307 test loss: 0.5828633904457092\n",
            "Epoch 110 train loss: 0.2548779547214508 test loss: 0.255136638879776\n",
            "Epoch 110 train loss: 1.2240707874298096 test loss: 1.2269212007522583\n",
            "Epoch 110 train loss: 0.46070683002471924 test loss: 0.4629141390323639\n",
            "Epoch 110 train loss: 0.5418057441711426 test loss: 0.5322896838188171\n",
            "Epoch 110 train loss: 0.6088065505027771 test loss: 0.6184291243553162\n",
            "Epoch 110 train loss: 0.7475523948669434 test loss: 0.7481982707977295\n",
            "Epoch 110 train loss: 0.4510989487171173 test loss: 0.4596511423587799\n",
            "Epoch 110 train loss: 0.22811347246170044 test loss: 0.22728528082370758\n",
            "Epoch 110 train loss: 0.42847079038619995 test loss: 0.421852707862854\n",
            "Epoch 110 train loss: 0.16947880387306213 test loss: 0.16955482959747314\n",
            "Epoch 110 train loss: 0.6606216430664062 test loss: 0.6531017422676086\n",
            "Epoch 110 train loss: 0.4942898750305176 test loss: 0.48897066712379456\n",
            "Epoch 110 train loss: 0.349077969789505 test loss: 0.3574341833591461\n",
            "Epoch 110 train loss: 0.6549631953239441 test loss: 0.6560951471328735\n",
            "Epoch 110 train loss: 0.19239026308059692 test loss: 0.1972549557685852\n",
            "Epoch 110 train loss: 0.2685542404651642 test loss: 0.26865506172180176\n",
            "Epoch 110 train loss: 0.45788997411727905 test loss: 0.4551377296447754\n",
            "Epoch 110 train loss: 0.42042455077171326 test loss: 0.4259428381919861\n",
            "Epoch 120 train loss: 0.5786496996879578 test loss: 0.5723754167556763\n",
            "Epoch 120 train loss: 0.837125301361084 test loss: 0.8378118276596069\n",
            "Epoch 120 train loss: 1.3551967144012451 test loss: 1.3499841690063477\n",
            "Epoch 120 train loss: 0.39270660281181335 test loss: 0.3967869281768799\n",
            "Epoch 120 train loss: 0.6654828190803528 test loss: 0.6744353175163269\n",
            "Epoch 120 train loss: 0.4082019329071045 test loss: 0.4031643271446228\n",
            "Epoch 120 train loss: 0.2799563705921173 test loss: 0.27676060795783997\n",
            "Epoch 120 train loss: 1.051387906074524 test loss: 1.049494981765747\n",
            "Epoch 120 train loss: 0.4379579722881317 test loss: 0.4362587034702301\n",
            "Epoch 120 train loss: 0.2549710273742676 test loss: 0.26681801676750183\n",
            "Epoch 120 train loss: 0.5217390060424805 test loss: 0.5016108751296997\n",
            "Epoch 120 train loss: 1.0838491916656494 test loss: 1.0606311559677124\n",
            "Epoch 120 train loss: 1.0459812879562378 test loss: 1.0431495904922485\n",
            "Epoch 120 train loss: 0.34193673729896545 test loss: 0.364120751619339\n",
            "Epoch 120 train loss: 0.4751136004924774 test loss: 0.4995892643928528\n",
            "Epoch 120 train loss: 0.36440950632095337 test loss: 0.36164650321006775\n",
            "Epoch 120 train loss: 0.9700780510902405 test loss: 0.9733762741088867\n",
            "Epoch 120 train loss: 0.7363047003746033 test loss: 0.7415924668312073\n",
            "Epoch 120 train loss: 1.2089176177978516 test loss: 1.211448311805725\n",
            "Epoch 120 train loss: 0.5152201056480408 test loss: 0.5265213847160339\n",
            "Epoch 120 train loss: 1.259069800376892 test loss: 1.2542067766189575\n",
            "Epoch 120 train loss: 0.6263853311538696 test loss: 0.6237693428993225\n",
            "Epoch 120 train loss: 0.28695154190063477 test loss: 0.2951146364212036\n",
            "Epoch 120 train loss: 0.9442972540855408 test loss: 0.9454622864723206\n",
            "Epoch 120 train loss: 0.6368955373764038 test loss: 0.6373916268348694\n",
            "Epoch 120 train loss: 0.3105262219905853 test loss: 0.3117196559906006\n",
            "Epoch 120 train loss: 0.2267199158668518 test loss: 0.22465780377388\n",
            "Epoch 120 train loss: 0.530042827129364 test loss: 0.5347654819488525\n",
            "Epoch 120 train loss: 0.6157284379005432 test loss: 0.6462611556053162\n",
            "Epoch 120 train loss: 0.17576177418231964 test loss: 0.1767636239528656\n",
            "Epoch 120 train loss: 0.5260039567947388 test loss: 0.5308107137680054\n",
            "Epoch 120 train loss: 0.6084720492362976 test loss: 0.605912983417511\n",
            "Epoch 120 train loss: 0.25614047050476074 test loss: 0.25795936584472656\n",
            "Epoch 120 train loss: 1.1712920665740967 test loss: 1.167772650718689\n",
            "Epoch 120 train loss: 0.41449055075645447 test loss: 0.409207284450531\n",
            "Epoch 120 train loss: 0.4649454951286316 test loss: 0.46119484305381775\n",
            "Epoch 120 train loss: 0.3928717374801636 test loss: 0.3913242816925049\n",
            "Epoch 120 train loss: 0.6657580733299255 test loss: 0.6671203970909119\n",
            "Epoch 120 train loss: 0.4755050837993622 test loss: 0.4785987436771393\n",
            "Epoch 120 train loss: 0.22110025584697723 test loss: 0.2157575488090515\n",
            "Epoch 120 train loss: 0.782075047492981 test loss: 0.7709367871284485\n",
            "Epoch 120 train loss: 0.13640369474887848 test loss: 0.13408489525318146\n",
            "Epoch 120 train loss: 1.0476304292678833 test loss: 1.0672876834869385\n",
            "Epoch 120 train loss: 0.3128012716770172 test loss: 0.3133457899093628\n",
            "Epoch 120 train loss: 0.32803982496261597 test loss: 0.3274703621864319\n",
            "Epoch 120 train loss: 0.5987626314163208 test loss: 0.6012303829193115\n",
            "Epoch 120 train loss: 0.1753585934638977 test loss: 0.17162933945655823\n",
            "Epoch 120 train loss: 0.20779503881931305 test loss: 0.20589935779571533\n",
            "Epoch 120 train loss: 0.6030017137527466 test loss: 0.5942147970199585\n",
            "Epoch 120 train loss: 0.5591849088668823 test loss: 0.5563581585884094\n",
            "Epoch 130 train loss: 0.630181074142456 test loss: 0.6277002096176147\n",
            "Epoch 130 train loss: 0.5288674235343933 test loss: 0.5298055410385132\n",
            "Epoch 130 train loss: 1.450233817100525 test loss: 1.4744685888290405\n",
            "Epoch 130 train loss: 0.3126834034919739 test loss: 0.3153414726257324\n",
            "Epoch 130 train loss: 0.960226833820343 test loss: 0.9576917886734009\n",
            "Epoch 130 train loss: 0.452409029006958 test loss: 0.4495967924594879\n",
            "Epoch 130 train loss: 0.24690912663936615 test loss: 0.2501930296421051\n",
            "Epoch 130 train loss: 1.110214114189148 test loss: 1.1101218461990356\n",
            "Epoch 130 train loss: 0.5999897718429565 test loss: 0.5892718434333801\n",
            "Epoch 130 train loss: 0.23297825455665588 test loss: 0.23065726459026337\n",
            "Epoch 130 train loss: 0.609320878982544 test loss: 0.6142460703849792\n",
            "Epoch 130 train loss: 0.9794549345970154 test loss: 0.9591547250747681\n",
            "Epoch 130 train loss: 0.8994950652122498 test loss: 0.9024186134338379\n",
            "Epoch 130 train loss: 0.2725014090538025 test loss: 0.27103012800216675\n",
            "Epoch 130 train loss: 0.32307276129722595 test loss: 0.33741164207458496\n",
            "Epoch 130 train loss: 0.27981165051460266 test loss: 0.2899184226989746\n",
            "Epoch 130 train loss: 0.9776925444602966 test loss: 0.9845564365386963\n",
            "Epoch 130 train loss: 0.6419135928153992 test loss: 0.64439857006073\n",
            "Epoch 130 train loss: 1.2125909328460693 test loss: 1.2053354978561401\n",
            "Epoch 130 train loss: 1.03360915184021 test loss: 1.039609670639038\n",
            "Epoch 130 train loss: 0.39374294877052307 test loss: 0.39947086572647095\n",
            "Epoch 130 train loss: 0.5715742707252502 test loss: 0.569632351398468\n",
            "Epoch 130 train loss: 0.504716157913208 test loss: 0.5091372132301331\n",
            "Epoch 130 train loss: 0.5204874277114868 test loss: 0.5179687738418579\n",
            "Epoch 130 train loss: 0.4161675274372101 test loss: 0.4207150638103485\n",
            "Epoch 130 train loss: 0.3363839387893677 test loss: 0.3328903019428253\n",
            "Epoch 130 train loss: 0.2277604192495346 test loss: 0.22457881271839142\n",
            "Epoch 130 train loss: 0.47198235988616943 test loss: 0.4722501337528229\n",
            "Epoch 130 train loss: 0.5736237168312073 test loss: 0.5812843441963196\n",
            "Epoch 130 train loss: 0.21887794137001038 test loss: 0.21513868868350983\n",
            "Epoch 130 train loss: 0.409006267786026 test loss: 0.40472543239593506\n",
            "Epoch 130 train loss: 0.5361529588699341 test loss: 0.5326792001724243\n",
            "Epoch 130 train loss: 0.15086807310581207 test loss: 0.15226073563098907\n",
            "Epoch 130 train loss: 1.3628443479537964 test loss: 1.3764673471450806\n",
            "Epoch 130 train loss: 0.5704545974731445 test loss: 0.566480278968811\n",
            "Epoch 130 train loss: 0.43194088339805603 test loss: 0.42678889632225037\n",
            "Epoch 130 train loss: 0.40251556038856506 test loss: 0.40592220425605774\n",
            "Epoch 130 train loss: 0.6255097985267639 test loss: 0.6302742958068848\n",
            "Epoch 130 train loss: 0.7242212295532227 test loss: 0.727864146232605\n",
            "Epoch 130 train loss: 0.2514159083366394 test loss: 0.25192758440971375\n",
            "Epoch 130 train loss: 0.4116104543209076 test loss: 0.4141407608985901\n",
            "Epoch 130 train loss: 0.16856537759304047 test loss: 0.1716528683900833\n",
            "Epoch 130 train loss: 0.560225784778595 test loss: 0.5545989274978638\n",
            "Epoch 130 train loss: 0.32456108927726746 test loss: 0.3254490792751312\n",
            "Epoch 130 train loss: 0.23185251653194427 test loss: 0.22826173901557922\n",
            "Epoch 130 train loss: 0.31329792737960815 test loss: 0.3184918463230133\n",
            "Epoch 130 train loss: 0.18976718187332153 test loss: 0.1918550282716751\n",
            "Epoch 130 train loss: 0.256205677986145 test loss: 0.2534083425998688\n",
            "Epoch 130 train loss: 0.3626936078071594 test loss: 0.3658170700073242\n",
            "Epoch 130 train loss: 0.8252143859863281 test loss: 0.8225129842758179\n",
            "Epoch 140 train loss: 0.6267863512039185 test loss: 0.6269448399543762\n",
            "Epoch 140 train loss: 0.5077455639839172 test loss: 0.5057584047317505\n",
            "Epoch 140 train loss: 1.1864122152328491 test loss: 1.1903672218322754\n",
            "Epoch 140 train loss: 0.3568534851074219 test loss: 0.36234959959983826\n",
            "Epoch 140 train loss: 0.9218510985374451 test loss: 0.9362283945083618\n",
            "Epoch 140 train loss: 0.40560808777809143 test loss: 0.4125913381576538\n",
            "Epoch 140 train loss: 0.32949548959732056 test loss: 0.3306318521499634\n",
            "Epoch 140 train loss: 1.24833083152771 test loss: 1.2464960813522339\n",
            "Epoch 140 train loss: 0.4939483106136322 test loss: 0.4883434474468231\n",
            "Epoch 140 train loss: 0.26388347148895264 test loss: 0.2716309428215027\n",
            "Epoch 140 train loss: 0.5209465622901917 test loss: 0.5193052291870117\n",
            "Epoch 140 train loss: 0.9040536284446716 test loss: 0.8982478976249695\n",
            "Epoch 140 train loss: 0.8953761458396912 test loss: 0.8918249607086182\n",
            "Epoch 140 train loss: 0.5082152485847473 test loss: 0.49887287616729736\n",
            "Epoch 140 train loss: 0.37891149520874023 test loss: 0.3990941345691681\n",
            "Epoch 140 train loss: 0.21120814979076385 test loss: 0.21492743492126465\n",
            "Epoch 140 train loss: 0.8336631059646606 test loss: 0.8222906589508057\n",
            "Epoch 140 train loss: 0.5502927899360657 test loss: 0.549047589302063\n",
            "Epoch 140 train loss: 1.1187856197357178 test loss: 1.131372094154358\n",
            "Epoch 140 train loss: 0.6729596257209778 test loss: 0.6689426302909851\n",
            "Epoch 140 train loss: 0.43012428283691406 test loss: 0.43409693241119385\n",
            "Epoch 140 train loss: 0.6442174315452576 test loss: 0.6563910245895386\n",
            "Epoch 140 train loss: 0.3909633755683899 test loss: 0.39297226071357727\n",
            "Epoch 140 train loss: 0.5455353260040283 test loss: 0.5421749949455261\n",
            "Epoch 140 train loss: 0.3939039409160614 test loss: 0.3973855674266815\n",
            "Epoch 140 train loss: 0.3191465437412262 test loss: 0.31857192516326904\n",
            "Epoch 140 train loss: 0.2626681625843048 test loss: 0.2563231289386749\n",
            "Epoch 140 train loss: 0.41843026876449585 test loss: 0.4164426326751709\n",
            "Epoch 140 train loss: 0.552929162979126 test loss: 0.58290696144104\n",
            "Epoch 140 train loss: 0.17460383474826813 test loss: 0.17840290069580078\n",
            "Epoch 140 train loss: 0.43075186014175415 test loss: 0.42547568678855896\n",
            "Epoch 140 train loss: 0.6573095321655273 test loss: 0.6448982357978821\n",
            "Epoch 140 train loss: 0.15047232806682587 test loss: 0.15302523970603943\n",
            "Epoch 140 train loss: 1.242385745048523 test loss: 1.2414071559906006\n",
            "Epoch 140 train loss: 0.5336981415748596 test loss: 0.5016513466835022\n",
            "Epoch 140 train loss: 0.4376782476902008 test loss: 0.4437248110771179\n",
            "Epoch 140 train loss: 0.4088139832019806 test loss: 0.4086991250514984\n",
            "Epoch 140 train loss: 0.7107496857643127 test loss: 0.7215405106544495\n",
            "Epoch 140 train loss: 0.5880077481269836 test loss: 0.584118127822876\n",
            "Epoch 140 train loss: 0.14486221969127655 test loss: 0.14340977370738983\n",
            "Epoch 140 train loss: 0.375418096780777 test loss: 0.3731016218662262\n",
            "Epoch 140 train loss: 0.19255153834819794 test loss: 0.1889979988336563\n",
            "Epoch 140 train loss: 0.49314603209495544 test loss: 0.48819366097450256\n",
            "Epoch 140 train loss: 0.44355544447898865 test loss: 0.44044703245162964\n",
            "Epoch 140 train loss: 0.3053855299949646 test loss: 0.30270814895629883\n",
            "Epoch 140 train loss: 0.2734277844429016 test loss: 0.27932441234588623\n",
            "Epoch 140 train loss: 0.15118396282196045 test loss: 0.1491037756204605\n",
            "Epoch 140 train loss: 0.12005086243152618 test loss: 0.12218203395605087\n",
            "Epoch 140 train loss: 0.29719844460487366 test loss: 0.2928328514099121\n",
            "Epoch 140 train loss: 0.6760705709457397 test loss: 0.67478346824646\n",
            "Epoch 150 train loss: 0.6639082431793213 test loss: 0.6604492664337158\n",
            "Epoch 150 train loss: 0.4131512939929962 test loss: 0.406431645154953\n",
            "Epoch 150 train loss: 1.1067705154418945 test loss: 1.1089614629745483\n",
            "Epoch 150 train loss: 0.4986579716205597 test loss: 0.5060042142868042\n",
            "Epoch 150 train loss: 0.7156550884246826 test loss: 0.7252044081687927\n",
            "Epoch 150 train loss: 0.6028432846069336 test loss: 0.5929726362228394\n",
            "Epoch 150 train loss: 0.5274919867515564 test loss: 0.5283452272415161\n",
            "Epoch 150 train loss: 0.962090015411377 test loss: 0.9541582465171814\n",
            "Epoch 150 train loss: 0.5411694645881653 test loss: 0.5361423492431641\n",
            "Epoch 150 train loss: 0.3433884382247925 test loss: 0.3460034728050232\n",
            "Epoch 150 train loss: 0.42345693707466125 test loss: 0.43021145462989807\n",
            "Epoch 150 train loss: 0.8754935264587402 test loss: 0.8691074252128601\n",
            "Epoch 150 train loss: 0.8963455557823181 test loss: 0.8935750722885132\n",
            "Epoch 150 train loss: 0.2805282771587372 test loss: 0.2793755829334259\n",
            "Epoch 150 train loss: 0.39049848914146423 test loss: 0.4035639464855194\n",
            "Epoch 150 train loss: 0.3522677719593048 test loss: 0.3494194447994232\n",
            "Epoch 150 train loss: 0.6491532325744629 test loss: 0.6497095227241516\n",
            "Epoch 150 train loss: 0.5550343990325928 test loss: 0.5511727929115295\n",
            "Epoch 150 train loss: 1.094638466835022 test loss: 1.0923404693603516\n",
            "Epoch 150 train loss: 0.6857916712760925 test loss: 0.6805729269981384\n",
            "Epoch 150 train loss: 0.4391157627105713 test loss: 0.44321945309638977\n",
            "Epoch 150 train loss: 0.6028944849967957 test loss: 0.6022576689720154\n",
            "Epoch 150 train loss: 0.386694073677063 test loss: 0.39137402176856995\n",
            "Epoch 150 train loss: 0.5255418419837952 test loss: 0.520482063293457\n",
            "Epoch 150 train loss: 0.43375301361083984 test loss: 0.43587538599967957\n",
            "Epoch 150 train loss: 0.324721097946167 test loss: 0.3228922188282013\n",
            "Epoch 150 train loss: 0.282419353723526 test loss: 0.2739916443824768\n",
            "Epoch 150 train loss: 0.41561535000801086 test loss: 0.41056299209594727\n",
            "Epoch 150 train loss: 0.6288889646530151 test loss: 0.6554796099662781\n",
            "Epoch 150 train loss: 0.11145291477441788 test loss: 0.10645604133605957\n",
            "Epoch 150 train loss: 0.4424745738506317 test loss: 0.4417998790740967\n",
            "Epoch 150 train loss: 0.5745410323143005 test loss: 0.5740483403205872\n",
            "Epoch 150 train loss: 0.18451038002967834 test loss: 0.1837558001279831\n",
            "Epoch 150 train loss: 0.8755911588668823 test loss: 0.8871546387672424\n",
            "Epoch 150 train loss: 0.6235806345939636 test loss: 0.5998749136924744\n",
            "Epoch 150 train loss: 0.28443852066993713 test loss: 0.28404781222343445\n",
            "Epoch 150 train loss: 0.3189235329627991 test loss: 0.3173977732658386\n",
            "Epoch 150 train loss: 0.5113921165466309 test loss: 0.5084711909294128\n",
            "Epoch 150 train loss: 0.5112317800521851 test loss: 0.5052052736282349\n",
            "Epoch 150 train loss: 0.10885580629110336 test loss: 0.10674146562814713\n",
            "Epoch 150 train loss: 0.25482791662216187 test loss: 0.24817728996276855\n",
            "Epoch 150 train loss: 0.1722889542579651 test loss: 0.17242898046970367\n",
            "Epoch 150 train loss: 0.5196533203125 test loss: 0.5158457159996033\n",
            "Epoch 150 train loss: 0.5164690613746643 test loss: 0.5072742104530334\n",
            "Epoch 150 train loss: 0.39786118268966675 test loss: 0.3920494019985199\n",
            "Epoch 150 train loss: 0.3371639549732208 test loss: 0.339405357837677\n",
            "Epoch 150 train loss: 0.1097576767206192 test loss: 0.11379146575927734\n",
            "Epoch 150 train loss: 0.09576711058616638 test loss: 0.0955650806427002\n",
            "Epoch 150 train loss: 0.21594394743442535 test loss: 0.2202776074409485\n",
            "Epoch 150 train loss: 0.5361578464508057 test loss: 0.5374934673309326\n",
            "Epoch 160 train loss: 0.6681972742080688 test loss: 0.6820217967033386\n",
            "Epoch 160 train loss: 0.36462196707725525 test loss: 0.3678593933582306\n",
            "Epoch 160 train loss: 1.2764793634414673 test loss: 1.2660638093948364\n",
            "Epoch 160 train loss: 0.34221944212913513 test loss: 0.3442695438861847\n",
            "Epoch 160 train loss: 0.8137921094894409 test loss: 0.8189218044281006\n",
            "Epoch 160 train loss: 0.44453683495521545 test loss: 0.43777018785476685\n",
            "Epoch 160 train loss: 0.196541890501976 test loss: 0.1932717263698578\n",
            "Epoch 160 train loss: 1.1609104871749878 test loss: 1.1700257062911987\n",
            "Epoch 160 train loss: 0.48290106654167175 test loss: 0.49305087327957153\n",
            "Epoch 160 train loss: 0.24567700922489166 test loss: 0.24872559309005737\n",
            "Epoch 160 train loss: 0.40325483679771423 test loss: 0.4010232388973236\n",
            "Epoch 160 train loss: 0.8602380752563477 test loss: 0.843636155128479\n",
            "Epoch 160 train loss: 0.9647069573402405 test loss: 0.957239031791687\n",
            "Epoch 160 train loss: 0.45504626631736755 test loss: 0.45647045969963074\n",
            "Epoch 160 train loss: 0.3152223825454712 test loss: 0.3287190794944763\n",
            "Epoch 160 train loss: 0.2868131995201111 test loss: 0.2877732813358307\n",
            "Epoch 160 train loss: 0.5073907375335693 test loss: 0.5086908340454102\n",
            "Epoch 160 train loss: 0.506976842880249 test loss: 0.5149021148681641\n",
            "Epoch 160 train loss: 1.078523874282837 test loss: 1.0790064334869385\n",
            "Epoch 160 train loss: 0.8575830459594727 test loss: 0.8624787926673889\n",
            "Epoch 160 train loss: 0.3371724784374237 test loss: 0.34121185541152954\n",
            "Epoch 160 train loss: 0.6108494400978088 test loss: 0.6145197153091431\n",
            "Epoch 160 train loss: 0.4166547954082489 test loss: 0.41391071677207947\n",
            "Epoch 160 train loss: 0.47657111287117004 test loss: 0.47556960582733154\n",
            "Epoch 160 train loss: 0.39530080556869507 test loss: 0.3929870128631592\n",
            "Epoch 160 train loss: 0.2875129282474518 test loss: 0.28929322957992554\n",
            "Epoch 160 train loss: 0.16017478704452515 test loss: 0.1622115522623062\n",
            "Epoch 160 train loss: 0.276748389005661 test loss: 0.2766992449760437\n",
            "Epoch 160 train loss: 0.6134393811225891 test loss: 0.6180819869041443\n",
            "Epoch 160 train loss: 0.09427475184202194 test loss: 0.09591898322105408\n",
            "Epoch 160 train loss: 0.4578302800655365 test loss: 0.44880127906799316\n",
            "Epoch 160 train loss: 0.5731120705604553 test loss: 0.5672574639320374\n",
            "Epoch 160 train loss: 0.13695931434631348 test loss: 0.13777530193328857\n",
            "Epoch 160 train loss: 0.8515521883964539 test loss: 0.844890296459198\n",
            "Epoch 160 train loss: 0.4165036976337433 test loss: 0.42932167649269104\n",
            "Epoch 160 train loss: 0.25625479221343994 test loss: 0.255460649728775\n",
            "Epoch 160 train loss: 0.3073161840438843 test loss: 0.31179821491241455\n",
            "Epoch 160 train loss: 0.535474956035614 test loss: 0.5301888585090637\n",
            "Epoch 160 train loss: 0.4772547781467438 test loss: 0.47458958625793457\n",
            "Epoch 160 train loss: 0.08366545289754868 test loss: 0.08482520282268524\n",
            "Epoch 160 train loss: 0.24075064063072205 test loss: 0.2415018379688263\n",
            "Epoch 160 train loss: 0.20208433270454407 test loss: 0.20083379745483398\n",
            "Epoch 160 train loss: 0.4282657206058502 test loss: 0.4350340664386749\n",
            "Epoch 160 train loss: 0.37587249279022217 test loss: 0.37476086616516113\n",
            "Epoch 160 train loss: 0.27471473813056946 test loss: 0.2744130492210388\n",
            "Epoch 160 train loss: 0.21256445348262787 test loss: 0.21697525680065155\n",
            "Epoch 160 train loss: 0.050073519349098206 test loss: 0.04914790391921997\n",
            "Epoch 160 train loss: 0.10930769890546799 test loss: 0.10964640974998474\n",
            "Epoch 160 train loss: 0.22658771276474 test loss: 0.2270137369632721\n",
            "Epoch 160 train loss: 0.5171582698822021 test loss: 0.5213265419006348\n",
            "Epoch 170 train loss: 1.4260120391845703 test loss: 1.4187744855880737\n",
            "Epoch 170 train loss: 0.5879948735237122 test loss: 0.5864519476890564\n",
            "Epoch 170 train loss: 1.1460354328155518 test loss: 1.13376784324646\n",
            "Epoch 170 train loss: 0.37861168384552 test loss: 0.38716933131217957\n",
            "Epoch 170 train loss: 0.7314430475234985 test loss: 0.7362256050109863\n",
            "Epoch 170 train loss: 0.547507643699646 test loss: 0.5428891181945801\n",
            "Epoch 170 train loss: 0.2632366120815277 test loss: 0.2655693292617798\n",
            "Epoch 170 train loss: 1.2346105575561523 test loss: 1.2339584827423096\n",
            "Epoch 170 train loss: 0.5185073018074036 test loss: 0.522255003452301\n",
            "Epoch 170 train loss: 0.3041307032108307 test loss: 0.31319087743759155\n",
            "Epoch 170 train loss: 0.4230455458164215 test loss: 0.42765381932258606\n",
            "Epoch 170 train loss: 0.8675969243049622 test loss: 0.8691186904907227\n",
            "Epoch 170 train loss: 1.118134617805481 test loss: 1.1241329908370972\n",
            "Epoch 170 train loss: 0.5605652928352356 test loss: 0.555494487285614\n",
            "Epoch 170 train loss: 0.5097708702087402 test loss: 0.5408430695533752\n",
            "Epoch 170 train loss: 0.5379810929298401 test loss: 0.5422120690345764\n",
            "Epoch 170 train loss: 0.9174532890319824 test loss: 0.913605272769928\n",
            "Epoch 170 train loss: 0.630432665348053 test loss: 0.6342723369598389\n",
            "Epoch 170 train loss: 1.0826423168182373 test loss: 1.0892088413238525\n",
            "Epoch 170 train loss: 0.6607552766799927 test loss: 0.6696394085884094\n",
            "Epoch 170 train loss: 0.9617876410484314 test loss: 0.9534976482391357\n",
            "Epoch 170 train loss: 0.9990016222000122 test loss: 1.0050591230392456\n",
            "Epoch 170 train loss: 0.8718053102493286 test loss: 0.8659581542015076\n",
            "Epoch 170 train loss: 0.9638441801071167 test loss: 0.9618272185325623\n",
            "Epoch 170 train loss: 1.2253905534744263 test loss: 1.2243999242782593\n",
            "Epoch 170 train loss: 0.671628475189209 test loss: 0.6675958633422852\n",
            "Epoch 170 train loss: 0.21424047648906708 test loss: 0.21395307779312134\n",
            "Epoch 170 train loss: 1.044179081916809 test loss: 1.0427680015563965\n",
            "Epoch 170 train loss: 0.5975735187530518 test loss: 0.6519902944564819\n",
            "Epoch 170 train loss: 0.259311705827713 test loss: 0.262081503868103\n",
            "Epoch 170 train loss: 0.6900607347488403 test loss: 0.6892834901809692\n",
            "Epoch 170 train loss: 1.1306232213974 test loss: 1.1429777145385742\n",
            "Epoch 170 train loss: 0.15403467416763306 test loss: 0.15559084713459015\n",
            "Epoch 170 train loss: 0.8571226000785828 test loss: 0.8485509157180786\n",
            "Epoch 170 train loss: 0.4340687394142151 test loss: 0.441361665725708\n",
            "Epoch 170 train loss: 0.34147998690605164 test loss: 0.3425416946411133\n",
            "Epoch 170 train loss: 0.566192090511322 test loss: 0.5646054744720459\n",
            "Epoch 170 train loss: 0.4787679612636566 test loss: 0.48077717423439026\n",
            "Epoch 170 train loss: 0.6526261568069458 test loss: 0.6537120342254639\n",
            "Epoch 170 train loss: 0.2786121368408203 test loss: 0.2718046307563782\n",
            "Epoch 170 train loss: 0.6605319380760193 test loss: 0.657716691493988\n",
            "Epoch 170 train loss: 0.2752044200897217 test loss: 0.27233418822288513\n",
            "Epoch 170 train loss: 0.6722930073738098 test loss: 0.6642304062843323\n",
            "Epoch 170 train loss: 0.44964417815208435 test loss: 0.452837735414505\n",
            "Epoch 170 train loss: 0.62794029712677 test loss: 0.6079046130180359\n",
            "Epoch 170 train loss: 0.9524282217025757 test loss: 0.9447630643844604\n",
            "Epoch 170 train loss: 0.10017892718315125 test loss: 0.10196356475353241\n",
            "Epoch 170 train loss: 0.16902610659599304 test loss: 0.16672098636627197\n",
            "Epoch 170 train loss: 0.5631406307220459 test loss: 0.55797278881073\n",
            "Epoch 170 train loss: 0.5054231882095337 test loss: 0.5058207511901855\n",
            "Epoch 180 train loss: 0.7904139757156372 test loss: 0.7899845242500305\n",
            "Epoch 180 train loss: 0.28869006037712097 test loss: 0.27765971422195435\n",
            "Epoch 180 train loss: 0.6715057492256165 test loss: 0.6554754972457886\n",
            "Epoch 180 train loss: 0.3796413242816925 test loss: 0.38108503818511963\n",
            "Epoch 180 train loss: 0.7543384432792664 test loss: 0.7596408128738403\n",
            "Epoch 180 train loss: 0.3813553750514984 test loss: 0.38489875197410583\n",
            "Epoch 180 train loss: 0.1944473385810852 test loss: 0.19181059300899506\n",
            "Epoch 180 train loss: 0.9840985536575317 test loss: 0.9932020306587219\n",
            "Epoch 180 train loss: 0.45875391364097595 test loss: 0.4781981110572815\n",
            "Epoch 180 train loss: 0.15949295461177826 test loss: 0.16308926045894623\n",
            "Epoch 180 train loss: 0.36116376519203186 test loss: 0.37256988883018494\n",
            "Epoch 180 train loss: 0.7065472602844238 test loss: 0.6928844451904297\n",
            "Epoch 180 train loss: 0.8010809421539307 test loss: 0.8050680756568909\n",
            "Epoch 180 train loss: 0.5258561372756958 test loss: 0.5237811207771301\n",
            "Epoch 180 train loss: 0.2589935064315796 test loss: 0.2755938768386841\n",
            "Epoch 180 train loss: 0.20111140608787537 test loss: 0.19774484634399414\n",
            "Epoch 180 train loss: 0.44017845392227173 test loss: 0.4448445737361908\n",
            "Epoch 180 train loss: 0.4599843919277191 test loss: 0.45454561710357666\n",
            "Epoch 180 train loss: 0.97452312707901 test loss: 0.9817641973495483\n",
            "Epoch 180 train loss: 0.730474591255188 test loss: 0.7440985441207886\n",
            "Epoch 180 train loss: 0.30539625883102417 test loss: 0.3086226284503937\n",
            "Epoch 180 train loss: 0.649929404258728 test loss: 0.6555802226066589\n",
            "Epoch 180 train loss: 0.4287649393081665 test loss: 0.44205358624458313\n",
            "Epoch 180 train loss: 0.4830740690231323 test loss: 0.48145434260368347\n",
            "Epoch 180 train loss: 0.3091124892234802 test loss: 0.3162757158279419\n",
            "Epoch 180 train loss: 0.26081594824790955 test loss: 0.26390761137008667\n",
            "Epoch 180 train loss: 0.16963328421115875 test loss: 0.1680343747138977\n",
            "Epoch 180 train loss: 0.30532246828079224 test loss: 0.2994772493839264\n",
            "Epoch 180 train loss: 0.5475544929504395 test loss: 0.5670070052146912\n",
            "Epoch 180 train loss: 0.06886634230613708 test loss: 0.07135283946990967\n",
            "Epoch 180 train loss: 0.484230101108551 test loss: 0.4874708950519562\n",
            "Epoch 180 train loss: 0.7580665349960327 test loss: 0.740107536315918\n",
            "Epoch 180 train loss: 0.10617608577013016 test loss: 0.10300146788358688\n",
            "Epoch 180 train loss: 0.6685859560966492 test loss: 0.6764256358146667\n",
            "Epoch 180 train loss: 0.36958327889442444 test loss: 0.3754483759403229\n",
            "Epoch 180 train loss: 0.2083115428686142 test loss: 0.20556281507015228\n",
            "Epoch 180 train loss: 0.3166375160217285 test loss: 0.3100081980228424\n",
            "Epoch 180 train loss: 0.5535759329795837 test loss: 0.5460006594657898\n",
            "Epoch 180 train loss: 0.3749626576900482 test loss: 0.3771202564239502\n",
            "Epoch 180 train loss: 0.0661826804280281 test loss: 0.07379139214754105\n",
            "Epoch 180 train loss: 0.26756829023361206 test loss: 0.26952388882637024\n",
            "Epoch 180 train loss: 0.23706947267055511 test loss: 0.2379900962114334\n",
            "Epoch 180 train loss: 0.4489991068840027 test loss: 0.4429076015949249\n",
            "Epoch 180 train loss: 0.37953755259513855 test loss: 0.3888229429721832\n",
            "Epoch 180 train loss: 0.16742298007011414 test loss: 0.16439010202884674\n",
            "Epoch 180 train loss: 0.2305217832326889 test loss: 0.2275640219449997\n",
            "Epoch 180 train loss: 0.08866959065198898 test loss: 0.08548801392316818\n",
            "Epoch 180 train loss: 0.19490987062454224 test loss: 0.19770710170269012\n",
            "Epoch 180 train loss: 0.44031816720962524 test loss: 0.43701738119125366\n",
            "Epoch 180 train loss: 0.724820613861084 test loss: 0.7166969180107117\n",
            "Epoch 190 train loss: 0.5730726718902588 test loss: 0.5694561004638672\n",
            "Epoch 190 train loss: 0.33541157841682434 test loss: 0.3333980143070221\n",
            "Epoch 190 train loss: 0.6301929950714111 test loss: 0.6336348056793213\n",
            "Epoch 190 train loss: 0.4669574797153473 test loss: 0.47311824560165405\n",
            "Epoch 190 train loss: 0.9889388680458069 test loss: 0.9833317995071411\n",
            "Epoch 190 train loss: 0.2837965786457062 test loss: 0.2733779549598694\n",
            "Epoch 190 train loss: 0.2016644924879074 test loss: 0.20344693958759308\n",
            "Epoch 190 train loss: 1.1464817523956299 test loss: 1.1489739418029785\n",
            "Epoch 190 train loss: 0.4911140501499176 test loss: 0.4571200907230377\n",
            "Epoch 190 train loss: 0.2267100065946579 test loss: 0.21814878284931183\n",
            "Epoch 190 train loss: 0.2534348964691162 test loss: 0.24848508834838867\n",
            "Epoch 190 train loss: 0.6324254870414734 test loss: 0.6243059635162354\n",
            "Epoch 190 train loss: 0.9778949618339539 test loss: 1.0109165906906128\n",
            "Epoch 190 train loss: 0.05893957242369652 test loss: 0.06092260405421257\n",
            "Epoch 190 train loss: 0.36930420994758606 test loss: 0.38988277316093445\n",
            "Epoch 190 train loss: 0.27166321873664856 test loss: 0.27472352981567383\n",
            "Epoch 190 train loss: 0.3872770667076111 test loss: 0.38663649559020996\n",
            "Epoch 190 train loss: 0.47945111989974976 test loss: 0.4800716042518616\n",
            "Epoch 190 train loss: 1.0444313287734985 test loss: 1.0448887348175049\n",
            "Epoch 190 train loss: 0.8357298970222473 test loss: 0.8402955532073975\n",
            "Epoch 190 train loss: 0.31683388352394104 test loss: 0.31277596950531006\n",
            "Epoch 190 train loss: 0.5401626825332642 test loss: 0.555057942867279\n",
            "Epoch 190 train loss: 0.40523114800453186 test loss: 0.4011467695236206\n",
            "Epoch 190 train loss: 0.48690420389175415 test loss: 0.4812914729118347\n",
            "Epoch 190 train loss: 0.3477676808834076 test loss: 0.34016871452331543\n",
            "Epoch 190 train loss: 0.3153601884841919 test loss: 0.3167963922023773\n",
            "Epoch 190 train loss: 0.13769742846488953 test loss: 0.13278619945049286\n",
            "Epoch 190 train loss: 0.24018055200576782 test loss: 0.24006080627441406\n",
            "Epoch 190 train loss: 0.5881422758102417 test loss: 0.6293932199478149\n",
            "Epoch 190 train loss: 0.15293866395950317 test loss: 0.157932847738266\n",
            "Epoch 190 train loss: 0.5576022267341614 test loss: 0.5562582015991211\n",
            "Epoch 190 train loss: 0.48492592573165894 test loss: 0.49129828810691833\n",
            "Epoch 190 train loss: 0.11495678126811981 test loss: 0.11545050144195557\n",
            "Epoch 190 train loss: 0.633901059627533 test loss: 0.6416969299316406\n",
            "Epoch 190 train loss: 0.5799877047538757 test loss: 0.5773032307624817\n",
            "Epoch 190 train loss: 0.3063283860683441 test loss: 0.3053414821624756\n",
            "Epoch 190 train loss: 0.36354225873947144 test loss: 0.36773642897605896\n",
            "Epoch 190 train loss: 0.6221042275428772 test loss: 0.626367449760437\n",
            "Epoch 190 train loss: 0.5152401328086853 test loss: 0.5142717361450195\n",
            "Epoch 190 train loss: 0.09332190454006195 test loss: 0.09294316172599792\n",
            "Epoch 190 train loss: 0.2541895806789398 test loss: 0.25177237391471863\n",
            "Epoch 190 train loss: 0.3159305453300476 test loss: 0.32703810930252075\n",
            "Epoch 190 train loss: 0.2684156000614166 test loss: 0.2747077941894531\n",
            "Epoch 190 train loss: 0.40863943099975586 test loss: 0.4053133726119995\n",
            "Epoch 190 train loss: 0.1744016855955124 test loss: 0.1745978593826294\n",
            "Epoch 190 train loss: 0.24181126058101654 test loss: 0.24227499961853027\n",
            "Epoch 190 train loss: 0.0519566535949707 test loss: 0.05253033712506294\n",
            "Epoch 190 train loss: 0.12961632013320923 test loss: 0.1258869618177414\n",
            "Epoch 190 train loss: 0.23595841228961945 test loss: 0.23525799810886383\n",
            "Epoch 190 train loss: 0.5293627977371216 test loss: 0.5295107960700989\n",
            "Epoch 200 train loss: 0.7405834197998047 test loss: 0.7338559627532959\n",
            "Epoch 200 train loss: 0.24945782124996185 test loss: 0.25296249985694885\n",
            "Epoch 200 train loss: 0.39395850896835327 test loss: 0.39413905143737793\n",
            "Epoch 200 train loss: 0.32336223125457764 test loss: 0.322867751121521\n",
            "Epoch 200 train loss: 0.7176806926727295 test loss: 0.7163515090942383\n",
            "Epoch 200 train loss: 0.35816490650177 test loss: 0.3628881275653839\n",
            "Epoch 200 train loss: 0.17580898106098175 test loss: 0.1745985746383667\n",
            "Epoch 200 train loss: 0.8717181086540222 test loss: 0.8685007691383362\n",
            "Epoch 200 train loss: 0.39743122458457947 test loss: 0.385921835899353\n",
            "Epoch 200 train loss: 0.2511872947216034 test loss: 0.24548383057117462\n",
            "Epoch 200 train loss: 0.2817245125770569 test loss: 0.28423354029655457\n",
            "Epoch 200 train loss: 0.6548946499824524 test loss: 0.6448630094528198\n",
            "Epoch 200 train loss: 0.657764196395874 test loss: 0.6587634086608887\n",
            "Epoch 200 train loss: 0.5280156135559082 test loss: 0.5268248319625854\n",
            "Epoch 200 train loss: 0.24250127375125885 test loss: 0.23444455862045288\n",
            "Epoch 200 train loss: 0.15728706121444702 test loss: 0.1605672389268875\n",
            "Epoch 200 train loss: 0.4790915548801422 test loss: 0.4842316508293152\n",
            "Epoch 200 train loss: 0.38931021094322205 test loss: 0.3948015868663788\n",
            "Epoch 200 train loss: 0.8761193752288818 test loss: 0.8735381364822388\n",
            "Epoch 200 train loss: 0.21053828299045563 test loss: 0.2098362147808075\n",
            "Epoch 200 train loss: 0.5132889747619629 test loss: 0.5173988342285156\n",
            "Epoch 200 train loss: 0.6201552748680115 test loss: 0.6138012409210205\n",
            "Epoch 200 train loss: 0.356734961271286 test loss: 0.3618565797805786\n",
            "Epoch 200 train loss: 0.7048490047454834 test loss: 0.7140039205551147\n",
            "Epoch 200 train loss: 0.30182284116744995 test loss: 0.30002832412719727\n",
            "Epoch 200 train loss: 0.41322603821754456 test loss: 0.4161253869533539\n",
            "Epoch 200 train loss: 0.10828892886638641 test loss: 0.10519196838140488\n",
            "Epoch 200 train loss: 0.5944466590881348 test loss: 0.6013327836990356\n",
            "Epoch 200 train loss: 0.5617311596870422 test loss: 0.5958665013313293\n",
            "Epoch 200 train loss: 0.0873308777809143 test loss: 0.08815361559391022\n",
            "Epoch 200 train loss: 0.3421177566051483 test loss: 0.3444804847240448\n",
            "Epoch 200 train loss: 0.547834038734436 test loss: 0.5454251170158386\n",
            "Epoch 200 train loss: 0.09075877070426941 test loss: 0.09178872406482697\n",
            "Epoch 200 train loss: 0.6734275221824646 test loss: 0.6698496341705322\n",
            "Epoch 200 train loss: 0.2765916585922241 test loss: 0.27586793899536133\n",
            "Epoch 200 train loss: 0.29621046781539917 test loss: 0.2968977689743042\n",
            "Epoch 200 train loss: 0.3124370872974396 test loss: 0.31369203329086304\n",
            "Epoch 200 train loss: 0.7492024302482605 test loss: 0.7595223784446716\n",
            "Epoch 200 train loss: 0.36748170852661133 test loss: 0.3674605190753937\n",
            "Epoch 200 train loss: 0.11070715636014938 test loss: 0.1095823347568512\n",
            "Epoch 200 train loss: 0.3919034004211426 test loss: 0.3934065103530884\n",
            "Epoch 200 train loss: 0.2557646632194519 test loss: 0.2581804394721985\n",
            "Epoch 200 train loss: 0.32433822751045227 test loss: 0.32593628764152527\n",
            "Epoch 200 train loss: 0.26714539527893066 test loss: 0.2673802673816681\n",
            "Epoch 200 train loss: 0.16989979147911072 test loss: 0.17340563237667084\n",
            "Epoch 200 train loss: 0.15167279541492462 test loss: 0.1510329395532608\n",
            "Epoch 200 train loss: 0.04483725503087044 test loss: 0.04766755923628807\n",
            "Epoch 200 train loss: 0.07804197818040848 test loss: 0.07708374410867691\n",
            "Epoch 200 train loss: 0.2700423002243042 test loss: 0.2784591317176819\n",
            "Epoch 200 train loss: 0.5402418971061707 test loss: 0.5406171083450317\n",
            "Saving model \n",
            "\n",
            "Done\n",
            "Epoch 210 train loss: 0.7037776112556458 test loss: 0.6980537176132202\n",
            "Epoch 210 train loss: 0.25647470355033875 test loss: 0.26003676652908325\n",
            "Epoch 210 train loss: 0.8407608270645142 test loss: 0.8168221712112427\n",
            "Epoch 210 train loss: 0.28689318895339966 test loss: 0.2941041886806488\n",
            "Epoch 210 train loss: 0.8065834045410156 test loss: 0.8093360662460327\n",
            "Epoch 210 train loss: 0.3143741488456726 test loss: 0.3283765912055969\n",
            "Epoch 210 train loss: 0.1676890105009079 test loss: 0.1695864349603653\n",
            "Epoch 210 train loss: 1.0885009765625 test loss: 1.0771912336349487\n",
            "Epoch 210 train loss: 0.3325554430484772 test loss: 0.3301793932914734\n",
            "Epoch 210 train loss: 0.14153577387332916 test loss: 0.14103811979293823\n",
            "Epoch 210 train loss: 0.192376971244812 test loss: 0.18936145305633545\n",
            "Epoch 210 train loss: 0.5948600172996521 test loss: 0.5926759243011475\n",
            "Epoch 210 train loss: 0.781622052192688 test loss: 0.7736899852752686\n",
            "Epoch 210 train loss: 0.3705858886241913 test loss: 0.37185147404670715\n",
            "Epoch 210 train loss: 0.23680928349494934 test loss: 0.23161821067333221\n",
            "Epoch 210 train loss: 0.19002659618854523 test loss: 0.19006198644638062\n",
            "Epoch 210 train loss: 1.7662907838821411 test loss: 1.7713006734848022\n",
            "Epoch 210 train loss: 0.35670551657676697 test loss: 0.35626912117004395\n",
            "Epoch 210 train loss: 1.6959046125411987 test loss: 1.6967980861663818\n",
            "Epoch 210 train loss: 1.1056960821151733 test loss: 1.1187400817871094\n",
            "Epoch 210 train loss: 0.3517628312110901 test loss: 0.35325348377227783\n",
            "Epoch 210 train loss: 0.5675652027130127 test loss: 0.5777989029884338\n",
            "Epoch 210 train loss: 0.42533040046691895 test loss: 0.4233328402042389\n",
            "Epoch 210 train loss: 0.48601192235946655 test loss: 0.4829694926738739\n",
            "Epoch 210 train loss: 0.23868638277053833 test loss: 0.23957593739032745\n",
            "Epoch 210 train loss: 0.33127665519714355 test loss: 0.32965201139450073\n",
            "Epoch 210 train loss: 0.12570393085479736 test loss: 0.12597690522670746\n",
            "Epoch 210 train loss: 0.2226206213235855 test loss: 0.22034186124801636\n",
            "Epoch 210 train loss: 0.6261618733406067 test loss: 0.6686546802520752\n",
            "Epoch 210 train loss: 0.10677755624055862 test loss: 0.10667818039655685\n",
            "Epoch 210 train loss: 0.4343595504760742 test loss: 0.4305526912212372\n",
            "Epoch 210 train loss: 0.5285096168518066 test loss: 0.527235746383667\n",
            "Epoch 210 train loss: 0.11674254387617111 test loss: 0.11406413465738297\n",
            "Epoch 210 train loss: 0.6348114609718323 test loss: 0.636296808719635\n",
            "Epoch 210 train loss: 0.3076930642127991 test loss: 0.30146950483322144\n",
            "Epoch 210 train loss: 0.31884220242500305 test loss: 0.3228168189525604\n",
            "Epoch 210 train loss: 0.3812861442565918 test loss: 0.38305097818374634\n",
            "Epoch 210 train loss: 0.5753216743469238 test loss: 0.5707184076309204\n",
            "Epoch 210 train loss: 0.40698081254959106 test loss: 0.40474504232406616\n",
            "Epoch 210 train loss: 0.07169739156961441 test loss: 0.07281870394945145\n",
            "Epoch 210 train loss: 0.24487420916557312 test loss: 0.24939289689064026\n",
            "Epoch 210 train loss: 0.25295326113700867 test loss: 0.2570476830005646\n",
            "Epoch 210 train loss: 0.2786692678928375 test loss: 0.2779463231563568\n",
            "Epoch 210 train loss: 0.27589359879493713 test loss: 0.27768445014953613\n",
            "Epoch 210 train loss: 0.15794110298156738 test loss: 0.16195060312747955\n",
            "Epoch 210 train loss: 0.1471046805381775 test loss: 0.144785076379776\n",
            "Epoch 210 train loss: 0.09458202123641968 test loss: 0.08654017746448517\n",
            "Epoch 210 train loss: 0.07477378845214844 test loss: 0.07348467409610748\n",
            "Epoch 210 train loss: 0.20819565653800964 test loss: 0.20415085554122925\n",
            "Epoch 210 train loss: 0.5293214321136475 test loss: 0.5255430936813354\n",
            "Epoch 220 train loss: 0.7188279032707214 test loss: 0.7177246809005737\n",
            "Epoch 220 train loss: 0.18981976807117462 test loss: 0.1875220090150833\n",
            "Epoch 220 train loss: 0.5795859694480896 test loss: 0.5861750245094299\n",
            "Epoch 220 train loss: 0.26815366744995117 test loss: 0.26044532656669617\n",
            "Epoch 220 train loss: 0.7947276830673218 test loss: 0.7940517067909241\n",
            "Epoch 220 train loss: 0.29193684458732605 test loss: 0.2975955009460449\n",
            "Epoch 220 train loss: 0.1540912687778473 test loss: 0.15098607540130615\n",
            "Epoch 220 train loss: 0.9202623963356018 test loss: 0.9299360513687134\n",
            "Epoch 220 train loss: 0.30095747113227844 test loss: 0.29956749081611633\n",
            "Epoch 220 train loss: 0.2500661015510559 test loss: 0.2568556070327759\n",
            "Epoch 220 train loss: 0.2294904589653015 test loss: 0.23255932331085205\n",
            "Epoch 220 train loss: 0.6465162634849548 test loss: 0.6231880784034729\n",
            "Epoch 220 train loss: 1.1386536359786987 test loss: 1.1884866952896118\n",
            "Epoch 220 train loss: 0.11688049882650375 test loss: 0.1183689683675766\n",
            "Epoch 220 train loss: 0.2715757191181183 test loss: 0.25197872519493103\n",
            "Epoch 220 train loss: 0.1410912424325943 test loss: 0.14348122477531433\n",
            "Epoch 220 train loss: 0.9567044377326965 test loss: 0.955455482006073\n",
            "Epoch 220 train loss: 0.43815574049949646 test loss: 0.4414702355861664\n",
            "Epoch 220 train loss: 1.3253144025802612 test loss: 1.337852120399475\n",
            "Epoch 220 train loss: 1.1808098554611206 test loss: 1.182037115097046\n",
            "Epoch 220 train loss: 0.26219624280929565 test loss: 0.2527007758617401\n",
            "Epoch 220 train loss: 0.6521102786064148 test loss: 0.6519986987113953\n",
            "Epoch 220 train loss: 0.35387158393859863 test loss: 0.3744141161441803\n",
            "Epoch 220 train loss: 0.46667352318763733 test loss: 0.46146973967552185\n",
            "Epoch 220 train loss: 0.9881205558776855 test loss: 0.9869323968887329\n",
            "Epoch 220 train loss: 0.3087022304534912 test loss: 0.302757203578949\n",
            "Epoch 220 train loss: 0.15485252439975739 test loss: 0.15402346849441528\n",
            "Epoch 220 train loss: 0.23262721300125122 test loss: 0.24127189815044403\n",
            "Epoch 220 train loss: 0.5003169178962708 test loss: 0.5143484473228455\n",
            "Epoch 220 train loss: 0.1657259464263916 test loss: 0.161357581615448\n",
            "Epoch 220 train loss: 0.41797274351119995 test loss: 0.40763387084007263\n",
            "Epoch 220 train loss: 0.8369676470756531 test loss: 0.8391864895820618\n",
            "Epoch 220 train loss: 0.09118138998746872 test loss: 0.09369264543056488\n",
            "Epoch 220 train loss: 0.9828615188598633 test loss: 0.9569069147109985\n",
            "Epoch 220 train loss: 0.3692988455295563 test loss: 0.3779776692390442\n",
            "Epoch 220 train loss: 0.21418415009975433 test loss: 0.2163754254579544\n",
            "Epoch 220 train loss: 0.3603537380695343 test loss: 0.3605554401874542\n",
            "Epoch 220 train loss: 0.6291430592536926 test loss: 0.6330687403678894\n",
            "Epoch 220 train loss: 0.4072749614715576 test loss: 0.40270861983299255\n",
            "Epoch 220 train loss: 0.11639348417520523 test loss: 0.12202568352222443\n",
            "Epoch 220 train loss: 0.35782819986343384 test loss: 0.37026339769363403\n",
            "Epoch 220 train loss: 0.2889970541000366 test loss: 0.28020766377449036\n",
            "Epoch 220 train loss: 0.2690657377243042 test loss: 0.2739451229572296\n",
            "Epoch 220 train loss: 0.37651127576828003 test loss: 0.37260738015174866\n",
            "Epoch 220 train loss: 0.19629575312137604 test loss: 0.19908656179904938\n",
            "Epoch 220 train loss: 0.982072651386261 test loss: 0.9756198525428772\n",
            "Epoch 220 train loss: 0.08493325859308243 test loss: 0.08597616851329803\n",
            "Epoch 220 train loss: 0.08202345669269562 test loss: 0.08199340850114822\n",
            "Epoch 220 train loss: 0.3444503843784332 test loss: 0.3431997299194336\n",
            "Epoch 220 train loss: 0.4586383104324341 test loss: 0.4648337662220001\n",
            "Epoch 230 train loss: 0.7297388315200806 test loss: 0.7272825241088867\n",
            "Epoch 230 train loss: 0.577847421169281 test loss: 0.5289288759231567\n",
            "Epoch 230 train loss: 0.6876317262649536 test loss: 0.6680981516838074\n",
            "Epoch 230 train loss: 1.0030461549758911 test loss: 0.9950150847434998\n",
            "Epoch 230 train loss: 0.720270037651062 test loss: 0.728381872177124\n",
            "Epoch 230 train loss: 0.29024556279182434 test loss: 0.2892865836620331\n",
            "Epoch 230 train loss: 0.1904681771993637 test loss: 0.19152870774269104\n",
            "Epoch 230 train loss: 0.8075329065322876 test loss: 0.823506236076355\n",
            "Epoch 230 train loss: 0.27058568596839905 test loss: 0.26731911301612854\n",
            "Epoch 230 train loss: 0.3500111699104309 test loss: 0.35862812399864197\n",
            "Epoch 230 train loss: 0.28924262523651123 test loss: 0.2889266610145569\n",
            "Epoch 230 train loss: 0.44224685430526733 test loss: 0.43272873759269714\n",
            "Epoch 230 train loss: 0.6570722460746765 test loss: 0.6975563764572144\n",
            "Epoch 230 train loss: 0.27496370673179626 test loss: 0.28072085976600647\n",
            "Epoch 230 train loss: 0.26868754625320435 test loss: 0.285612016916275\n",
            "Epoch 230 train loss: 0.2515496611595154 test loss: 0.24998126924037933\n",
            "Epoch 230 train loss: 0.7182663679122925 test loss: 0.7509073615074158\n",
            "Epoch 230 train loss: 0.5069394111633301 test loss: 0.5047692060470581\n",
            "Epoch 230 train loss: 1.1442593336105347 test loss: 1.1005619764328003\n",
            "Epoch 230 train loss: 0.5493447780609131 test loss: 0.5473604798316956\n",
            "Epoch 230 train loss: 0.8453220725059509 test loss: 0.8375768065452576\n",
            "Epoch 230 train loss: 0.4778728187084198 test loss: 0.4828628897666931\n",
            "Epoch 230 train loss: 0.2871691882610321 test loss: 0.2871752381324768\n",
            "Epoch 230 train loss: 1.0635226964950562 test loss: 1.0590219497680664\n",
            "Epoch 230 train loss: 0.8947629332542419 test loss: 0.9016546010971069\n",
            "Epoch 230 train loss: 0.5386754274368286 test loss: 0.5356306433677673\n",
            "Epoch 230 train loss: 0.11980115622282028 test loss: 0.1267627477645874\n",
            "Epoch 230 train loss: 1.4119062423706055 test loss: 1.41652512550354\n",
            "Epoch 230 train loss: 0.5141974091529846 test loss: 0.5219457149505615\n",
            "Epoch 230 train loss: 0.21962574124336243 test loss: 0.22678007185459137\n",
            "Epoch 230 train loss: 0.43938836455345154 test loss: 0.46293842792510986\n",
            "Epoch 230 train loss: 0.5374831557273865 test loss: 0.5384746789932251\n",
            "Epoch 230 train loss: 0.1709783971309662 test loss: 0.16753558814525604\n",
            "Epoch 230 train loss: 1.2030181884765625 test loss: 1.188685655593872\n",
            "Epoch 230 train loss: 0.30549341440200806 test loss: 0.30866944789886475\n",
            "Epoch 230 train loss: 0.4968709945678711 test loss: 0.49245667457580566\n",
            "Epoch 230 train loss: 0.42625972628593445 test loss: 0.42677634954452515\n",
            "Epoch 230 train loss: 0.6765462160110474 test loss: 0.6849594116210938\n",
            "Epoch 230 train loss: 0.5078450441360474 test loss: 0.5015967488288879\n",
            "Epoch 230 train loss: 0.2489943653345108 test loss: 0.24160706996917725\n",
            "Epoch 230 train loss: 0.5137231945991516 test loss: 0.514375627040863\n",
            "Epoch 230 train loss: 0.20267759263515472 test loss: 0.20540447533130646\n",
            "Epoch 230 train loss: 0.4350196123123169 test loss: 0.43977266550064087\n",
            "Epoch 230 train loss: 0.36937350034713745 test loss: 0.3703180253505707\n",
            "Epoch 230 train loss: 0.18688885867595673 test loss: 0.18043586611747742\n",
            "Epoch 230 train loss: 1.150325894355774 test loss: 1.1418449878692627\n",
            "Epoch 230 train loss: 0.15237705409526825 test loss: 0.15232455730438232\n",
            "Epoch 230 train loss: 0.20592358708381653 test loss: 0.20340067148208618\n",
            "Epoch 230 train loss: 0.3463708460330963 test loss: 0.3447369933128357\n",
            "Epoch 230 train loss: 0.5338228344917297 test loss: 0.536137580871582\n",
            "Epoch 240 train loss: 1.4121372699737549 test loss: 1.4067968130111694\n",
            "Epoch 240 train loss: 0.23565608263015747 test loss: 0.228385329246521\n",
            "Epoch 240 train loss: 0.7056677341461182 test loss: 0.7129786610603333\n",
            "Epoch 240 train loss: 0.38170021772384644 test loss: 0.3737650215625763\n",
            "Epoch 240 train loss: 0.7998529672622681 test loss: 0.8003054857254028\n",
            "Epoch 240 train loss: 0.5182435512542725 test loss: 0.5152916312217712\n",
            "Epoch 240 train loss: 0.29753896594047546 test loss: 0.2996964156627655\n",
            "Epoch 240 train loss: 1.1333400011062622 test loss: 1.1288037300109863\n",
            "Epoch 240 train loss: 0.3613108694553375 test loss: 0.35825568437576294\n",
            "Epoch 240 train loss: 0.3588045835494995 test loss: 0.3590996265411377\n",
            "Epoch 240 train loss: 0.2284594029188156 test loss: 0.24519388377666473\n",
            "Epoch 240 train loss: 0.4872431755065918 test loss: 0.4898642301559448\n",
            "Epoch 240 train loss: 0.8488304615020752 test loss: 0.8505442142486572\n",
            "Epoch 240 train loss: 0.28072234988212585 test loss: 0.2841876745223999\n",
            "Epoch 240 train loss: 0.3119085729122162 test loss: 0.3794577419757843\n",
            "Epoch 240 train loss: 0.2225607931613922 test loss: 0.21944110095500946\n",
            "Epoch 240 train loss: 0.5696299076080322 test loss: 0.5687260627746582\n",
            "Epoch 240 train loss: 0.4790501296520233 test loss: 0.4759140908718109\n",
            "Epoch 240 train loss: 1.0405638217926025 test loss: 1.0492761135101318\n",
            "Epoch 240 train loss: 1.0412756204605103 test loss: 1.0246328115463257\n",
            "Epoch 240 train loss: 0.3479593098163605 test loss: 0.34819313883781433\n",
            "Epoch 240 train loss: 0.47743159532546997 test loss: 0.48516547679901123\n",
            "Epoch 240 train loss: 0.4422079622745514 test loss: 0.4539364278316498\n",
            "Epoch 240 train loss: 0.5507725477218628 test loss: 0.5619937777519226\n",
            "Epoch 240 train loss: 0.5652135014533997 test loss: 0.5740576386451721\n",
            "Epoch 240 train loss: 0.3027774691581726 test loss: 0.30254530906677246\n",
            "Epoch 240 train loss: 0.22875362634658813 test loss: 0.2384970635175705\n",
            "Epoch 240 train loss: 0.3128156363964081 test loss: 0.3137550950050354\n",
            "Epoch 240 train loss: 0.47878479957580566 test loss: 0.46771934628486633\n",
            "Epoch 240 train loss: 0.09145088493824005 test loss: 0.09292210638523102\n",
            "Epoch 240 train loss: 0.3414752781391144 test loss: 0.341803640127182\n",
            "Epoch 240 train loss: 0.5409432649612427 test loss: 0.540703535079956\n",
            "Epoch 240 train loss: 0.12183745205402374 test loss: 0.11568523198366165\n",
            "Epoch 240 train loss: 0.6088320016860962 test loss: 0.6136861443519592\n",
            "Epoch 240 train loss: 0.23125851154327393 test loss: 0.2233317345380783\n",
            "Epoch 240 train loss: 0.30388695001602173 test loss: 0.3073815405368805\n",
            "Epoch 240 train loss: 0.3141624331474304 test loss: 0.3172655701637268\n",
            "Epoch 240 train loss: 0.5602056980133057 test loss: 0.5746778845787048\n",
            "Epoch 240 train loss: 0.3465645909309387 test loss: 0.355197012424469\n",
            "Epoch 240 train loss: 0.08978649973869324 test loss: 0.09130308032035828\n",
            "Epoch 240 train loss: 0.21627789735794067 test loss: 0.21077953279018402\n",
            "Epoch 240 train loss: 0.2371344268321991 test loss: 0.23398122191429138\n",
            "Epoch 240 train loss: 0.43025562167167664 test loss: 0.4313805103302002\n",
            "Epoch 240 train loss: 0.3159697949886322 test loss: 0.3149873912334442\n",
            "Epoch 240 train loss: 0.2461729198694229 test loss: 0.2503865659236908\n",
            "Epoch 240 train loss: 0.5749351978302002 test loss: 0.5790808200836182\n",
            "Epoch 240 train loss: 0.075971819460392 test loss: 0.07699324190616608\n",
            "Epoch 240 train loss: 0.09337358176708221 test loss: 0.09409171342849731\n",
            "Epoch 240 train loss: 0.14553087949752808 test loss: 0.1522771716117859\n",
            "Epoch 240 train loss: 0.5892292857170105 test loss: 0.585364043712616\n",
            "Epoch 250 train loss: 0.7480752468109131 test loss: 0.7491466403007507\n",
            "Epoch 250 train loss: 0.24649524688720703 test loss: 0.2390022724866867\n",
            "Epoch 250 train loss: 0.335053950548172 test loss: 0.3367448151111603\n",
            "Epoch 250 train loss: 0.3273215591907501 test loss: 0.33240005373954773\n",
            "Epoch 250 train loss: 0.6820482611656189 test loss: 0.6819468140602112\n",
            "Epoch 250 train loss: 0.24762554466724396 test loss: 0.2409251183271408\n",
            "Epoch 250 train loss: 0.11675429344177246 test loss: 0.11887599527835846\n",
            "Epoch 250 train loss: 0.8783536553382874 test loss: 0.8694689869880676\n",
            "Epoch 250 train loss: 0.2279207855463028 test loss: 0.22432415187358856\n",
            "Epoch 250 train loss: 0.23815736174583435 test loss: 0.22966203093528748\n",
            "Epoch 250 train loss: 0.19634130597114563 test loss: 0.1992735117673874\n",
            "Epoch 250 train loss: 0.38304954767227173 test loss: 0.38745978474617004\n",
            "Epoch 250 train loss: 0.6495587229728699 test loss: 0.649983286857605\n",
            "Epoch 250 train loss: 0.1189383789896965 test loss: 0.11684846878051758\n",
            "Epoch 250 train loss: 0.15318790078163147 test loss: 0.16279083490371704\n",
            "Epoch 250 train loss: 0.17335738241672516 test loss: 0.16958917677402496\n",
            "Epoch 250 train loss: 0.32572534680366516 test loss: 0.3225383758544922\n",
            "Epoch 250 train loss: 0.27880802750587463 test loss: 0.28954553604125977\n",
            "Epoch 250 train loss: 0.8064162731170654 test loss: 0.8041991591453552\n",
            "Epoch 250 train loss: 0.4695943593978882 test loss: 0.4743604362010956\n",
            "Epoch 250 train loss: 0.3069086968898773 test loss: 0.309011310338974\n",
            "Epoch 250 train loss: 0.4692295789718628 test loss: 0.476008802652359\n",
            "Epoch 250 train loss: 0.23141178488731384 test loss: 0.23151268064975739\n",
            "Epoch 250 train loss: 0.45487064123153687 test loss: 0.44765505194664\n",
            "Epoch 250 train loss: 0.3496229350566864 test loss: 0.35472193360328674\n",
            "Epoch 250 train loss: 0.2532365322113037 test loss: 0.2508140504360199\n",
            "Epoch 250 train loss: 0.1124294325709343 test loss: 0.11132985353469849\n",
            "Epoch 250 train loss: 0.11028989404439926 test loss: 0.11063643544912338\n",
            "Epoch 250 train loss: 0.42223164439201355 test loss: 0.43414783477783203\n",
            "Epoch 250 train loss: 0.10193876177072525 test loss: 0.10000085830688477\n",
            "Epoch 250 train loss: 0.429155170917511 test loss: 0.4222167432308197\n",
            "Epoch 250 train loss: 0.3821060359477997 test loss: 0.3706763982772827\n",
            "Epoch 250 train loss: 0.10261654853820801 test loss: 0.0982336699962616\n",
            "Epoch 250 train loss: 0.5036455988883972 test loss: 0.5057889223098755\n",
            "Epoch 250 train loss: 0.23581664264202118 test loss: 0.23380117118358612\n",
            "Epoch 250 train loss: 0.277666836977005 test loss: 0.2705693542957306\n",
            "Epoch 250 train loss: 0.27540072798728943 test loss: 0.2766135334968567\n",
            "Epoch 250 train loss: 0.6453356742858887 test loss: 0.6412956118583679\n",
            "Epoch 250 train loss: 0.375516802072525 test loss: 0.3795742392539978\n",
            "Epoch 250 train loss: 0.04321048781275749 test loss: 0.042134493589401245\n",
            "Epoch 250 train loss: 0.19191598892211914 test loss: 0.1985594630241394\n",
            "Epoch 250 train loss: 0.21094846725463867 test loss: 0.2094935029745102\n",
            "Epoch 250 train loss: 0.20493324100971222 test loss: 0.2049853652715683\n",
            "Epoch 250 train loss: 0.35163044929504395 test loss: 0.35009765625\n",
            "Epoch 250 train loss: 0.15277770161628723 test loss: 0.15446680784225464\n",
            "Epoch 250 train loss: 0.27113795280456543 test loss: 0.26977309584617615\n",
            "Epoch 250 train loss: 0.058109160512685776 test loss: 0.05680793151259422\n",
            "Epoch 250 train loss: 0.04485681280493736 test loss: 0.04541794955730438\n",
            "Epoch 250 train loss: 0.16929489374160767 test loss: 0.17243613302707672\n",
            "Epoch 250 train loss: 0.5094171166419983 test loss: 0.5194130539894104\n",
            "Epoch 260 train loss: 0.703090488910675 test loss: 0.7111901044845581\n",
            "Epoch 260 train loss: 0.16823245584964752 test loss: 0.1674172431230545\n",
            "Epoch 260 train loss: 0.28656306862831116 test loss: 0.28970885276794434\n",
            "Epoch 260 train loss: 0.24048732221126556 test loss: 0.2418239563703537\n",
            "Epoch 260 train loss: 0.7281741499900818 test loss: 0.7325859665870667\n",
            "Epoch 260 train loss: 0.15126989781856537 test loss: 0.15495894849300385\n",
            "Epoch 260 train loss: 0.1261250525712967 test loss: 0.13048681616783142\n",
            "Epoch 260 train loss: 0.7785871624946594 test loss: 0.7764198780059814\n",
            "Epoch 260 train loss: 0.24400945007801056 test loss: 0.23392152786254883\n",
            "Epoch 260 train loss: 0.22803175449371338 test loss: 0.22828000783920288\n",
            "Epoch 260 train loss: 0.16131247580051422 test loss: 0.16212023794651031\n",
            "Epoch 260 train loss: 0.3470284938812256 test loss: 0.3499167561531067\n",
            "Epoch 260 train loss: 0.5540418028831482 test loss: 0.5588985681533813\n",
            "Epoch 260 train loss: 0.08579394221305847 test loss: 0.08614197373390198\n",
            "Epoch 260 train loss: 0.1340360790491104 test loss: 0.13636472821235657\n",
            "Epoch 260 train loss: 0.1225760206580162 test loss: 0.11972463876008987\n",
            "Epoch 260 train loss: 0.24597884714603424 test loss: 0.24488987028598785\n",
            "Epoch 260 train loss: 0.28543880581855774 test loss: 0.28370654582977295\n",
            "Epoch 260 train loss: 0.8661447167396545 test loss: 0.8673941493034363\n",
            "Epoch 260 train loss: 0.18345506489276886 test loss: 0.18132545053958893\n",
            "Epoch 260 train loss: 0.24296927452087402 test loss: 0.25223878026008606\n",
            "Epoch 260 train loss: 0.5106779932975769 test loss: 0.5113765597343445\n",
            "Epoch 260 train loss: 0.2269478291273117 test loss: 0.22308550775051117\n",
            "Epoch 260 train loss: 0.4013425409793854 test loss: 0.40396907925605774\n",
            "Epoch 260 train loss: 0.1534312516450882 test loss: 0.15677054226398468\n",
            "Epoch 260 train loss: 0.2307174652814865 test loss: 0.2364369034767151\n",
            "Epoch 260 train loss: 0.1305704116821289 test loss: 0.1435527801513672\n",
            "Epoch 260 train loss: 0.1073056310415268 test loss: 0.10662095993757248\n",
            "Epoch 260 train loss: 0.39179763197898865 test loss: 0.4106530547142029\n",
            "Epoch 260 train loss: 0.055838219821453094 test loss: 0.06107499077916145\n",
            "Epoch 260 train loss: 0.3368076682090759 test loss: 0.33829784393310547\n",
            "Epoch 260 train loss: 0.5222019553184509 test loss: 0.5318501591682434\n",
            "Epoch 260 train loss: 0.09163586795330048 test loss: 0.09993545711040497\n",
            "Epoch 260 train loss: 0.4187585115432739 test loss: 0.42843562364578247\n",
            "Epoch 260 train loss: 0.199804425239563 test loss: 0.20141872763633728\n",
            "Epoch 260 train loss: 0.2610440254211426 test loss: 0.2514057755470276\n",
            "Epoch 260 train loss: 0.24831484258174896 test loss: 0.24768058955669403\n",
            "Epoch 260 train loss: 0.6170122623443604 test loss: 0.610749363899231\n",
            "Epoch 260 train loss: 0.29349225759506226 test loss: 0.30100736021995544\n",
            "Epoch 260 train loss: 0.04018016159534454 test loss: 0.038975074887275696\n",
            "Epoch 260 train loss: 0.14816007018089294 test loss: 0.14666274189949036\n",
            "Epoch 260 train loss: 0.172123983502388 test loss: 0.1746879667043686\n",
            "Epoch 260 train loss: 0.16100454330444336 test loss: 0.15653812885284424\n",
            "Epoch 260 train loss: 0.2807166278362274 test loss: 0.2795570194721222\n",
            "Epoch 260 train loss: 0.1442631632089615 test loss: 0.1459551453590393\n",
            "Epoch 260 train loss: 0.15099069476127625 test loss: 0.15159006416797638\n",
            "Epoch 260 train loss: 0.04940222576260567 test loss: 0.04513385891914368\n",
            "Epoch 260 train loss: 0.045972712337970734 test loss: 0.04661215469241142\n",
            "Epoch 260 train loss: 0.11760707944631577 test loss: 0.1199205219745636\n",
            "Epoch 260 train loss: 0.40804243087768555 test loss: 0.4134960472583771\n",
            "Epoch 270 train loss: 0.71049964427948 test loss: 0.7210678458213806\n",
            "Epoch 270 train loss: 0.11777536571025848 test loss: 0.11712554097175598\n",
            "Epoch 270 train loss: 0.22622816264629364 test loss: 0.2226819396018982\n",
            "Epoch 270 train loss: 0.2317187339067459 test loss: 0.22915342450141907\n",
            "Epoch 270 train loss: 0.744253933429718 test loss: 0.7485296726226807\n",
            "Epoch 270 train loss: 0.12564758956432343 test loss: 0.13125963509082794\n",
            "Epoch 270 train loss: 0.11272341758012772 test loss: 0.11557004600763321\n",
            "Epoch 270 train loss: 0.816468358039856 test loss: 0.8122491836547852\n",
            "Epoch 270 train loss: 0.22774313390254974 test loss: 0.22773925960063934\n",
            "Epoch 270 train loss: 0.20714996755123138 test loss: 0.20983055233955383\n",
            "Epoch 270 train loss: 0.15082129836082458 test loss: 0.15106888115406036\n",
            "Epoch 270 train loss: 0.32843416929244995 test loss: 0.3202267587184906\n",
            "Epoch 270 train loss: 0.5069037079811096 test loss: 0.5097717642784119\n",
            "Epoch 270 train loss: 0.09461767971515656 test loss: 0.09455122798681259\n",
            "Epoch 270 train loss: 0.11689843982458115 test loss: 0.12932036817073822\n",
            "Epoch 270 train loss: 0.05950143560767174 test loss: 0.06245168671011925\n",
            "Epoch 270 train loss: 0.2589868903160095 test loss: 0.2602405548095703\n",
            "Epoch 270 train loss: 0.2816002368927002 test loss: 0.2771487832069397\n",
            "Epoch 270 train loss: 0.8487621545791626 test loss: 0.8493203520774841\n",
            "Epoch 270 train loss: 0.1640525609254837 test loss: 0.1687690168619156\n",
            "Epoch 270 train loss: 0.21017742156982422 test loss: 0.2110222578048706\n",
            "Epoch 270 train loss: 0.4745013415813446 test loss: 0.4747520983219147\n",
            "Epoch 270 train loss: 0.19482068717479706 test loss: 0.2004278600215912\n",
            "Epoch 270 train loss: 0.3471907377243042 test loss: 0.3410324156284332\n",
            "Epoch 270 train loss: 0.10889709740877151 test loss: 0.10928457230329514\n",
            "Epoch 270 train loss: 0.2151213437318802 test loss: 0.21484576165676117\n",
            "Epoch 270 train loss: 0.10273756086826324 test loss: 0.11707127839326859\n",
            "Epoch 270 train loss: 0.11103377491235733 test loss: 0.10875531286001205\n",
            "Epoch 270 train loss: 0.3443250358104706 test loss: 0.35019272565841675\n",
            "Epoch 270 train loss: 0.037005435675382614 test loss: 0.03908403962850571\n",
            "Epoch 270 train loss: 0.30381208658218384 test loss: 0.3085673153400421\n",
            "Epoch 270 train loss: 0.5310286283493042 test loss: 0.5219808220863342\n",
            "Epoch 270 train loss: 0.08170650899410248 test loss: 0.08031423389911652\n",
            "Epoch 270 train loss: 0.42503175139427185 test loss: 0.43442055583000183\n",
            "Epoch 270 train loss: 0.21072563529014587 test loss: 0.20056672394275665\n",
            "Epoch 270 train loss: 0.24608206748962402 test loss: 0.24941344559192657\n",
            "Epoch 270 train loss: 0.22246380150318146 test loss: 0.22464950382709503\n",
            "Epoch 270 train loss: 0.5401144623756409 test loss: 0.5395441055297852\n",
            "Epoch 270 train loss: 0.2613898515701294 test loss: 0.266838014125824\n",
            "Epoch 270 train loss: 0.040739670395851135 test loss: 0.041729409247636795\n",
            "Epoch 270 train loss: 0.16336911916732788 test loss: 0.16207997500896454\n",
            "Epoch 270 train loss: 0.17977634072303772 test loss: 0.17231740057468414\n",
            "Epoch 270 train loss: 0.15415260195732117 test loss: 0.16199082136154175\n",
            "Epoch 270 train loss: 0.2666855454444885 test loss: 0.2635830044746399\n",
            "Epoch 270 train loss: 0.1427760273218155 test loss: 0.1446385532617569\n",
            "Epoch 270 train loss: 0.1289772093296051 test loss: 0.138455331325531\n",
            "Epoch 270 train loss: 0.04094431549310684 test loss: 0.03995304927229881\n",
            "Epoch 270 train loss: 0.046586137264966965 test loss: 0.05057184770703316\n",
            "Epoch 270 train loss: 0.13117866218090057 test loss: 0.1327246129512787\n",
            "Epoch 270 train loss: 0.33720067143440247 test loss: 0.33778512477874756\n",
            "Epoch 280 train loss: 0.663533091545105 test loss: 0.6536581516265869\n",
            "Epoch 280 train loss: 0.13762812316417694 test loss: 0.1422738879919052\n",
            "Epoch 280 train loss: 0.42254146933555603 test loss: 0.4207514524459839\n",
            "Epoch 280 train loss: 0.3505726456642151 test loss: 0.34979212284088135\n",
            "Epoch 280 train loss: 0.7874873280525208 test loss: 0.777761697769165\n",
            "Epoch 280 train loss: 0.14160557091236115 test loss: 0.13574762642383575\n",
            "Epoch 280 train loss: 0.11489085108041763 test loss: 0.10910173505544662\n",
            "Epoch 280 train loss: 0.9487332105636597 test loss: 0.9421161413192749\n",
            "Epoch 280 train loss: 0.3947996497154236 test loss: 0.373352974653244\n",
            "Epoch 280 train loss: 0.21173477172851562 test loss: 0.2186208963394165\n",
            "Epoch 280 train loss: 0.33058106899261475 test loss: 0.32807931303977966\n",
            "Epoch 280 train loss: 0.2805211544036865 test loss: 0.28182753920555115\n",
            "Epoch 280 train loss: 0.45579713582992554 test loss: 0.43775418400764465\n",
            "Epoch 280 train loss: 0.07715265452861786 test loss: 0.07746429741382599\n",
            "Epoch 280 train loss: 0.1213013082742691 test loss: 0.1625705361366272\n",
            "Epoch 280 train loss: 0.1489792913198471 test loss: 0.14902997016906738\n",
            "Epoch 280 train loss: 0.3615516722202301 test loss: 0.3561483323574066\n",
            "Epoch 280 train loss: 0.40120285749435425 test loss: 0.39542409777641296\n",
            "Epoch 280 train loss: 0.8462997674942017 test loss: 0.8406352996826172\n",
            "Epoch 280 train loss: 0.26197877526283264 test loss: 0.26198315620422363\n",
            "Epoch 280 train loss: 0.25996580719947815 test loss: 0.26551100611686707\n",
            "Epoch 280 train loss: 0.5437224507331848 test loss: 0.5479141473770142\n",
            "Epoch 280 train loss: 0.2161368578672409 test loss: 0.21357420086860657\n",
            "Epoch 280 train loss: 0.45474299788475037 test loss: 0.4507046043872833\n",
            "Epoch 280 train loss: 0.09024418890476227 test loss: 0.0899890884757042\n",
            "Epoch 280 train loss: 0.2309001237154007 test loss: 0.22362962365150452\n",
            "Epoch 280 train loss: 0.16654101014137268 test loss: 0.15711216628551483\n",
            "Epoch 280 train loss: 0.1553860902786255 test loss: 0.1599114090204239\n",
            "Epoch 280 train loss: 0.3202442228794098 test loss: 0.3632940649986267\n",
            "Epoch 280 train loss: 0.07175372540950775 test loss: 0.07774068415164948\n",
            "Epoch 280 train loss: 0.30546674132347107 test loss: 0.30211305618286133\n",
            "Epoch 280 train loss: 0.531799852848053 test loss: 0.5362037420272827\n",
            "Epoch 280 train loss: 0.1241961345076561 test loss: 0.12698405981063843\n",
            "Epoch 280 train loss: 0.641316294670105 test loss: 0.6451505422592163\n",
            "Epoch 280 train loss: 0.33627748489379883 test loss: 0.35306599736213684\n",
            "Epoch 280 train loss: 0.3139493763446808 test loss: 0.32422566413879395\n",
            "Epoch 280 train loss: 0.3287797272205353 test loss: 0.3228435218334198\n",
            "Epoch 280 train loss: 0.554913341999054 test loss: 0.5412296652793884\n",
            "Epoch 280 train loss: 0.35873404145240784 test loss: 0.36119431257247925\n",
            "Epoch 280 train loss: 0.05642414465546608 test loss: 0.05359061807394028\n",
            "Epoch 280 train loss: 0.1279962807893753 test loss: 0.12338148802518845\n",
            "Epoch 280 train loss: 0.16774201393127441 test loss: 0.1623770147562027\n",
            "Epoch 280 train loss: 0.13349668681621552 test loss: 0.13195304572582245\n",
            "Epoch 280 train loss: 0.3332591652870178 test loss: 0.3352222442626953\n",
            "Epoch 280 train loss: 0.14176392555236816 test loss: 0.14706705510616302\n",
            "Epoch 280 train loss: 0.09648647159337997 test loss: 0.09345699846744537\n",
            "Epoch 280 train loss: 0.04162489250302315 test loss: 0.04342774301767349\n",
            "Epoch 280 train loss: 0.05857709050178528 test loss: 0.05700329318642616\n",
            "Epoch 280 train loss: 0.09923017770051956 test loss: 0.09950100630521774\n",
            "Epoch 280 train loss: 0.31250032782554626 test loss: 0.308675616979599\n",
            "Epoch 290 train loss: 0.8415682315826416 test loss: 0.8543557524681091\n",
            "Epoch 290 train loss: 0.10146468132734299 test loss: 0.10005556046962738\n",
            "Epoch 290 train loss: 0.19634263217449188 test loss: 0.19215917587280273\n",
            "Epoch 290 train loss: 0.26839521527290344 test loss: 0.2715093195438385\n",
            "Epoch 290 train loss: 0.6055155396461487 test loss: 0.6030874252319336\n",
            "Epoch 290 train loss: 0.15719138085842133 test loss: 0.15370185673236847\n",
            "Epoch 290 train loss: 0.09622612595558167 test loss: 0.0995318666100502\n",
            "Epoch 290 train loss: 0.47553756833076477 test loss: 0.47106897830963135\n",
            "Epoch 290 train loss: 0.2305595874786377 test loss: 0.23710526525974274\n",
            "Epoch 290 train loss: 0.19997668266296387 test loss: 0.2069774866104126\n",
            "Epoch 290 train loss: 0.2170935422182083 test loss: 0.21511490643024445\n",
            "Epoch 290 train loss: 0.22157679498195648 test loss: 0.21982736885547638\n",
            "Epoch 290 train loss: 0.4056374728679657 test loss: 0.41026824712753296\n",
            "Epoch 290 train loss: 0.07244676351547241 test loss: 0.06986643373966217\n",
            "Epoch 290 train loss: 0.09133816510438919 test loss: 0.10658207535743713\n",
            "Epoch 290 train loss: 0.07432728260755539 test loss: 0.07132495194673538\n",
            "Epoch 290 train loss: 0.23388205468654633 test loss: 0.22766609489917755\n",
            "Epoch 290 train loss: 0.35934892296791077 test loss: 0.36634859442710876\n",
            "Epoch 290 train loss: 0.6713499426841736 test loss: 0.6786230802536011\n",
            "Epoch 290 train loss: 0.153790682554245 test loss: 0.15656116604804993\n",
            "Epoch 290 train loss: 0.2924221158027649 test loss: 0.2987877428531647\n",
            "Epoch 290 train loss: 0.5389604568481445 test loss: 0.5530292391777039\n",
            "Epoch 290 train loss: 0.16893628239631653 test loss: 0.16794933378696442\n",
            "Epoch 290 train loss: 0.39984190464019775 test loss: 0.3936150074005127\n",
            "Epoch 290 train loss: 0.04542814940214157 test loss: 0.04813472181558609\n",
            "Epoch 290 train loss: 0.1646731048822403 test loss: 0.1610022634267807\n",
            "Epoch 290 train loss: 0.15286652743816376 test loss: 0.1520393043756485\n",
            "Epoch 290 train loss: 0.11212123185396194 test loss: 0.10937530547380447\n",
            "Epoch 290 train loss: 0.26259562373161316 test loss: 0.2772141098976135\n",
            "Epoch 290 train loss: 0.037741076201200485 test loss: 0.038996413350105286\n",
            "Epoch 290 train loss: 0.4280470311641693 test loss: 0.42247071862220764\n",
            "Epoch 290 train loss: 0.6310582160949707 test loss: 0.6351093053817749\n",
            "Epoch 290 train loss: 0.08149149268865585 test loss: 0.07802610844373703\n",
            "Epoch 290 train loss: 0.5577999353408813 test loss: 0.5568543672561646\n",
            "Epoch 290 train loss: 0.20654061436653137 test loss: 0.21282348036766052\n",
            "Epoch 290 train loss: 0.23955173790454865 test loss: 0.25485092401504517\n",
            "Epoch 290 train loss: 0.21703840792179108 test loss: 0.21397507190704346\n",
            "Epoch 290 train loss: 0.5599640011787415 test loss: 0.5577663779258728\n",
            "Epoch 290 train loss: 0.46891218423843384 test loss: 0.4782731831073761\n",
            "Epoch 290 train loss: 0.04951058700680733 test loss: 0.04911597818136215\n",
            "Epoch 290 train loss: 0.19160151481628418 test loss: 0.18645834922790527\n",
            "Epoch 290 train loss: 0.23118801414966583 test loss: 0.2298661321401596\n",
            "Epoch 290 train loss: 0.17263422906398773 test loss: 0.17166969180107117\n",
            "Epoch 290 train loss: 0.41061267256736755 test loss: 0.4092385470867157\n",
            "Epoch 290 train loss: 0.17943571507930756 test loss: 0.17917466163635254\n",
            "Epoch 290 train loss: 0.05535499379038811 test loss: 0.054104696959257126\n",
            "Epoch 290 train loss: 0.060996249318122864 test loss: 0.06702958792448044\n",
            "Epoch 290 train loss: 0.048768218606710434 test loss: 0.05364401638507843\n",
            "Epoch 290 train loss: 0.13359211385250092 test loss: 0.1385672688484192\n",
            "Epoch 290 train loss: 0.2524753510951996 test loss: 0.2562050223350525\n",
            "Epoch 300 train loss: 0.72650146484375 test loss: 0.7276371121406555\n",
            "Epoch 300 train loss: 0.1468479335308075 test loss: 0.14240799844264984\n",
            "Epoch 300 train loss: 0.20643360912799835 test loss: 0.21601979434490204\n",
            "Epoch 300 train loss: 0.22028577327728271 test loss: 0.22230079770088196\n",
            "Epoch 300 train loss: 0.581121027469635 test loss: 0.5841858386993408\n",
            "Epoch 300 train loss: 0.11869055032730103 test loss: 0.12123459577560425\n",
            "Epoch 300 train loss: 0.13857677578926086 test loss: 0.1393130123615265\n",
            "Epoch 300 train loss: 0.5933361053466797 test loss: 0.5803788304328918\n",
            "Epoch 300 train loss: 0.17788290977478027 test loss: 0.18812263011932373\n",
            "Epoch 300 train loss: 0.14539895951747894 test loss: 0.11970328539609909\n",
            "Epoch 300 train loss: 0.18874140083789825 test loss: 0.18759123980998993\n",
            "Epoch 300 train loss: 0.19134047627449036 test loss: 0.2011389434337616\n",
            "Epoch 300 train loss: 0.3896018862724304 test loss: 0.3904586732387543\n",
            "Epoch 300 train loss: 0.21092121303081512 test loss: 0.2164745032787323\n",
            "Epoch 300 train loss: 0.09380997717380524 test loss: 0.08987145870923996\n",
            "Epoch 300 train loss: 0.061180416494607925 test loss: 0.05897489935159683\n",
            "Epoch 300 train loss: 0.2928554117679596 test loss: 0.2862253487110138\n",
            "Epoch 300 train loss: 0.3661523759365082 test loss: 0.37405744194984436\n",
            "Epoch 300 train loss: 0.7564405798912048 test loss: 0.7535709142684937\n",
            "Epoch 300 train loss: 0.3190962076187134 test loss: 0.35265693068504333\n",
            "Epoch 300 train loss: 0.2972816228866577 test loss: 0.3015151023864746\n",
            "Epoch 300 train loss: 0.6478816270828247 test loss: 0.6453233957290649\n",
            "Epoch 300 train loss: 0.3543376326560974 test loss: 0.34375327825546265\n",
            "Epoch 300 train loss: 0.4660213589668274 test loss: 0.46246016025543213\n",
            "Epoch 300 train loss: 0.4600986838340759 test loss: 0.4546286165714264\n",
            "Epoch 300 train loss: 0.21867218613624573 test loss: 0.21633054316043854\n",
            "Epoch 300 train loss: 0.23884299397468567 test loss: 0.2453964799642563\n",
            "Epoch 300 train loss: 0.18358469009399414 test loss: 0.17939051985740662\n",
            "Epoch 300 train loss: 0.2630375027656555 test loss: 0.285478800535202\n",
            "Epoch 300 train loss: 0.05500868707895279 test loss: 0.054610468447208405\n",
            "Epoch 300 train loss: 0.41218069195747375 test loss: 0.4061194062232971\n",
            "Epoch 300 train loss: 0.763420820236206 test loss: 0.7662583589553833\n",
            "Epoch 300 train loss: 0.20108744502067566 test loss: 0.2008887678384781\n",
            "Epoch 300 train loss: 1.3047850131988525 test loss: 1.2968828678131104\n",
            "Epoch 300 train loss: 0.2703356146812439 test loss: 0.2659914791584015\n",
            "Epoch 300 train loss: 0.491574227809906 test loss: 0.5127053260803223\n",
            "Epoch 300 train loss: 0.49784785509109497 test loss: 0.4969245195388794\n",
            "Epoch 300 train loss: 0.7434756755828857 test loss: 0.7620368003845215\n",
            "Epoch 300 train loss: 0.2731483578681946 test loss: 0.28230172395706177\n",
            "Epoch 300 train loss: 0.13421368598937988 test loss: 0.1377732902765274\n",
            "Epoch 300 train loss: 0.4509788751602173 test loss: 0.447876900434494\n",
            "Epoch 300 train loss: 0.48081743717193604 test loss: 0.4789721965789795\n",
            "Epoch 300 train loss: 0.23964914679527283 test loss: 0.24529947340488434\n",
            "Epoch 300 train loss: 1.2709776163101196 test loss: 1.278920292854309\n",
            "Epoch 300 train loss: 0.1297198235988617 test loss: 0.1307457685470581\n",
            "Epoch 300 train loss: 1.3569035530090332 test loss: 1.3586753606796265\n",
            "Epoch 300 train loss: 0.07369957119226456 test loss: 0.07421626895666122\n",
            "Epoch 300 train loss: 0.14998145401477814 test loss: 0.15184266865253448\n",
            "Epoch 300 train loss: 0.2662992775440216 test loss: 0.2672138810157776\n",
            "Epoch 300 train loss: 0.18524430692195892 test loss: 0.18119588494300842\n",
            "Saving model \n",
            "\n",
            "Done\n",
            "Epoch 310 train loss: 0.7202315330505371 test loss: 0.7215108871459961\n",
            "Epoch 310 train loss: 0.13528884947299957 test loss: 0.13666214048862457\n",
            "Epoch 310 train loss: 0.23469458520412445 test loss: 0.2274654656648636\n",
            "Epoch 310 train loss: 0.23652176558971405 test loss: 0.24159623682498932\n",
            "Epoch 310 train loss: 0.6554899215698242 test loss: 0.6539143919944763\n",
            "Epoch 310 train loss: 0.3711913824081421 test loss: 0.3350711762905121\n",
            "Epoch 310 train loss: 0.14608100056648254 test loss: 0.1477544754743576\n",
            "Epoch 310 train loss: 0.720898449420929 test loss: 0.7337361574172974\n",
            "Epoch 310 train loss: 0.3828413188457489 test loss: 0.3475930392742157\n",
            "Epoch 310 train loss: 0.14048023521900177 test loss: 0.14935004711151123\n",
            "Epoch 310 train loss: 0.21981830894947052 test loss: 0.21811677515506744\n",
            "Epoch 310 train loss: 0.458202064037323 test loss: 0.4658539593219757\n",
            "Epoch 310 train loss: 0.8145504593849182 test loss: 0.8120964765548706\n",
            "Epoch 310 train loss: 0.09563718736171722 test loss: 0.09484145045280457\n",
            "Epoch 310 train loss: 0.23182916641235352 test loss: 0.2701970934867859\n",
            "Epoch 310 train loss: 0.14582286775112152 test loss: 0.14403636753559113\n",
            "Epoch 310 train loss: 0.4512281119823456 test loss: 0.45250204205513\n",
            "Epoch 310 train loss: 0.3965373933315277 test loss: 0.384789377450943\n",
            "Epoch 310 train loss: 1.3682204484939575 test loss: 1.3475672006607056\n",
            "Epoch 310 train loss: 0.9372299313545227 test loss: 0.9714285731315613\n",
            "Epoch 310 train loss: 0.23259380459785461 test loss: 0.23609259724617004\n",
            "Epoch 310 train loss: 0.46375805139541626 test loss: 0.4673346281051636\n",
            "Epoch 310 train loss: 0.2578643262386322 test loss: 0.24879932403564453\n",
            "Epoch 310 train loss: 0.5566143989562988 test loss: 0.578079104423523\n",
            "Epoch 310 train loss: 0.4574352502822876 test loss: 0.43339303135871887\n",
            "Epoch 310 train loss: 0.24577903747558594 test loss: 0.25513821840286255\n",
            "Epoch 310 train loss: 0.11678110063076019 test loss: 0.14146071672439575\n",
            "Epoch 310 train loss: 0.17539094388484955 test loss: 0.17579001188278198\n",
            "Epoch 310 train loss: 0.42168372869491577 test loss: 0.4812609553337097\n",
            "Epoch 310 train loss: 0.1529698520898819 test loss: 0.16972440481185913\n",
            "Epoch 310 train loss: 0.7120293378829956 test loss: 0.7577438354492188\n",
            "Epoch 310 train loss: 2.2486765384674072 test loss: 2.2668848037719727\n",
            "Epoch 310 train loss: 0.19980588555335999 test loss: 0.20904313027858734\n",
            "Epoch 310 train loss: 1.9348074197769165 test loss: 2.010685920715332\n",
            "Epoch 310 train loss: 0.8542910218238831 test loss: 0.9079171419143677\n",
            "Epoch 310 train loss: 1.7715569734573364 test loss: 1.8291313648223877\n",
            "Epoch 310 train loss: 0.9997471570968628 test loss: 1.0091581344604492\n",
            "Epoch 310 train loss: 0.6694724559783936 test loss: 0.6575971841812134\n",
            "Epoch 310 train loss: 0.5988209247589111 test loss: 0.4879739284515381\n",
            "Epoch 310 train loss: 1.0193169116973877 test loss: 0.9758142828941345\n",
            "Epoch 310 train loss: 1.9679075479507446 test loss: 1.9023919105529785\n",
            "Epoch 310 train loss: 0.6440650820732117 test loss: 0.6179336309432983\n",
            "Epoch 310 train loss: 1.1783498525619507 test loss: 1.2552727460861206\n",
            "Epoch 310 train loss: 1.0910403728485107 test loss: 1.054472804069519\n",
            "Epoch 310 train loss: 0.7548583149909973 test loss: 0.7095927000045776\n",
            "Epoch 310 train loss: 0.7408908009529114 test loss: 0.7352158427238464\n",
            "Epoch 310 train loss: 0.7573021650314331 test loss: 0.8552262783050537\n",
            "Epoch 310 train loss: 0.3938116729259491 test loss: 0.47966739535331726\n",
            "Epoch 310 train loss: 0.40779948234558105 test loss: 0.4182834327220917\n",
            "Epoch 310 train loss: 0.7590962648391724 test loss: 0.7610838413238525\n",
            "Epoch 320 train loss: 0.6496189832687378 test loss: 0.6670135855674744\n",
            "Epoch 320 train loss: 0.2447482794523239 test loss: 0.23713518679141998\n",
            "Epoch 320 train loss: 0.21728016436100006 test loss: 0.21239204704761505\n",
            "Epoch 320 train loss: 0.2872166037559509 test loss: 0.2918837368488312\n",
            "Epoch 320 train loss: 0.8866186738014221 test loss: 0.883486270904541\n",
            "Epoch 320 train loss: 0.24466778337955475 test loss: 0.2532472610473633\n",
            "Epoch 320 train loss: 0.24564456939697266 test loss: 0.24834565818309784\n",
            "Epoch 320 train loss: 0.8865364193916321 test loss: 0.8950564861297607\n",
            "Epoch 320 train loss: 0.2572307288646698 test loss: 0.2822103500366211\n",
            "Epoch 320 train loss: 0.1759655475616455 test loss: 0.17819999158382416\n",
            "Epoch 320 train loss: 0.3237098455429077 test loss: 0.3160862922668457\n",
            "Epoch 320 train loss: 0.3052508533000946 test loss: 0.2908327281475067\n",
            "Epoch 320 train loss: 0.802571177482605 test loss: 0.7967830896377563\n",
            "Epoch 320 train loss: 0.10474733263254166 test loss: 0.10087622702121735\n",
            "Epoch 320 train loss: 0.2758021950721741 test loss: 0.33929243683815\n",
            "Epoch 320 train loss: 0.18949271738529205 test loss: 0.1974029392004013\n",
            "Epoch 320 train loss: 0.25568854808807373 test loss: 0.24873188138008118\n",
            "Epoch 320 train loss: 0.4140501022338867 test loss: 0.41380852460861206\n",
            "Epoch 320 train loss: 0.9704664945602417 test loss: 0.9718914031982422\n",
            "Epoch 320 train loss: 0.48296400904655457 test loss: 0.47984305024147034\n",
            "Epoch 320 train loss: 0.2470909208059311 test loss: 0.2520882785320282\n",
            "Epoch 320 train loss: 0.5180174708366394 test loss: 0.5205710530281067\n",
            "Epoch 320 train loss: 0.2177974134683609 test loss: 0.21679207682609558\n",
            "Epoch 320 train loss: 0.39245620369911194 test loss: 0.3899499773979187\n",
            "Epoch 320 train loss: 0.15566088259220123 test loss: 0.15550071001052856\n",
            "Epoch 320 train loss: 0.16126753389835358 test loss: 0.16161367297172546\n",
            "Epoch 320 train loss: 0.426116019487381 test loss: 0.4126901924610138\n",
            "Epoch 320 train loss: 0.2928623855113983 test loss: 0.2852545380592346\n",
            "Epoch 320 train loss: 0.7090351581573486 test loss: 0.7508881688117981\n",
            "Epoch 320 train loss: 0.35752835869789124 test loss: 0.3637271225452423\n",
            "Epoch 320 train loss: 0.26758912205696106 test loss: 0.26553231477737427\n",
            "Epoch 320 train loss: 0.43817707896232605 test loss: 0.4327208399772644\n",
            "Epoch 320 train loss: 0.08172889798879623 test loss: 0.08301176875829697\n",
            "Epoch 320 train loss: 0.5753974318504333 test loss: 0.5687350034713745\n",
            "Epoch 320 train loss: 0.23452509939670563 test loss: 0.22693893313407898\n",
            "Epoch 320 train loss: 0.2929244637489319 test loss: 0.296328067779541\n",
            "Epoch 320 train loss: 0.26127395033836365 test loss: 0.26217249035835266\n",
            "Epoch 320 train loss: 0.6245946288108826 test loss: 0.6251675486564636\n",
            "Epoch 320 train loss: 0.35382112860679626 test loss: 0.3554855287075043\n",
            "Epoch 320 train loss: 0.06689747422933578 test loss: 0.06561647355556488\n",
            "Epoch 320 train loss: 0.21430572867393494 test loss: 0.20718134939670563\n",
            "Epoch 320 train loss: 0.34657546877861023 test loss: 0.3572787940502167\n",
            "Epoch 320 train loss: 0.06796906143426895 test loss: 0.06595270335674286\n",
            "Epoch 320 train loss: 0.33895546197891235 test loss: 0.33905813097953796\n",
            "Epoch 320 train loss: 0.17683231830596924 test loss: 0.17912757396697998\n",
            "Epoch 320 train loss: 0.1525646448135376 test loss: 0.15295971930027008\n",
            "Epoch 320 train loss: 0.1198916956782341 test loss: 0.1239418089389801\n",
            "Epoch 320 train loss: 0.07351487874984741 test loss: 0.07737650722265244\n",
            "Epoch 320 train loss: 0.2044222503900528 test loss: 0.198310986161232\n",
            "Epoch 320 train loss: 0.48338884115219116 test loss: 0.48463311791419983\n",
            "Epoch 330 train loss: 0.6356794834136963 test loss: 0.6437208652496338\n",
            "Epoch 330 train loss: 0.06252744048833847 test loss: 0.06269973516464233\n",
            "Epoch 330 train loss: 0.15314896404743195 test loss: 0.1477990448474884\n",
            "Epoch 330 train loss: 0.20849214494228363 test loss: 0.2129146158695221\n",
            "Epoch 330 train loss: 0.672078013420105 test loss: 0.6708360314369202\n",
            "Epoch 330 train loss: 0.18645566701889038 test loss: 0.18838994204998016\n",
            "Epoch 330 train loss: 0.15475378930568695 test loss: 0.1606520712375641\n",
            "Epoch 330 train loss: 0.7301338315010071 test loss: 0.7277356386184692\n",
            "Epoch 330 train loss: 0.2528393864631653 test loss: 0.23981107771396637\n",
            "Epoch 330 train loss: 0.11018960922956467 test loss: 0.14465272426605225\n",
            "Epoch 330 train loss: 0.18312150239944458 test loss: 0.18591000139713287\n",
            "Epoch 330 train loss: 0.1869133710861206 test loss: 0.1800357699394226\n",
            "Epoch 330 train loss: 0.7529542446136475 test loss: 0.7543352246284485\n",
            "Epoch 330 train loss: 0.17684629559516907 test loss: 0.17476077377796173\n",
            "Epoch 330 train loss: 0.1838933527469635 test loss: 0.33317017555236816\n",
            "Epoch 330 train loss: 0.054202333092689514 test loss: 0.049609288573265076\n",
            "Epoch 330 train loss: 0.19179148972034454 test loss: 0.18699359893798828\n",
            "Epoch 330 train loss: 0.23648840188980103 test loss: 0.2386074662208557\n",
            "Epoch 330 train loss: 1.0592998266220093 test loss: 1.0598011016845703\n",
            "Epoch 330 train loss: 0.32306531071662903 test loss: 0.3283034861087799\n",
            "Epoch 330 train loss: 0.21021735668182373 test loss: 0.207999587059021\n",
            "Epoch 330 train loss: 0.39566269516944885 test loss: 0.3918892741203308\n",
            "Epoch 330 train loss: 0.15570612251758575 test loss: 0.1582556515932083\n",
            "Epoch 330 train loss: 0.3782460391521454 test loss: 0.38079366087913513\n",
            "Epoch 330 train loss: 0.11723444610834122 test loss: 0.1075172945857048\n",
            "Epoch 330 train loss: 0.10390949249267578 test loss: 0.10328343510627747\n",
            "Epoch 330 train loss: 0.33203330636024475 test loss: 0.3164171278476715\n",
            "Epoch 330 train loss: 0.21842308342456818 test loss: 0.22071681916713715\n",
            "Epoch 330 train loss: 0.5454161763191223 test loss: 0.6069262027740479\n",
            "Epoch 330 train loss: 0.050590261816978455 test loss: 0.05208994448184967\n",
            "Epoch 330 train loss: 0.3116980195045471 test loss: 0.3183850049972534\n",
            "Epoch 330 train loss: 0.3911552429199219 test loss: 0.3940907418727875\n",
            "Epoch 330 train loss: 0.07227875292301178 test loss: 0.07343756407499313\n",
            "Epoch 330 train loss: 0.5294228792190552 test loss: 0.5853023529052734\n",
            "Epoch 330 train loss: 0.225428968667984 test loss: 0.2252350151538849\n",
            "Epoch 330 train loss: 0.3583293557167053 test loss: 0.36907270550727844\n",
            "Epoch 330 train loss: 0.24421058595180511 test loss: 0.24711500108242035\n",
            "Epoch 330 train loss: 0.4914143681526184 test loss: 0.49516844749450684\n",
            "Epoch 330 train loss: 0.3304167687892914 test loss: 0.33543798327445984\n",
            "Epoch 330 train loss: 0.06811267137527466 test loss: 0.05796336010098457\n",
            "Epoch 330 train loss: 0.12134439498186111 test loss: 0.12962329387664795\n",
            "Epoch 330 train loss: 0.24925492703914642 test loss: 0.2508397698402405\n",
            "Epoch 330 train loss: 0.0935172587633133 test loss: 0.08893044292926788\n",
            "Epoch 330 train loss: 0.1908336877822876 test loss: 0.19143640995025635\n",
            "Epoch 330 train loss: 0.1567501723766327 test loss: 0.16207489371299744\n",
            "Epoch 330 train loss: 0.09641438722610474 test loss: 0.10060569643974304\n",
            "Epoch 330 train loss: 0.11349918693304062 test loss: 0.1051667258143425\n",
            "Epoch 330 train loss: 0.06441336125135422 test loss: 0.061885103583335876\n",
            "Epoch 330 train loss: 0.10888241231441498 test loss: 0.10731838643550873\n",
            "Epoch 330 train loss: 0.24435636401176453 test loss: 0.24882252514362335\n",
            "Epoch 340 train loss: 0.8549387454986572 test loss: 0.8464576601982117\n",
            "Epoch 340 train loss: 0.7065054178237915 test loss: 0.7089666128158569\n",
            "Epoch 340 train loss: 0.9505022168159485 test loss: 0.9638931155204773\n",
            "Epoch 340 train loss: 0.2888964116573334 test loss: 0.2933095097541809\n",
            "Epoch 340 train loss: 0.7188119292259216 test loss: 0.7241803407669067\n",
            "Epoch 340 train loss: 0.23492375016212463 test loss: 0.22800377011299133\n",
            "Epoch 340 train loss: 0.3103236258029938 test loss: 0.307674765586853\n",
            "Epoch 340 train loss: 1.0706521272659302 test loss: 1.0767154693603516\n",
            "Epoch 340 train loss: 0.4871220886707306 test loss: 0.48931506276130676\n",
            "Epoch 340 train loss: 0.2657064199447632 test loss: 0.25395774841308594\n",
            "Epoch 340 train loss: 0.5115674734115601 test loss: 0.5034129023551941\n",
            "Epoch 340 train loss: 0.5682162046432495 test loss: 0.5651649236679077\n",
            "Epoch 340 train loss: 0.9247787594795227 test loss: 0.9159368276596069\n",
            "Epoch 340 train loss: 0.9071223735809326 test loss: 0.9072837829589844\n",
            "Epoch 340 train loss: 0.43763467669487 test loss: 0.4217076599597931\n",
            "Epoch 340 train loss: 0.3388681709766388 test loss: 0.34691476821899414\n",
            "Epoch 340 train loss: 1.0951437950134277 test loss: 1.1136970520019531\n",
            "Epoch 340 train loss: 0.5755077004432678 test loss: 0.5787414312362671\n",
            "Epoch 340 train loss: 1.180197834968567 test loss: 1.1615580320358276\n",
            "Epoch 340 train loss: 0.7778152227401733 test loss: 0.7804068922996521\n",
            "Epoch 340 train loss: 0.3544183373451233 test loss: 0.3538723289966583\n",
            "Epoch 340 train loss: 0.781338632106781 test loss: 0.8120527863502502\n",
            "Epoch 340 train loss: 0.49550703167915344 test loss: 0.5064007043838501\n",
            "Epoch 340 train loss: 0.5768390893936157 test loss: 0.5815476179122925\n",
            "Epoch 340 train loss: 0.6445951461791992 test loss: 0.6493235230445862\n",
            "Epoch 340 train loss: 0.26813459396362305 test loss: 0.2687910795211792\n",
            "Epoch 340 train loss: 0.4298662841320038 test loss: 0.43677574396133423\n",
            "Epoch 340 train loss: 0.42624688148498535 test loss: 0.43016451597213745\n",
            "Epoch 340 train loss: 0.6912038922309875 test loss: 0.7080222368240356\n",
            "Epoch 340 train loss: 0.14629611372947693 test loss: 0.15711016952991486\n",
            "Epoch 340 train loss: 0.49278587102890015 test loss: 0.4870899021625519\n",
            "Epoch 340 train loss: 0.6320459842681885 test loss: 0.6318442225456238\n",
            "Epoch 340 train loss: 0.08598031848669052 test loss: 0.0922301858663559\n",
            "Epoch 340 train loss: 1.090934157371521 test loss: 1.106715440750122\n",
            "Epoch 340 train loss: 0.5914613604545593 test loss: 0.5748634338378906\n",
            "Epoch 340 train loss: 0.7560681104660034 test loss: 0.7499217987060547\n",
            "Epoch 340 train loss: 0.30503517389297485 test loss: 0.3037705719470978\n",
            "Epoch 340 train loss: 0.5567639470100403 test loss: 0.5524560213088989\n",
            "Epoch 340 train loss: 0.5004429817199707 test loss: 0.49431112408638\n",
            "Epoch 340 train loss: 0.20233988761901855 test loss: 0.20223800837993622\n",
            "Epoch 340 train loss: 0.3542248606681824 test loss: 0.3507290780544281\n",
            "Epoch 340 train loss: 0.18830983340740204 test loss: 0.18559733033180237\n",
            "Epoch 340 train loss: 0.5804378986358643 test loss: 0.5755531191825867\n",
            "Epoch 340 train loss: 0.3834126889705658 test loss: 0.40087950229644775\n",
            "Epoch 340 train loss: 0.2289627641439438 test loss: 0.22917509078979492\n",
            "Epoch 340 train loss: 0.2121765911579132 test loss: 0.21571989357471466\n",
            "Epoch 340 train loss: 0.12208879739046097 test loss: 0.12685222923755646\n",
            "Epoch 340 train loss: 0.18138235807418823 test loss: 0.1824568510055542\n",
            "Epoch 340 train loss: 0.33387020230293274 test loss: 0.3413727879524231\n",
            "Epoch 340 train loss: 0.5986124873161316 test loss: 0.5920950174331665\n",
            "Epoch 350 train loss: 0.6306370496749878 test loss: 0.6386826038360596\n",
            "Epoch 350 train loss: 0.8875170946121216 test loss: 0.8755486607551575\n",
            "Epoch 350 train loss: 0.3678528368473053 test loss: 0.3664558231830597\n",
            "Epoch 350 train loss: 0.3675168752670288 test loss: 0.38225221633911133\n",
            "Epoch 350 train loss: 0.7896085977554321 test loss: 0.7930042147636414\n",
            "Epoch 350 train loss: 0.4371960759162903 test loss: 0.43100419640541077\n",
            "Epoch 350 train loss: 0.2335645705461502 test loss: 0.23010383546352386\n",
            "Epoch 350 train loss: 1.1094696521759033 test loss: 1.1031073331832886\n",
            "Epoch 350 train loss: 0.43500709533691406 test loss: 0.42499274015426636\n",
            "Epoch 350 train loss: 0.3462430536746979 test loss: 0.3489028811454773\n",
            "Epoch 350 train loss: 0.41841572523117065 test loss: 0.4078886806964874\n",
            "Epoch 350 train loss: 0.8373462557792664 test loss: 0.8425711393356323\n",
            "Epoch 350 train loss: 1.230640172958374 test loss: 1.2465951442718506\n",
            "Epoch 350 train loss: 0.35297369956970215 test loss: 0.366660475730896\n",
            "Epoch 350 train loss: 0.4481494426727295 test loss: 0.44009363651275635\n",
            "Epoch 350 train loss: 0.2230152189731598 test loss: 0.21937796473503113\n",
            "Epoch 350 train loss: 0.6300795078277588 test loss: 0.6286388039588928\n",
            "Epoch 350 train loss: 0.6038763523101807 test loss: 0.6009323596954346\n",
            "Epoch 350 train loss: 0.9656252861022949 test loss: 0.9801177978515625\n",
            "Epoch 350 train loss: 0.46728387475013733 test loss: 0.47119539976119995\n",
            "Epoch 350 train loss: 0.49014535546302795 test loss: 0.4910944402217865\n",
            "Epoch 350 train loss: 0.5035423040390015 test loss: 0.5016301870346069\n",
            "Epoch 350 train loss: 0.44893404841423035 test loss: 0.4635460376739502\n",
            "Epoch 350 train loss: 0.5898957848548889 test loss: 0.5966257452964783\n",
            "Epoch 350 train loss: 0.2075929343700409 test loss: 0.20344693958759308\n",
            "Epoch 350 train loss: 0.3790699541568756 test loss: 0.3759406805038452\n",
            "Epoch 350 train loss: 0.3143848478794098 test loss: 0.3119601309299469\n",
            "Epoch 350 train loss: 0.5870348215103149 test loss: 0.5860355496406555\n",
            "Epoch 350 train loss: 0.8038410544395447 test loss: 0.8429796695709229\n",
            "Epoch 350 train loss: 0.1550506055355072 test loss: 0.15415756404399872\n",
            "Epoch 350 train loss: 0.4136514663696289 test loss: 0.4071662724018097\n",
            "Epoch 350 train loss: 0.7291228175163269 test loss: 0.7216473817825317\n",
            "Epoch 350 train loss: 0.10319633781909943 test loss: 0.1024119108915329\n",
            "Epoch 350 train loss: 0.9681870341300964 test loss: 0.9596736431121826\n",
            "Epoch 350 train loss: 0.47965824604034424 test loss: 0.4800724983215332\n",
            "Epoch 350 train loss: 0.7365027070045471 test loss: 0.7427980303764343\n",
            "Epoch 350 train loss: 0.3767796754837036 test loss: 0.3814373016357422\n",
            "Epoch 350 train loss: 0.5604195594787598 test loss: 0.5659981966018677\n",
            "Epoch 350 train loss: 0.5080907940864563 test loss: 0.5040205717086792\n",
            "Epoch 350 train loss: 0.2232455462217331 test loss: 0.22441187500953674\n",
            "Epoch 350 train loss: 0.5235702395439148 test loss: 0.525326669216156\n",
            "Epoch 350 train loss: 0.11169205605983734 test loss: 0.11421570181846619\n",
            "Epoch 350 train loss: 0.5247045159339905 test loss: 0.5213895440101624\n",
            "Epoch 350 train loss: 0.2763832211494446 test loss: 0.27632641792297363\n",
            "Epoch 350 train loss: 0.1809435337781906 test loss: 0.17858752608299255\n",
            "Epoch 350 train loss: 0.4897932708263397 test loss: 0.5089963674545288\n",
            "Epoch 350 train loss: 0.13715708255767822 test loss: 0.13811980187892914\n",
            "Epoch 350 train loss: 0.19501057267189026 test loss: 0.19689594209194183\n",
            "Epoch 350 train loss: 0.4190567433834076 test loss: 0.41343629360198975\n",
            "Epoch 350 train loss: 0.3429519832134247 test loss: 0.3448711931705475\n",
            "Epoch 360 train loss: 0.9421210289001465 test loss: 0.9494677186012268\n",
            "Epoch 360 train loss: 0.33195996284484863 test loss: 0.33249327540397644\n",
            "Epoch 360 train loss: 0.7302324771881104 test loss: 0.6855124831199646\n",
            "Epoch 360 train loss: 0.20612269639968872 test loss: 0.21712690591812134\n",
            "Epoch 360 train loss: 0.6612197756767273 test loss: 0.6700085997581482\n",
            "Epoch 360 train loss: 0.19981515407562256 test loss: 0.19847489893436432\n",
            "Epoch 360 train loss: 0.22468559443950653 test loss: 0.22246778011322021\n",
            "Epoch 360 train loss: 0.9249488711357117 test loss: 0.9362517595291138\n",
            "Epoch 360 train loss: 0.46428486704826355 test loss: 0.4456705152988434\n",
            "Epoch 360 train loss: 0.11622703820466995 test loss: 0.11267118155956268\n",
            "Epoch 360 train loss: 0.19725839793682098 test loss: 0.18764322996139526\n",
            "Epoch 360 train loss: 0.33496350049972534 test loss: 0.3265232741832733\n",
            "Epoch 360 train loss: 0.8633455634117126 test loss: 0.8646979331970215\n",
            "Epoch 360 train loss: 0.0826229453086853 test loss: 0.08655385673046112\n",
            "Epoch 360 train loss: 0.3468368947505951 test loss: 0.33941224217414856\n",
            "Epoch 360 train loss: 0.2138310968875885 test loss: 0.21229591965675354\n",
            "Epoch 360 train loss: 0.62907475233078 test loss: 0.6277728080749512\n",
            "Epoch 360 train loss: 0.5215553641319275 test loss: 0.5203495025634766\n",
            "Epoch 360 train loss: 0.7577712535858154 test loss: 0.7606440782546997\n",
            "Epoch 360 train loss: 0.4617239534854889 test loss: 0.45225784182548523\n",
            "Epoch 360 train loss: 0.22775503993034363 test loss: 0.2337922602891922\n",
            "Epoch 360 train loss: 0.6389198303222656 test loss: 0.6301819086074829\n",
            "Epoch 360 train loss: 0.340350478887558 test loss: 0.33495816588401794\n",
            "Epoch 360 train loss: 0.46329963207244873 test loss: 0.4651699364185333\n",
            "Epoch 360 train loss: 0.4669959843158722 test loss: 0.4728456735610962\n",
            "Epoch 360 train loss: 0.2405083179473877 test loss: 0.23634470999240875\n",
            "Epoch 360 train loss: 0.2723150849342346 test loss: 0.2713211476802826\n",
            "Epoch 360 train loss: 0.30691367387771606 test loss: 0.3023330271244049\n",
            "Epoch 360 train loss: 0.46184760332107544 test loss: 0.5279120206832886\n",
            "Epoch 360 train loss: 0.15868283808231354 test loss: 0.15652219951152802\n",
            "Epoch 360 train loss: 0.29231250286102295 test loss: 0.3013191819190979\n",
            "Epoch 360 train loss: 0.803955078125 test loss: 0.7932663559913635\n",
            "Epoch 360 train loss: 0.08975046128034592 test loss: 0.08629504591226578\n",
            "Epoch 360 train loss: 0.9110093116760254 test loss: 0.9679601788520813\n",
            "Epoch 360 train loss: 0.31808969378471375 test loss: 0.3252869248390198\n",
            "Epoch 360 train loss: 0.46408939361572266 test loss: 0.511061429977417\n",
            "Epoch 360 train loss: 0.33527034521102905 test loss: 0.33647704124450684\n",
            "Epoch 360 train loss: 0.7531754374504089 test loss: 0.7639336585998535\n",
            "Epoch 360 train loss: 0.47771868109703064 test loss: 0.494100421667099\n",
            "Epoch 360 train loss: 0.25035274028778076 test loss: 0.2544524073600769\n",
            "Epoch 360 train loss: 0.42257723212242126 test loss: 0.4317362308502197\n",
            "Epoch 360 train loss: 0.21248123049736023 test loss: 0.24389295279979706\n",
            "Epoch 360 train loss: 0.3889625072479248 test loss: 0.3828814923763275\n",
            "Epoch 360 train loss: 0.7035632729530334 test loss: 0.7487077713012695\n",
            "Epoch 360 train loss: 0.23558887839317322 test loss: 0.2411855310201645\n",
            "Epoch 360 train loss: 0.5564197301864624 test loss: 0.5603686571121216\n",
            "Epoch 360 train loss: 0.1303456723690033 test loss: 0.13195352256298065\n",
            "Epoch 360 train loss: 0.2541046738624573 test loss: 0.25899773836135864\n",
            "Epoch 360 train loss: 0.2036682516336441 test loss: 0.19940009713172913\n",
            "Epoch 360 train loss: 0.43538087606430054 test loss: 0.43920502066612244\n",
            "Epoch 370 train loss: 0.8550330996513367 test loss: 0.8425959944725037\n",
            "Epoch 370 train loss: 0.4593202769756317 test loss: 0.4625014364719391\n",
            "Epoch 370 train loss: 0.2828141450881958 test loss: 0.2843798100948334\n",
            "Epoch 370 train loss: 0.26253849267959595 test loss: 0.26427873969078064\n",
            "Epoch 370 train loss: 0.6118120551109314 test loss: 0.6109232902526855\n",
            "Epoch 370 train loss: 0.23573076725006104 test loss: 0.23726075887680054\n",
            "Epoch 370 train loss: 0.1635076403617859 test loss: 0.15581071376800537\n",
            "Epoch 370 train loss: 0.6001070141792297 test loss: 0.604508101940155\n",
            "Epoch 370 train loss: 0.3145194351673126 test loss: 0.31233516335487366\n",
            "Epoch 370 train loss: 0.15321084856987 test loss: 0.15633440017700195\n",
            "Epoch 370 train loss: 0.33655866980552673 test loss: 0.3335224390029907\n",
            "Epoch 370 train loss: 0.3276336193084717 test loss: 0.32888245582580566\n",
            "Epoch 370 train loss: 0.6754456162452698 test loss: 0.6790750026702881\n",
            "Epoch 370 train loss: 0.0692046508193016 test loss: 0.07421014457941055\n",
            "Epoch 370 train loss: 0.28848665952682495 test loss: 0.29808923602104187\n",
            "Epoch 370 train loss: 0.11660562455654144 test loss: 0.11792416870594025\n",
            "Epoch 370 train loss: 0.4791634976863861 test loss: 0.4797540605068207\n",
            "Epoch 370 train loss: 0.3362889587879181 test loss: 0.33340179920196533\n",
            "Epoch 370 train loss: 0.7269288301467896 test loss: 0.7446311116218567\n",
            "Epoch 370 train loss: 1.1550195217132568 test loss: 1.1540398597717285\n",
            "Epoch 370 train loss: 0.2906777560710907 test loss: 0.28476786613464355\n",
            "Epoch 370 train loss: 0.4431266784667969 test loss: 0.4302530288696289\n",
            "Epoch 370 train loss: 0.3488839268684387 test loss: 0.3399381935596466\n",
            "Epoch 370 train loss: 0.4996834397315979 test loss: 0.5013366937637329\n",
            "Epoch 370 train loss: 0.2894887924194336 test loss: 0.29390019178390503\n",
            "Epoch 370 train loss: 0.19491486251354218 test loss: 0.19362568855285645\n",
            "Epoch 370 train loss: 0.077999547123909 test loss: 0.07462531328201294\n",
            "Epoch 370 train loss: 0.19390520453453064 test loss: 0.18881258368492126\n",
            "Epoch 370 train loss: 0.38627889752388 test loss: 0.4025517702102661\n",
            "Epoch 370 train loss: 0.22512662410736084 test loss: 0.22504529356956482\n",
            "Epoch 370 train loss: 0.5024645328521729 test loss: 0.5031118988990784\n",
            "Epoch 370 train loss: 0.6378771662712097 test loss: 0.6403661966323853\n",
            "Epoch 370 train loss: 0.07004307955503464 test loss: 0.07123036682605743\n",
            "Epoch 370 train loss: 0.6763781905174255 test loss: 0.6837287545204163\n",
            "Epoch 370 train loss: 0.3416217267513275 test loss: 0.3418610394001007\n",
            "Epoch 370 train loss: 0.35906970500946045 test loss: 0.3637712597846985\n",
            "Epoch 370 train loss: 0.4023224115371704 test loss: 0.40408653020858765\n",
            "Epoch 370 train loss: 0.6554912328720093 test loss: 0.6310070157051086\n",
            "Epoch 370 train loss: 0.37212133407592773 test loss: 0.36419937014579773\n",
            "Epoch 370 train loss: 0.06857720762491226 test loss: 0.0697319284081459\n",
            "Epoch 370 train loss: 0.24950815737247467 test loss: 0.2596985101699829\n",
            "Epoch 370 train loss: 0.1557685285806656 test loss: 0.15637360513210297\n",
            "Epoch 370 train loss: 0.533341646194458 test loss: 0.5317350029945374\n",
            "Epoch 370 train loss: 0.2318856567144394 test loss: 0.23149576783180237\n",
            "Epoch 370 train loss: 0.1509343683719635 test loss: 0.14623600244522095\n",
            "Epoch 370 train loss: 0.28516334295272827 test loss: 0.2790089547634125\n",
            "Epoch 370 train loss: 0.044014789164066315 test loss: 0.047010257840156555\n",
            "Epoch 370 train loss: 0.10591688007116318 test loss: 0.10658780485391617\n",
            "Epoch 370 train loss: 0.18987609446048737 test loss: 0.18465600907802582\n",
            "Epoch 370 train loss: 0.6662188768386841 test loss: 0.6805078983306885\n",
            "Epoch 380 train loss: 0.7651089429855347 test loss: 0.7534804940223694\n",
            "Epoch 380 train loss: 0.2718560993671417 test loss: 0.2742699086666107\n",
            "Epoch 380 train loss: 0.25462964177131653 test loss: 0.25687792897224426\n",
            "Epoch 380 train loss: 0.2576654255390167 test loss: 0.25472623109817505\n",
            "Epoch 380 train loss: 0.6331190466880798 test loss: 0.6450676321983337\n",
            "Epoch 380 train loss: 0.2767396867275238 test loss: 0.2684847414493561\n",
            "Epoch 380 train loss: 0.11050329357385635 test loss: 0.11359903216362\n",
            "Epoch 380 train loss: 1.197316288948059 test loss: 1.1828815937042236\n",
            "Epoch 380 train loss: 0.42002174258232117 test loss: 0.4190302789211273\n",
            "Epoch 380 train loss: 0.13681617379188538 test loss: 0.13442538678646088\n",
            "Epoch 380 train loss: 0.2609441578388214 test loss: 0.257752388715744\n",
            "Epoch 380 train loss: 0.3994623124599457 test loss: 0.38121089339256287\n",
            "Epoch 380 train loss: 0.5784873366355896 test loss: 0.5731197595596313\n",
            "Epoch 380 train loss: 0.12921369075775146 test loss: 0.13249492645263672\n",
            "Epoch 380 train loss: 0.32476532459259033 test loss: 0.32434695959091187\n",
            "Epoch 380 train loss: 0.09935972094535828 test loss: 0.09505777060985565\n",
            "Epoch 380 train loss: 0.46300166845321655 test loss: 0.44430142641067505\n",
            "Epoch 380 train loss: 0.33690986037254333 test loss: 0.35173290967941284\n",
            "Epoch 380 train loss: 0.7660207748413086 test loss: 0.7738659977912903\n",
            "Epoch 380 train loss: 1.0105962753295898 test loss: 0.997848629951477\n",
            "Epoch 380 train loss: 0.2703076899051666 test loss: 0.2693382799625397\n",
            "Epoch 380 train loss: 0.4790396988391876 test loss: 0.4802250564098358\n",
            "Epoch 380 train loss: 0.44657373428344727 test loss: 0.4391772150993347\n",
            "Epoch 380 train loss: 0.46988561749458313 test loss: 0.4667707681655884\n",
            "Epoch 380 train loss: 0.20421496033668518 test loss: 0.20651088654994965\n",
            "Epoch 380 train loss: 0.22233614325523376 test loss: 0.22164537012577057\n",
            "Epoch 380 train loss: 0.08938822150230408 test loss: 0.08987060934305191\n",
            "Epoch 380 train loss: 0.20245805382728577 test loss: 0.20105285942554474\n",
            "Epoch 380 train loss: 0.3664310872554779 test loss: 0.38412225246429443\n",
            "Epoch 380 train loss: 0.1403471827507019 test loss: 0.14735199511051178\n",
            "Epoch 380 train loss: 0.4316716194152832 test loss: 0.41730931401252747\n",
            "Epoch 380 train loss: 0.37611597776412964 test loss: 0.37387871742248535\n",
            "Epoch 380 train loss: 0.0872327908873558 test loss: 0.08676403015851974\n",
            "Epoch 380 train loss: 0.557050347328186 test loss: 0.5632286667823792\n",
            "Epoch 380 train loss: 0.3600615859031677 test loss: 0.3571726381778717\n",
            "Epoch 380 train loss: 0.2659333646297455 test loss: 0.28084254264831543\n",
            "Epoch 380 train loss: 0.4899705946445465 test loss: 0.48282501101493835\n",
            "Epoch 380 train loss: 0.5615642070770264 test loss: 0.566902756690979\n",
            "Epoch 380 train loss: 0.35238057374954224 test loss: 0.3484719693660736\n",
            "Epoch 380 train loss: 0.05583546310663223 test loss: 0.05562813952565193\n",
            "Epoch 380 train loss: 0.30614063143730164 test loss: 0.2997036874294281\n",
            "Epoch 380 train loss: 0.24400117993354797 test loss: 0.24719032645225525\n",
            "Epoch 380 train loss: 0.2658200263977051 test loss: 0.26282399892807007\n",
            "Epoch 380 train loss: 0.2573520243167877 test loss: 0.2564128339290619\n",
            "Epoch 380 train loss: 0.18034526705741882 test loss: 0.1769418716430664\n",
            "Epoch 380 train loss: 0.20025475323200226 test loss: 0.20529364049434662\n",
            "Epoch 380 train loss: 0.04894304275512695 test loss: 0.04560493305325508\n",
            "Epoch 380 train loss: 0.06290370970964432 test loss: 0.06193080171942711\n",
            "Epoch 380 train loss: 0.22276556491851807 test loss: 0.23169872164726257\n",
            "Epoch 380 train loss: 0.40509676933288574 test loss: 0.40261903405189514\n",
            "Epoch 390 train loss: 0.8910058736801147 test loss: 0.8845286965370178\n",
            "Epoch 390 train loss: 0.36951106786727905 test loss: 0.3703714907169342\n",
            "Epoch 390 train loss: 1.4583858251571655 test loss: 1.4629155397415161\n",
            "Epoch 390 train loss: 0.14371894299983978 test loss: 0.1435796022415161\n",
            "Epoch 390 train loss: 0.6510758996009827 test loss: 0.6479393243789673\n",
            "Epoch 390 train loss: 0.21251007914543152 test loss: 0.2151940017938614\n",
            "Epoch 390 train loss: 0.2185588926076889 test loss: 0.21789142489433289\n",
            "Epoch 390 train loss: 0.5407252311706543 test loss: 0.5451797842979431\n",
            "Epoch 390 train loss: 0.3852296471595764 test loss: 0.3670145571231842\n",
            "Epoch 390 train loss: 0.4756952226161957 test loss: 0.47635698318481445\n",
            "Epoch 390 train loss: 0.6171892881393433 test loss: 0.6090272665023804\n",
            "Epoch 390 train loss: 0.3880939483642578 test loss: 0.38933131098747253\n",
            "Epoch 390 train loss: 1.0678668022155762 test loss: 1.0636712312698364\n",
            "Epoch 390 train loss: 0.1028270348906517 test loss: 0.10325232148170471\n",
            "Epoch 390 train loss: 0.6626949310302734 test loss: 0.6630245447158813\n",
            "Epoch 390 train loss: 0.12393416464328766 test loss: 0.1317400336265564\n",
            "Epoch 390 train loss: 0.6985962390899658 test loss: 0.6938360333442688\n",
            "Epoch 390 train loss: 0.6763862371444702 test loss: 0.6859392523765564\n",
            "Epoch 390 train loss: 0.957305908203125 test loss: 0.9642027020454407\n",
            "Epoch 390 train loss: 1.2746422290802002 test loss: 1.2765640020370483\n",
            "Epoch 390 train loss: 0.3278193473815918 test loss: 0.33197176456451416\n",
            "Epoch 390 train loss: 0.5069889426231384 test loss: 0.5166030526161194\n",
            "Epoch 390 train loss: 0.33416980504989624 test loss: 0.33760976791381836\n",
            "Epoch 390 train loss: 0.5119940638542175 test loss: 0.5192554593086243\n",
            "Epoch 390 train loss: 0.6771042943000793 test loss: 0.6798465251922607\n",
            "Epoch 390 train loss: 0.32634302973747253 test loss: 0.3346743881702423\n",
            "Epoch 390 train loss: 0.19139499962329865 test loss: 0.18742652237415314\n",
            "Epoch 390 train loss: 0.40733107924461365 test loss: 0.401340126991272\n",
            "Epoch 390 train loss: 1.1715316772460938 test loss: 1.1791921854019165\n",
            "Epoch 390 train loss: 0.12103095650672913 test loss: 0.11584354192018509\n",
            "Epoch 390 train loss: 0.4161178469657898 test loss: 0.4099028706550598\n",
            "Epoch 390 train loss: 0.5666975378990173 test loss: 0.5707764625549316\n",
            "Epoch 390 train loss: 0.34803450107574463 test loss: 0.34478437900543213\n",
            "Epoch 390 train loss: 0.842671275138855 test loss: 0.8462188839912415\n",
            "Epoch 390 train loss: 0.25296735763549805 test loss: 0.2556379437446594\n",
            "Epoch 390 train loss: 0.42826229333877563 test loss: 0.4360456168651581\n",
            "Epoch 390 train loss: 0.36843249201774597 test loss: 0.3648950457572937\n",
            "Epoch 390 train loss: 0.5918508768081665 test loss: 0.5914732813835144\n",
            "Epoch 390 train loss: 0.3985990285873413 test loss: 0.40218862891197205\n",
            "Epoch 390 train loss: 0.7292896509170532 test loss: 0.7285674214363098\n",
            "Epoch 390 train loss: 0.364159494638443 test loss: 0.3662327826023102\n",
            "Epoch 390 train loss: 0.10563460737466812 test loss: 0.10499517619609833\n",
            "Epoch 390 train loss: 0.41761109232902527 test loss: 0.4206748902797699\n",
            "Epoch 390 train loss: 0.12738582491874695 test loss: 0.1261465698480606\n",
            "Epoch 390 train loss: 0.2400931864976883 test loss: 0.24169562757015228\n",
            "Epoch 390 train loss: 0.5960092544555664 test loss: 0.5957530736923218\n",
            "Epoch 390 train loss: 0.0809435248374939 test loss: 0.07813431322574615\n",
            "Epoch 390 train loss: 0.7050438523292542 test loss: 0.693105161190033\n",
            "Epoch 390 train loss: 0.3212152123451233 test loss: 0.3204292058944702\n",
            "Epoch 390 train loss: 0.5287108421325684 test loss: 0.5275532007217407\n",
            "Epoch 400 train loss: 1.0649274587631226 test loss: 1.0496001243591309\n",
            "Epoch 400 train loss: 1.221585988998413 test loss: 1.2135193347930908\n",
            "Epoch 400 train loss: 0.8339236974716187 test loss: 0.8299793601036072\n",
            "Epoch 400 train loss: 0.24915073812007904 test loss: 0.24827955663204193\n",
            "Epoch 400 train loss: 0.7974064350128174 test loss: 0.8063673377037048\n",
            "Epoch 400 train loss: 0.19458720088005066 test loss: 0.19114312529563904\n",
            "Epoch 400 train loss: 0.27566492557525635 test loss: 0.2799254357814789\n",
            "Epoch 400 train loss: 1.0535798072814941 test loss: 1.0471383333206177\n",
            "Epoch 400 train loss: 0.36692434549331665 test loss: 0.3695509135723114\n",
            "Epoch 400 train loss: 0.836738109588623 test loss: 0.8129785656929016\n",
            "Epoch 400 train loss: 0.5550861954689026 test loss: 0.5536863207817078\n",
            "Epoch 400 train loss: 0.5004006028175354 test loss: 0.4868623912334442\n",
            "Epoch 400 train loss: 0.9452136754989624 test loss: 0.9490760564804077\n",
            "Epoch 400 train loss: 0.40035828948020935 test loss: 0.4050365686416626\n",
            "Epoch 400 train loss: 0.496299147605896 test loss: 0.4849821925163269\n",
            "Epoch 400 train loss: 0.13007307052612305 test loss: 0.13171714544296265\n",
            "Epoch 400 train loss: 0.8855106830596924 test loss: 0.8923643231391907\n",
            "Epoch 400 train loss: 0.8666596412658691 test loss: 0.8718254566192627\n",
            "Epoch 400 train loss: 1.1286383867263794 test loss: 1.1473124027252197\n",
            "Epoch 400 train loss: 0.9298050999641418 test loss: 0.9234505891799927\n",
            "Epoch 400 train loss: 0.4687458574771881 test loss: 0.4747423827648163\n",
            "Epoch 400 train loss: 0.7472832202911377 test loss: 0.7404079437255859\n",
            "Epoch 400 train loss: 0.8063986301422119 test loss: 0.8016335964202881\n",
            "Epoch 400 train loss: 0.5870856046676636 test loss: 0.5865041613578796\n",
            "Epoch 400 train loss: 1.0650053024291992 test loss: 1.0571353435516357\n",
            "Epoch 400 train loss: 0.7827970385551453 test loss: 0.7842286825180054\n",
            "Epoch 400 train loss: 0.24865365028381348 test loss: 0.22946472465991974\n",
            "Epoch 400 train loss: 0.8359814882278442 test loss: 0.8210018873214722\n",
            "Epoch 400 train loss: 0.8364667892456055 test loss: 0.8580366969108582\n",
            "Epoch 400 train loss: 0.13296222686767578 test loss: 0.13479913771152496\n",
            "Epoch 400 train loss: 0.45556458830833435 test loss: 0.45026695728302\n",
            "Epoch 400 train loss: 1.0640126466751099 test loss: 1.0604771375656128\n",
            "Epoch 400 train loss: 0.0964943990111351 test loss: 0.106474868953228\n",
            "Epoch 400 train loss: 1.094886064529419 test loss: 1.0957562923431396\n",
            "Epoch 400 train loss: 0.23399990797042847 test loss: 0.22768302261829376\n",
            "Epoch 400 train loss: 0.817773699760437 test loss: 0.8164588809013367\n",
            "Epoch 400 train loss: 0.6050023436546326 test loss: 0.6107032895088196\n",
            "Epoch 400 train loss: 0.6137154698371887 test loss: 0.626761257648468\n",
            "Epoch 400 train loss: 0.47838860750198364 test loss: 0.47473904490470886\n",
            "Epoch 400 train loss: 0.25079190731048584 test loss: 0.24718068540096283\n",
            "Epoch 400 train loss: 1.0308722257614136 test loss: 1.031241536140442\n",
            "Epoch 400 train loss: 0.1470341682434082 test loss: 0.1469765156507492\n",
            "Epoch 400 train loss: 0.8698132038116455 test loss: 0.87228924036026\n",
            "Epoch 400 train loss: 0.5391325354576111 test loss: 0.5253399610519409\n",
            "Epoch 400 train loss: 0.2407834380865097 test loss: 0.24833422899246216\n",
            "Epoch 400 train loss: 1.0429608821868896 test loss: 1.0356228351593018\n",
            "Epoch 400 train loss: 0.08556122332811356 test loss: 0.08398950845003128\n",
            "Epoch 400 train loss: 0.22356198728084564 test loss: 0.2279033064842224\n",
            "Epoch 400 train loss: 0.9695262908935547 test loss: 0.9806596040725708\n",
            "Epoch 400 train loss: 0.41346779465675354 test loss: 0.41961824893951416\n",
            "Saving model \n",
            "\n",
            "Done\n",
            "Epoch 410 train loss: 0.5253081321716309 test loss: 0.5361034870147705\n",
            "Epoch 410 train loss: 0.8309518694877625 test loss: 0.8275086283683777\n",
            "Epoch 410 train loss: 0.6637662053108215 test loss: 0.6143521070480347\n",
            "Epoch 410 train loss: 0.14969727396965027 test loss: 0.15154315531253815\n",
            "Epoch 410 train loss: 0.6333464980125427 test loss: 0.6323076486587524\n",
            "Epoch 410 train loss: 0.21569380164146423 test loss: 0.21083015203475952\n",
            "Epoch 410 train loss: 0.242160364985466 test loss: 0.23631314933300018\n",
            "Epoch 410 train loss: 0.6336082220077515 test loss: 0.640159010887146\n",
            "Epoch 410 train loss: 0.5541000366210938 test loss: 0.526330828666687\n",
            "Epoch 410 train loss: 0.1999356895685196 test loss: 0.21014738082885742\n",
            "Epoch 410 train loss: 0.5789726972579956 test loss: 0.5778118968009949\n",
            "Epoch 410 train loss: 0.39200782775878906 test loss: 0.378773957490921\n",
            "Epoch 410 train loss: 0.7894111275672913 test loss: 0.8029260039329529\n",
            "Epoch 410 train loss: 0.4329092502593994 test loss: 0.41887882351875305\n",
            "Epoch 410 train loss: 0.36710649728775024 test loss: 0.3518536686897278\n",
            "Epoch 410 train loss: 0.145019069314003 test loss: 0.1557290405035019\n",
            "Epoch 410 train loss: 0.7839973568916321 test loss: 0.7567403316497803\n",
            "Epoch 410 train loss: 0.6847450733184814 test loss: 0.6933154463768005\n",
            "Epoch 410 train loss: 1.3511216640472412 test loss: 1.3472578525543213\n",
            "Epoch 410 train loss: 0.8518565893173218 test loss: 0.8526929020881653\n",
            "Epoch 410 train loss: 0.4027629494667053 test loss: 0.40244391560554504\n",
            "Epoch 410 train loss: 0.5579540133476257 test loss: 0.5454654693603516\n",
            "Epoch 410 train loss: 0.35636061429977417 test loss: 0.3580707311630249\n",
            "Epoch 410 train loss: 0.5112192034721375 test loss: 0.5079265832901001\n",
            "Epoch 410 train loss: 0.9882425665855408 test loss: 0.9883604645729065\n",
            "Epoch 410 train loss: 0.47286081314086914 test loss: 0.46747860312461853\n",
            "Epoch 410 train loss: 0.1661382019519806 test loss: 0.14038361608982086\n",
            "Epoch 410 train loss: 0.30475836992263794 test loss: 0.29963892698287964\n",
            "Epoch 410 train loss: 0.5365257263183594 test loss: 0.6019815802574158\n",
            "Epoch 410 train loss: 0.1660592406988144 test loss: 0.1729993373155594\n",
            "Epoch 410 train loss: 0.4481695890426636 test loss: 0.45081424713134766\n",
            "Epoch 410 train loss: 0.9427782893180847 test loss: 0.9408205151557922\n",
            "Epoch 410 train loss: 0.06514480710029602 test loss: 0.06848710030317307\n",
            "Epoch 410 train loss: 1.4129722118377686 test loss: 1.4225964546203613\n",
            "Epoch 410 train loss: 0.4017983078956604 test loss: 0.39756035804748535\n",
            "Epoch 410 train loss: 0.6423724293708801 test loss: 0.6476201415061951\n",
            "Epoch 410 train loss: 0.28048864006996155 test loss: 0.2955365777015686\n",
            "Epoch 410 train loss: 0.5502327084541321 test loss: 0.5557224154472351\n",
            "Epoch 410 train loss: 0.4072563052177429 test loss: 0.41327112913131714\n",
            "Epoch 410 train loss: 0.2530424892902374 test loss: 0.2618291676044464\n",
            "Epoch 410 train loss: 0.6049519181251526 test loss: 0.6019551157951355\n",
            "Epoch 410 train loss: 0.16216495633125305 test loss: 0.15682901442050934\n",
            "Epoch 410 train loss: 0.5968390107154846 test loss: 0.5914973616600037\n",
            "Epoch 410 train loss: 0.5895720720291138 test loss: 0.561656653881073\n",
            "Epoch 410 train loss: 0.16180819272994995 test loss: 0.16276703774929047\n",
            "Epoch 410 train loss: 0.9643433094024658 test loss: 0.979484498500824\n",
            "Epoch 410 train loss: 0.05350038409233093 test loss: 0.04794536158442497\n",
            "Epoch 410 train loss: 0.10811220109462738 test loss: 0.1091722771525383\n",
            "Epoch 410 train loss: 0.5663591623306274 test loss: 0.5669875741004944\n",
            "Epoch 410 train loss: 0.3738853931427002 test loss: 0.3761935532093048\n",
            "Epoch 420 train loss: 0.7023283839225769 test loss: 0.7095211744308472\n",
            "Epoch 420 train loss: 0.37662920355796814 test loss: 0.383152037858963\n",
            "Epoch 420 train loss: 0.4851053059101105 test loss: 0.48507487773895264\n",
            "Epoch 420 train loss: 0.3469685912132263 test loss: 0.3494323492050171\n",
            "Epoch 420 train loss: 0.7550544142723083 test loss: 0.7623547315597534\n",
            "Epoch 420 train loss: 0.23089633882045746 test loss: 0.22871161997318268\n",
            "Epoch 420 train loss: 0.16331888735294342 test loss: 0.1610863357782364\n",
            "Epoch 420 train loss: 0.8685370087623596 test loss: 0.8573191165924072\n",
            "Epoch 420 train loss: 0.32817456126213074 test loss: 0.3318966329097748\n",
            "Epoch 420 train loss: 0.3915841579437256 test loss: 0.40264010429382324\n",
            "Epoch 420 train loss: 0.272643506526947 test loss: 0.2806563377380371\n",
            "Epoch 420 train loss: 0.1948632150888443 test loss: 0.19254261255264282\n",
            "Epoch 420 train loss: 0.6191732883453369 test loss: 0.6169017553329468\n",
            "Epoch 420 train loss: 0.17808890342712402 test loss: 0.18911150097846985\n",
            "Epoch 420 train loss: 0.26061469316482544 test loss: 0.25393006205558777\n",
            "Epoch 420 train loss: 0.06308899819850922 test loss: 0.06439251452684402\n",
            "Epoch 420 train loss: 0.6941959261894226 test loss: 0.6934899091720581\n",
            "Epoch 420 train loss: 0.38225257396698 test loss: 0.37552016973495483\n",
            "Epoch 420 train loss: 0.8530728220939636 test loss: 0.8429850935935974\n",
            "Epoch 420 train loss: 0.3098600506782532 test loss: 0.30231818556785583\n",
            "Epoch 420 train loss: 0.23519748449325562 test loss: 0.23698924481868744\n",
            "Epoch 420 train loss: 0.3569454550743103 test loss: 0.3638980984687805\n",
            "Epoch 420 train loss: 0.44855812191963196 test loss: 0.46110770106315613\n",
            "Epoch 420 train loss: 0.3863028585910797 test loss: 0.3836043179035187\n",
            "Epoch 420 train loss: 0.8731332421302795 test loss: 0.8729178309440613\n",
            "Epoch 420 train loss: 0.24252310395240784 test loss: 0.2441577911376953\n",
            "Epoch 420 train loss: 0.15792982280254364 test loss: 0.15561257302761078\n",
            "Epoch 420 train loss: 0.18326792120933533 test loss: 0.17990723252296448\n",
            "Epoch 420 train loss: 0.30325672030448914 test loss: 0.32842591404914856\n",
            "Epoch 420 train loss: 0.10874812304973602 test loss: 0.11351283639669418\n",
            "Epoch 420 train loss: 0.29694893956184387 test loss: 0.29288381338119507\n",
            "Epoch 420 train loss: 0.6830154061317444 test loss: 0.6986257433891296\n",
            "Epoch 420 train loss: 0.10230164229869843 test loss: 0.10650099813938141\n",
            "Epoch 420 train loss: 0.7526010274887085 test loss: 0.7654595375061035\n",
            "Epoch 420 train loss: 0.2055508941411972 test loss: 0.20142848789691925\n",
            "Epoch 420 train loss: 0.24511820077896118 test loss: 0.23811887204647064\n",
            "Epoch 420 train loss: 0.2627033293247223 test loss: 0.26604756712913513\n",
            "Epoch 420 train loss: 0.6009092330932617 test loss: 0.6087321639060974\n",
            "Epoch 420 train loss: 0.33252662420272827 test loss: 0.33379852771759033\n",
            "Epoch 420 train loss: 0.13781921565532684 test loss: 0.1336837112903595\n",
            "Epoch 420 train loss: 0.1983729600906372 test loss: 0.20555716753005981\n",
            "Epoch 420 train loss: 0.19158916175365448 test loss: 0.1889582872390747\n",
            "Epoch 420 train loss: 0.328608900308609 test loss: 0.3266562521457672\n",
            "Epoch 420 train loss: 0.33353906869888306 test loss: 0.33732303977012634\n",
            "Epoch 420 train loss: 0.1487303525209427 test loss: 0.14478367567062378\n",
            "Epoch 420 train loss: 0.8604851365089417 test loss: 0.8710612058639526\n",
            "Epoch 420 train loss: 0.07374055683612823 test loss: 0.0773981362581253\n",
            "Epoch 420 train loss: 0.1218954548239708 test loss: 0.14094609022140503\n",
            "Epoch 420 train loss: 0.18546825647354126 test loss: 0.1844547688961029\n",
            "Epoch 420 train loss: 0.5612816214561462 test loss: 0.5655515193939209\n",
            "Epoch 430 train loss: 0.6943942904472351 test loss: 0.6938138604164124\n",
            "Epoch 430 train loss: 0.17044144868850708 test loss: 0.16930320858955383\n",
            "Epoch 430 train loss: 0.4077880382537842 test loss: 0.38081204891204834\n",
            "Epoch 430 train loss: 0.20812144875526428 test loss: 0.2179623544216156\n",
            "Epoch 430 train loss: 0.7035555839538574 test loss: 0.6993458867073059\n",
            "Epoch 430 train loss: 0.14672325551509857 test loss: 0.14498484134674072\n",
            "Epoch 430 train loss: 0.13701939582824707 test loss: 0.13859422504901886\n",
            "Epoch 430 train loss: 0.6913130879402161 test loss: 0.6728776693344116\n",
            "Epoch 430 train loss: 0.30372926592826843 test loss: 0.3025777041912079\n",
            "Epoch 430 train loss: 0.18036411702632904 test loss: 0.18352274596691132\n",
            "Epoch 430 train loss: 0.21409916877746582 test loss: 0.2122942954301834\n",
            "Epoch 430 train loss: 0.24149438738822937 test loss: 0.20988737046718597\n",
            "Epoch 430 train loss: 0.6361549496650696 test loss: 0.6520121097564697\n",
            "Epoch 430 train loss: 0.07732444256544113 test loss: 0.06822621822357178\n",
            "Epoch 430 train loss: 0.31009888648986816 test loss: 0.2899529039859772\n",
            "Epoch 430 train loss: 0.10653698444366455 test loss: 0.107294000685215\n",
            "Epoch 430 train loss: 0.3593578636646271 test loss: 0.34834638237953186\n",
            "Epoch 430 train loss: 0.3097049295902252 test loss: 0.30833232402801514\n",
            "Epoch 430 train loss: 0.7268893718719482 test loss: 0.7264581918716431\n",
            "Epoch 430 train loss: 0.2193075716495514 test loss: 0.2195149064064026\n",
            "Epoch 430 train loss: 0.21551521122455597 test loss: 0.21735389530658722\n",
            "Epoch 430 train loss: 0.40202340483665466 test loss: 0.3991464674472809\n",
            "Epoch 430 train loss: 0.17002494633197784 test loss: 0.1762804388999939\n",
            "Epoch 430 train loss: 0.42081448435783386 test loss: 0.4253785312175751\n",
            "Epoch 430 train loss: 0.5824885368347168 test loss: 0.5898580551147461\n",
            "Epoch 430 train loss: 0.18412913382053375 test loss: 0.18371418118476868\n",
            "Epoch 430 train loss: 0.14266148209571838 test loss: 0.14272147417068481\n",
            "Epoch 430 train loss: 0.09806318581104279 test loss: 0.09410282224416733\n",
            "Epoch 430 train loss: 0.23671281337738037 test loss: 0.22518190741539001\n",
            "Epoch 430 train loss: 0.0522545650601387 test loss: 0.05102776736021042\n",
            "Epoch 430 train loss: 0.270830363035202 test loss: 0.2728630006313324\n",
            "Epoch 430 train loss: 0.5146679878234863 test loss: 0.5140547156333923\n",
            "Epoch 430 train loss: 0.0534164197742939 test loss: 0.05242187902331352\n",
            "Epoch 430 train loss: 0.5468131303787231 test loss: 0.5514503717422485\n",
            "Epoch 430 train loss: 0.1823125034570694 test loss: 0.1789344996213913\n",
            "Epoch 430 train loss: 0.31343451142311096 test loss: 0.311186820268631\n",
            "Epoch 430 train loss: 0.2408851981163025 test loss: 0.2323368936777115\n",
            "Epoch 430 train loss: 0.5269607305526733 test loss: 0.5237070322036743\n",
            "Epoch 430 train loss: 0.2919968068599701 test loss: 0.29298684000968933\n",
            "Epoch 430 train loss: 0.041066620498895645 test loss: 0.04574931040406227\n",
            "Epoch 430 train loss: 0.23533590137958527 test loss: 0.2390238493680954\n",
            "Epoch 430 train loss: 0.19192355871200562 test loss: 0.19394196569919586\n",
            "Epoch 430 train loss: 0.27382007241249084 test loss: 0.2718452215194702\n",
            "Epoch 430 train loss: 0.14056265354156494 test loss: 0.12746794521808624\n",
            "Epoch 430 train loss: 0.16390101611614227 test loss: 0.16194981336593628\n",
            "Epoch 430 train loss: 0.555946946144104 test loss: 0.5517178773880005\n",
            "Epoch 430 train loss: 0.05047896131873131 test loss: 0.05650147795677185\n",
            "Epoch 430 train loss: 0.07393111288547516 test loss: 0.09481211006641388\n",
            "Epoch 430 train loss: 0.13625308871269226 test loss: 0.14300118386745453\n",
            "Epoch 430 train loss: 0.563378095626831 test loss: 0.5668892860412598\n",
            "Epoch 440 train loss: 0.6233759522438049 test loss: 0.6280940175056458\n",
            "Epoch 440 train loss: 0.15486536920070648 test loss: 0.15491275489330292\n",
            "Epoch 440 train loss: 0.2992696464061737 test loss: 0.29875728487968445\n",
            "Epoch 440 train loss: 0.20843617618083954 test loss: 0.217000812292099\n",
            "Epoch 440 train loss: 0.7019386887550354 test loss: 0.7034040689468384\n",
            "Epoch 440 train loss: 0.13127735257148743 test loss: 0.14029742777347565\n",
            "Epoch 440 train loss: 0.13871513307094574 test loss: 0.1406603902578354\n",
            "Epoch 440 train loss: 0.7849244475364685 test loss: 0.7768808007240295\n",
            "Epoch 440 train loss: 0.25179997086524963 test loss: 0.24562276899814606\n",
            "Epoch 440 train loss: 0.17512759566307068 test loss: 0.17517083883285522\n",
            "Epoch 440 train loss: 0.18944178521633148 test loss: 0.1910039782524109\n",
            "Epoch 440 train loss: 0.11972679942846298 test loss: 0.12168963998556137\n",
            "Epoch 440 train loss: 0.5883026123046875 test loss: 0.5763536691665649\n",
            "Epoch 440 train loss: 0.08521066606044769 test loss: 0.09511115401983261\n",
            "Epoch 440 train loss: 0.2132706344127655 test loss: 0.2139492928981781\n",
            "Epoch 440 train loss: 0.06157650426030159 test loss: 0.0644170269370079\n",
            "Epoch 440 train loss: 0.2560443580150604 test loss: 0.25291067361831665\n",
            "Epoch 440 train loss: 0.27875855565071106 test loss: 0.27233436703681946\n",
            "Epoch 440 train loss: 0.6608173847198486 test loss: 0.6706944704055786\n",
            "Epoch 440 train loss: 0.33741793036460876 test loss: 0.33204784989356995\n",
            "Epoch 440 train loss: 0.24297937750816345 test loss: 0.23652935028076172\n",
            "Epoch 440 train loss: 0.2607356607913971 test loss: 0.25881221890449524\n",
            "Epoch 440 train loss: 0.2090899497270584 test loss: 0.213483527302742\n",
            "Epoch 440 train loss: 0.4537380635738373 test loss: 0.45966869592666626\n",
            "Epoch 440 train loss: 0.3324454128742218 test loss: 0.3196185529232025\n",
            "Epoch 440 train loss: 0.18375398218631744 test loss: 0.18692679703235626\n",
            "Epoch 440 train loss: 0.1160162091255188 test loss: 0.1147955060005188\n",
            "Epoch 440 train loss: 0.08613179624080658 test loss: 0.08242391049861908\n",
            "Epoch 440 train loss: 0.16755492985248566 test loss: 0.16319453716278076\n",
            "Epoch 440 train loss: 0.048181504011154175 test loss: 0.05047113448381424\n",
            "Epoch 440 train loss: 0.2117602676153183 test loss: 0.21504747867584229\n",
            "Epoch 440 train loss: 0.4848213791847229 test loss: 0.5009429454803467\n",
            "Epoch 440 train loss: 0.06304479390382767 test loss: 0.055581867694854736\n",
            "Epoch 440 train loss: 0.5643548965454102 test loss: 0.5563940405845642\n",
            "Epoch 440 train loss: 0.1713554859161377 test loss: 0.16371861100196838\n",
            "Epoch 440 train loss: 0.0714278593659401 test loss: 0.07376159727573395\n",
            "Epoch 440 train loss: 0.2140914797782898 test loss: 0.20913077890872955\n",
            "Epoch 440 train loss: 0.652540922164917 test loss: 0.659207820892334\n",
            "Epoch 440 train loss: 0.31624162197113037 test loss: 0.3260241746902466\n",
            "Epoch 440 train loss: 0.06184624135494232 test loss: 0.07223944365978241\n",
            "Epoch 440 train loss: 0.37449911236763 test loss: 0.3791247010231018\n",
            "Epoch 440 train loss: 0.18589411675930023 test loss: 0.19066298007965088\n",
            "Epoch 440 train loss: 0.16547486186027527 test loss: 0.15996767580509186\n",
            "Epoch 440 train loss: 0.09677408635616302 test loss: 0.09814739227294922\n",
            "Epoch 440 train loss: 0.16229727864265442 test loss: 0.16275182366371155\n",
            "Epoch 440 train loss: 0.3181769549846649 test loss: 0.31895753741264343\n",
            "Epoch 440 train loss: 0.05864686518907547 test loss: 0.058693017810583115\n",
            "Epoch 440 train loss: 0.04624955356121063 test loss: 0.047164276242256165\n",
            "Epoch 440 train loss: 0.17763307690620422 test loss: 0.17865978181362152\n",
            "Epoch 440 train loss: 0.5088724493980408 test loss: 0.5021956562995911\n",
            "Epoch 450 train loss: 0.8323954343795776 test loss: 0.8346333503723145\n",
            "Epoch 450 train loss: 0.49112966656684875 test loss: 0.49598753452301025\n",
            "Epoch 450 train loss: 0.5455305576324463 test loss: 0.5385912656784058\n",
            "Epoch 450 train loss: 0.5009035468101501 test loss: 0.5126193165779114\n",
            "Epoch 450 train loss: 0.8290229439735413 test loss: 0.8205708861351013\n",
            "Epoch 450 train loss: 0.18882235884666443 test loss: 0.19175821542739868\n",
            "Epoch 450 train loss: 0.16394536197185516 test loss: 0.16813871264457703\n",
            "Epoch 450 train loss: 0.8859831690788269 test loss: 0.8986077308654785\n",
            "Epoch 450 train loss: 0.2983081340789795 test loss: 0.2962605059146881\n",
            "Epoch 450 train loss: 0.20479685068130493 test loss: 0.20125743746757507\n",
            "Epoch 450 train loss: 0.2165779024362564 test loss: 0.2112300843000412\n",
            "Epoch 450 train loss: 0.47463101148605347 test loss: 0.49163660407066345\n",
            "Epoch 450 train loss: 0.6603204607963562 test loss: 0.6469327807426453\n",
            "Epoch 450 train loss: 0.3805917501449585 test loss: 0.3840041756629944\n",
            "Epoch 450 train loss: 0.23491913080215454 test loss: 0.21656714379787445\n",
            "Epoch 450 train loss: 0.10678550601005554 test loss: 0.10047625005245209\n",
            "Epoch 450 train loss: 0.40091291069984436 test loss: 0.40951794385910034\n",
            "Epoch 450 train loss: 0.3802328407764435 test loss: 0.37572407722473145\n",
            "Epoch 450 train loss: 0.6052498817443848 test loss: 0.5919652581214905\n",
            "Epoch 450 train loss: 0.5794221758842468 test loss: 0.5786808133125305\n",
            "Epoch 450 train loss: 0.2763260006904602 test loss: 0.28421956300735474\n",
            "Epoch 450 train loss: 0.5328940749168396 test loss: 0.5330323576927185\n",
            "Epoch 450 train loss: 0.19605092704296112 test loss: 0.19668830931186676\n",
            "Epoch 450 train loss: 0.4311484098434448 test loss: 0.43223753571510315\n",
            "Epoch 450 train loss: 0.29360878467559814 test loss: 0.2991270422935486\n",
            "Epoch 450 train loss: 0.21636037528514862 test loss: 0.21686872839927673\n",
            "Epoch 450 train loss: 0.10332469642162323 test loss: 0.1012161448597908\n",
            "Epoch 450 train loss: 0.1449197679758072 test loss: 0.14666028320789337\n",
            "Epoch 450 train loss: 0.18885628879070282 test loss: 0.20101456344127655\n",
            "Epoch 450 train loss: 0.05342576652765274 test loss: 0.05029820278286934\n",
            "Epoch 450 train loss: 0.3125725984573364 test loss: 0.30897077918052673\n",
            "Epoch 450 train loss: 0.5303910374641418 test loss: 0.5232046246528625\n",
            "Epoch 450 train loss: 0.0727686807513237 test loss: 0.06986348330974579\n",
            "Epoch 450 train loss: 1.0755277872085571 test loss: 1.0593181848526\n",
            "Epoch 450 train loss: 0.2288311868906021 test loss: 0.23161627352237701\n",
            "Epoch 450 train loss: 0.3295721113681793 test loss: 0.3505873382091522\n",
            "Epoch 450 train loss: 0.2615536153316498 test loss: 0.2577536106109619\n",
            "Epoch 450 train loss: 0.6462021470069885 test loss: 0.637677788734436\n",
            "Epoch 450 train loss: 0.41236230731010437 test loss: 0.4181736707687378\n",
            "Epoch 450 train loss: 0.06720907986164093 test loss: 0.06869226694107056\n",
            "Epoch 450 train loss: 0.2092808336019516 test loss: 0.2102912962436676\n",
            "Epoch 450 train loss: 0.19099856913089752 test loss: 0.19054827094078064\n",
            "Epoch 450 train loss: 0.3382877707481384 test loss: 0.32908350229263306\n",
            "Epoch 450 train loss: 0.3579595685005188 test loss: 0.34763088822364807\n",
            "Epoch 450 train loss: 0.17647510766983032 test loss: 0.1746494024991989\n",
            "Epoch 450 train loss: 0.2863127291202545 test loss: 0.28674599528312683\n",
            "Epoch 450 train loss: 0.06523428857326508 test loss: 0.06558508425951004\n",
            "Epoch 450 train loss: 0.08505916595458984 test loss: 0.08672423660755157\n",
            "Epoch 450 train loss: 0.17774799466133118 test loss: 0.1763501763343811\n",
            "Epoch 450 train loss: 0.2953207194805145 test loss: 0.26162487268447876\n",
            "Epoch 460 train loss: 0.42088258266448975 test loss: 0.42572203278541565\n",
            "Epoch 460 train loss: 0.09740652143955231 test loss: 0.09372228384017944\n",
            "Epoch 460 train loss: 0.41031989455223083 test loss: 0.409472793340683\n",
            "Epoch 460 train loss: 0.20478229224681854 test loss: 0.2073332667350769\n",
            "Epoch 460 train loss: 0.6229202151298523 test loss: 0.6246456503868103\n",
            "Epoch 460 train loss: 0.2408616691827774 test loss: 0.22769306600093842\n",
            "Epoch 460 train loss: 0.21295899152755737 test loss: 0.22908763587474823\n",
            "Epoch 460 train loss: 0.6086570024490356 test loss: 0.6366473436355591\n",
            "Epoch 460 train loss: 0.2731263339519501 test loss: 0.276310533285141\n",
            "Epoch 460 train loss: 0.1826537549495697 test loss: 0.18730169534683228\n",
            "Epoch 460 train loss: 0.17125631868839264 test loss: 0.16683945059776306\n",
            "Epoch 460 train loss: 0.1314578503370285 test loss: 0.1324852705001831\n",
            "Epoch 460 train loss: 0.7928466200828552 test loss: 0.784651517868042\n",
            "Epoch 460 train loss: 0.13158191740512848 test loss: 0.12664687633514404\n",
            "Epoch 460 train loss: 0.24043339490890503 test loss: 0.2447192370891571\n",
            "Epoch 460 train loss: 0.03598590940237045 test loss: 0.03541697561740875\n",
            "Epoch 460 train loss: 0.5420382618904114 test loss: 0.5489052534103394\n",
            "Epoch 460 train loss: 0.2496090829372406 test loss: 0.24538376927375793\n",
            "Epoch 460 train loss: 0.8457719087600708 test loss: 0.8447125554084778\n",
            "Epoch 460 train loss: 0.2729044258594513 test loss: 0.26159510016441345\n",
            "Epoch 460 train loss: 0.3180597722530365 test loss: 0.32421600818634033\n",
            "Epoch 460 train loss: 0.4105719327926636 test loss: 0.40756794810295105\n",
            "Epoch 460 train loss: 0.21745160222053528 test loss: 0.23477405309677124\n",
            "Epoch 460 train loss: 0.503741443157196 test loss: 0.4947454035282135\n",
            "Epoch 460 train loss: 0.1473299264907837 test loss: 0.13865330815315247\n",
            "Epoch 460 train loss: 0.1908896267414093 test loss: 0.1887313425540924\n",
            "Epoch 460 train loss: 0.14651703834533691 test loss: 0.13657337427139282\n",
            "Epoch 460 train loss: 0.12705667316913605 test loss: 0.1284024715423584\n",
            "Epoch 460 train loss: 0.35528942942619324 test loss: 0.37602823972702026\n",
            "Epoch 460 train loss: 0.09068416059017181 test loss: 0.08656920492649078\n",
            "Epoch 460 train loss: 0.21223975718021393 test loss: 0.216043159365654\n",
            "Epoch 460 train loss: 0.3981512188911438 test loss: 0.41283726692199707\n",
            "Epoch 460 train loss: 0.060671862214803696 test loss: 0.06110524386167526\n",
            "Epoch 460 train loss: 0.6272239089012146 test loss: 0.6087685823440552\n",
            "Epoch 460 train loss: 0.16641321778297424 test loss: 0.16883710026741028\n",
            "Epoch 460 train loss: 0.30131855607032776 test loss: 0.29391828179359436\n",
            "Epoch 460 train loss: 0.3380422592163086 test loss: 0.4267718195915222\n",
            "Epoch 460 train loss: 0.5716963410377502 test loss: 0.5705108046531677\n",
            "Epoch 460 train loss: 0.3353419601917267 test loss: 0.34567365050315857\n",
            "Epoch 460 train loss: 0.07803963124752045 test loss: 0.0717577114701271\n",
            "Epoch 460 train loss: 0.10211000591516495 test loss: 0.10239934176206589\n",
            "Epoch 460 train loss: 0.21790912747383118 test loss: 0.21813932061195374\n",
            "Epoch 460 train loss: 0.10630829632282257 test loss: 0.10909651964902878\n",
            "Epoch 460 train loss: 0.1490967571735382 test loss: 0.145935520529747\n",
            "Epoch 460 train loss: 0.16165919601917267 test loss: 0.16491730511188507\n",
            "Epoch 460 train loss: 0.1320478618144989 test loss: 0.13661396503448486\n",
            "Epoch 460 train loss: 0.05926810950040817 test loss: 0.05924384668469429\n",
            "Epoch 460 train loss: 0.1161801815032959 test loss: 0.11139104515314102\n",
            "Epoch 460 train loss: 0.10699031502008438 test loss: 0.10322363674640656\n",
            "Epoch 460 train loss: 0.26188376545906067 test loss: 0.26103460788726807\n",
            "Epoch 470 train loss: 0.34074512124061584 test loss: 0.33885714411735535\n",
            "Epoch 470 train loss: 0.08247341960668564 test loss: 0.08237839490175247\n",
            "Epoch 470 train loss: 0.4125406742095947 test loss: 0.4106031060218811\n",
            "Epoch 470 train loss: 0.07736766338348389 test loss: 0.09207841008901596\n",
            "Epoch 470 train loss: 0.6912668943405151 test loss: 0.6893447637557983\n",
            "Epoch 470 train loss: 0.07839516550302505 test loss: 0.07930751144886017\n",
            "Epoch 470 train loss: 0.12051105499267578 test loss: 0.12212389707565308\n",
            "Epoch 470 train loss: 0.43083396553993225 test loss: 0.44102293252944946\n",
            "Epoch 470 train loss: 0.26240959763526917 test loss: 0.2729494869709015\n",
            "Epoch 470 train loss: 0.11364997923374176 test loss: 0.09647456556558609\n",
            "Epoch 470 train loss: 0.10682042688131332 test loss: 0.10579219460487366\n",
            "Epoch 470 train loss: 0.1339026689529419 test loss: 0.12631183862686157\n",
            "Epoch 470 train loss: 0.49204781651496887 test loss: 0.4842470586299896\n",
            "Epoch 470 train loss: 0.06307467818260193 test loss: 0.06077482923865318\n",
            "Epoch 470 train loss: 0.16928599774837494 test loss: 0.1671891212463379\n",
            "Epoch 470 train loss: 0.03963875398039818 test loss: 0.0445503368973732\n",
            "Epoch 470 train loss: 0.24509865045547485 test loss: 0.25927096605300903\n",
            "Epoch 470 train loss: 0.27059999108314514 test loss: 0.2632739543914795\n",
            "Epoch 470 train loss: 0.5491878986358643 test loss: 0.5496760010719299\n",
            "Epoch 470 train loss: 0.13219046592712402 test loss: 0.12383244931697845\n",
            "Epoch 470 train loss: 0.23567602038383484 test loss: 0.23414301872253418\n",
            "Epoch 470 train loss: 0.41168421506881714 test loss: 0.41719844937324524\n",
            "Epoch 470 train loss: 0.1851358711719513 test loss: 0.17706476151943207\n",
            "Epoch 470 train loss: 0.4916899502277374 test loss: 0.4949018061161041\n",
            "Epoch 470 train loss: 0.0962076485157013 test loss: 0.10201369225978851\n",
            "Epoch 470 train loss: 0.13964565098285675 test loss: 0.14123226702213287\n",
            "Epoch 470 train loss: 0.09003235399723053 test loss: 0.08989474922418594\n",
            "Epoch 470 train loss: 0.08684978634119034 test loss: 0.08867668360471725\n",
            "Epoch 470 train loss: 0.21041886508464813 test loss: 0.21781422197818756\n",
            "Epoch 470 train loss: 0.04696890339255333 test loss: 0.039598703384399414\n",
            "Epoch 470 train loss: 0.2079475075006485 test loss: 0.20242643356323242\n",
            "Epoch 470 train loss: 0.1836741715669632 test loss: 0.19070515036582947\n",
            "Epoch 470 train loss: 0.083610400557518 test loss: 0.08573556691408157\n",
            "Epoch 470 train loss: 0.3846001625061035 test loss: 0.3872813582420349\n",
            "Epoch 470 train loss: 0.14656177163124084 test loss: 0.1552785038948059\n",
            "Epoch 470 train loss: 0.2834427058696747 test loss: 0.28372982144355774\n",
            "Epoch 470 train loss: 0.23639139533042908 test loss: 0.29733631014823914\n",
            "Epoch 470 train loss: 0.4241058826446533 test loss: 0.42870327830314636\n",
            "Epoch 470 train loss: 0.29307910799980164 test loss: 0.29100605845451355\n",
            "Epoch 470 train loss: 0.04313252866268158 test loss: 0.043915800750255585\n",
            "Epoch 470 train loss: 0.1413760483264923 test loss: 0.13636237382888794\n",
            "Epoch 470 train loss: 0.1383080631494522 test loss: 0.13883426785469055\n",
            "Epoch 470 train loss: 0.07440420240163803 test loss: 0.07420767843723297\n",
            "Epoch 470 train loss: 0.03936530649662018 test loss: 0.0774354636669159\n",
            "Epoch 470 train loss: 0.11699000746011734 test loss: 0.11934056133031845\n",
            "Epoch 470 train loss: 0.08309277147054672 test loss: 0.08208852261304855\n",
            "Epoch 470 train loss: 0.10134687274694443 test loss: 0.10065091401338577\n",
            "Epoch 470 train loss: 0.13949203491210938 test loss: 0.11462721973657608\n",
            "Epoch 470 train loss: 0.14337213337421417 test loss: 0.13992179930210114\n",
            "Epoch 470 train loss: 0.28147944808006287 test loss: 0.2797393500804901\n",
            "Epoch 480 train loss: 0.32021430134773254 test loss: 0.31104570627212524\n",
            "Epoch 480 train loss: 0.08352527022361755 test loss: 0.08667492866516113\n",
            "Epoch 480 train loss: 0.5210056304931641 test loss: 0.5343561172485352\n",
            "Epoch 480 train loss: 0.131085604429245 test loss: 0.13069501519203186\n",
            "Epoch 480 train loss: 0.5648640394210815 test loss: 0.5612038373947144\n",
            "Epoch 480 train loss: 0.12394074350595474 test loss: 0.13198032975196838\n",
            "Epoch 480 train loss: 0.14993983507156372 test loss: 0.14751987159252167\n",
            "Epoch 480 train loss: 0.4864763617515564 test loss: 0.47769251465797424\n",
            "Epoch 480 train loss: 0.24347315728664398 test loss: 0.23842185735702515\n",
            "Epoch 480 train loss: 0.08197595924139023 test loss: 0.09183304756879807\n",
            "Epoch 480 train loss: 0.1416403353214264 test loss: 0.1439296007156372\n",
            "Epoch 480 train loss: 0.10954157263040543 test loss: 0.11822104454040527\n",
            "Epoch 480 train loss: 0.49537965655326843 test loss: 0.5010716915130615\n",
            "Epoch 480 train loss: 0.08066891878843307 test loss: 0.08656465262174606\n",
            "Epoch 480 train loss: 0.1566523164510727 test loss: 0.1606958955526352\n",
            "Epoch 480 train loss: 0.04475456848740578 test loss: 0.04677053168416023\n",
            "Epoch 480 train loss: 0.44010061025619507 test loss: 0.44741278886795044\n",
            "Epoch 480 train loss: 0.25157296657562256 test loss: 0.24943500757217407\n",
            "Epoch 480 train loss: 0.8711938858032227 test loss: 0.8686375021934509\n",
            "Epoch 480 train loss: 0.3834652304649353 test loss: 0.37999802827835083\n",
            "Epoch 480 train loss: 0.2661127746105194 test loss: 0.2677152156829834\n",
            "Epoch 480 train loss: 0.46020737290382385 test loss: 0.4582344591617584\n",
            "Epoch 480 train loss: 0.23238186538219452 test loss: 0.2301846593618393\n",
            "Epoch 480 train loss: 0.47986435890197754 test loss: 0.47427499294281006\n",
            "Epoch 480 train loss: 0.15217246115207672 test loss: 0.16340039670467377\n",
            "Epoch 480 train loss: 0.19593816995620728 test loss: 0.20072388648986816\n",
            "Epoch 480 train loss: 0.09230627119541168 test loss: 0.08958641439676285\n",
            "Epoch 480 train loss: 0.06273342669010162 test loss: 0.06639031320810318\n",
            "Epoch 480 train loss: 0.1188996210694313 test loss: 0.11504033952951431\n",
            "Epoch 480 train loss: 0.0787629783153534 test loss: 0.08029522746801376\n",
            "Epoch 480 train loss: 0.22994773089885712 test loss: 0.23047159612178802\n",
            "Epoch 480 train loss: 0.3035731315612793 test loss: 0.3063146471977234\n",
            "Epoch 480 train loss: 0.04770863428711891 test loss: 0.04646903648972511\n",
            "Epoch 480 train loss: 0.8343392610549927 test loss: 0.8332363963127136\n",
            "Epoch 480 train loss: 0.14019788801670074 test loss: 0.137049600481987\n",
            "Epoch 480 train loss: 0.2973354756832123 test loss: 0.29650264978408813\n",
            "Epoch 480 train loss: 0.1415112316608429 test loss: 0.14489583671092987\n",
            "Epoch 480 train loss: 0.6827465891838074 test loss: 0.6753836274147034\n",
            "Epoch 480 train loss: 0.2448011189699173 test loss: 0.24376952648162842\n",
            "Epoch 480 train loss: 0.053316593170166016 test loss: 0.05039924383163452\n",
            "Epoch 480 train loss: 0.12173812836408615 test loss: 0.11848850548267365\n",
            "Epoch 480 train loss: 0.12358324229717255 test loss: 0.12570112943649292\n",
            "Epoch 480 train loss: 0.09351669251918793 test loss: 0.09004263579845428\n",
            "Epoch 480 train loss: 0.03534485772252083 test loss: 0.037473466247320175\n",
            "Epoch 480 train loss: 0.11046736687421799 test loss: 0.11324384808540344\n",
            "Epoch 480 train loss: 0.16631725430488586 test loss: 0.1633550077676773\n",
            "Epoch 480 train loss: 0.03410608321428299 test loss: 0.036078933626413345\n",
            "Epoch 480 train loss: 0.04654074087738991 test loss: 0.051487457007169724\n",
            "Epoch 480 train loss: 0.06703616678714752 test loss: 0.06721820682287216\n",
            "Epoch 480 train loss: 0.49849551916122437 test loss: 0.49447187781333923\n",
            "Epoch 490 train loss: 0.6799673438072205 test loss: 0.6823667287826538\n",
            "Epoch 490 train loss: 0.06680193543434143 test loss: 0.06514596194028854\n",
            "Epoch 490 train loss: 0.34484320878982544 test loss: 0.3360942602157593\n",
            "Epoch 490 train loss: 0.04151822626590729 test loss: 0.08846030384302139\n",
            "Epoch 490 train loss: 0.6107122898101807 test loss: 0.6113285422325134\n",
            "Epoch 490 train loss: 0.17246869206428528 test loss: 0.18833619356155396\n",
            "Epoch 490 train loss: 0.16630932688713074 test loss: 0.1612899899482727\n",
            "Epoch 490 train loss: 0.45003050565719604 test loss: 0.45432955026626587\n",
            "Epoch 490 train loss: 0.19020938873291016 test loss: 0.17672835290431976\n",
            "Epoch 490 train loss: 0.048263099044561386 test loss: 0.04724067077040672\n",
            "Epoch 490 train loss: 0.08220093697309494 test loss: 0.08460012823343277\n",
            "Epoch 490 train loss: 0.1678168773651123 test loss: 0.16250845789909363\n",
            "Epoch 490 train loss: 0.48602864146232605 test loss: 0.48521748185157776\n",
            "Epoch 490 train loss: 0.09175752848386765 test loss: 0.0920882299542427\n",
            "Epoch 490 train loss: 0.15265606343746185 test loss: 0.1555819809436798\n",
            "Epoch 490 train loss: 0.05686614662408829 test loss: 0.057722244411706924\n",
            "Epoch 490 train loss: 0.26481184363365173 test loss: 0.27152079343795776\n",
            "Epoch 490 train loss: 0.3145937919616699 test loss: 0.2841745615005493\n",
            "Epoch 490 train loss: 0.781852662563324 test loss: 0.7835497260093689\n",
            "Epoch 490 train loss: 0.13241742551326752 test loss: 0.13499078154563904\n",
            "Epoch 490 train loss: 0.249021977186203 test loss: 0.2475200742483139\n",
            "Epoch 490 train loss: 0.3924348056316376 test loss: 0.3958742022514343\n",
            "Epoch 490 train loss: 0.194813534617424 test loss: 0.1947088986635208\n",
            "Epoch 490 train loss: 0.4472670555114746 test loss: 0.44676339626312256\n",
            "Epoch 490 train loss: 0.19604477286338806 test loss: 0.1904963105916977\n",
            "Epoch 490 train loss: 0.21336984634399414 test loss: 0.21344627439975739\n",
            "Epoch 490 train loss: 0.07689424604177475 test loss: 0.08018877357244492\n",
            "Epoch 490 train loss: 0.08962523192167282 test loss: 0.08784982562065125\n",
            "Epoch 490 train loss: 0.10642272233963013 test loss: 0.11695712804794312\n",
            "Epoch 490 train loss: 0.04321802034974098 test loss: 0.041954439133405685\n",
            "Epoch 490 train loss: 0.23766383528709412 test loss: 0.25272196531295776\n",
            "Epoch 490 train loss: 0.4419301450252533 test loss: 0.41706690192222595\n",
            "Epoch 490 train loss: 0.052985042333602905 test loss: 0.04863045737147331\n",
            "Epoch 490 train loss: 0.3630392551422119 test loss: 0.38566988706588745\n",
            "Epoch 490 train loss: 0.17056037485599518 test loss: 0.17473913729190826\n",
            "Epoch 490 train loss: 0.29017454385757446 test loss: 0.2896163761615753\n",
            "Epoch 490 train loss: 0.1330050379037857 test loss: 0.19572195410728455\n",
            "Epoch 490 train loss: 0.4993344247341156 test loss: 0.4969140887260437\n",
            "Epoch 490 train loss: 0.26591020822525024 test loss: 0.264954537153244\n",
            "Epoch 490 train loss: 0.05068532004952431 test loss: 0.05083443596959114\n",
            "Epoch 490 train loss: 0.17296157777309418 test loss: 0.17627912759780884\n",
            "Epoch 490 train loss: 0.13827234506607056 test loss: 0.1380593627691269\n",
            "Epoch 490 train loss: 0.07165924459695816 test loss: 0.07677926123142242\n",
            "Epoch 490 train loss: 0.02566278725862503 test loss: 0.031315114349126816\n",
            "Epoch 490 train loss: 0.1140686422586441 test loss: 0.1105179488658905\n",
            "Epoch 490 train loss: 0.22424224019050598 test loss: 0.2208365648984909\n",
            "Epoch 490 train loss: 0.05191532522439957 test loss: 0.052667971700429916\n",
            "Epoch 490 train loss: 0.04574911668896675 test loss: 0.044346537441015244\n",
            "Epoch 490 train loss: 0.10172949731349945 test loss: 0.10120436549186707\n",
            "Epoch 490 train loss: 0.4612060785293579 test loss: 0.5011822581291199\n",
            "Epoch 500 train loss: 0.5793181657791138 test loss: 0.5817216634750366\n",
            "Epoch 500 train loss: 0.04154564440250397 test loss: 0.04460941627621651\n",
            "Epoch 500 train loss: 0.34643086791038513 test loss: 0.3521800637245178\n",
            "Epoch 500 train loss: 0.02193521521985531 test loss: 0.027555638924241066\n",
            "Epoch 500 train loss: 0.48719820380210876 test loss: 0.4898114502429962\n",
            "Epoch 500 train loss: 0.08481086045503616 test loss: 0.09037978947162628\n",
            "Epoch 500 train loss: 0.07783138751983643 test loss: 0.07989366352558136\n",
            "Epoch 500 train loss: 0.31668397784233093 test loss: 0.319265753030777\n",
            "Epoch 500 train loss: 0.18336722254753113 test loss: 0.1836559772491455\n",
            "Epoch 500 train loss: 0.06956826895475388 test loss: 0.07069552689790726\n",
            "Epoch 500 train loss: 0.08785127103328705 test loss: 0.09181869775056839\n",
            "Epoch 500 train loss: 0.09947560727596283 test loss: 0.10289263725280762\n",
            "Epoch 500 train loss: 0.4260718822479248 test loss: 0.4313191771507263\n",
            "Epoch 500 train loss: 0.05020546913146973 test loss: 0.050657130777835846\n",
            "Epoch 500 train loss: 0.11527387797832489 test loss: 0.11666355282068253\n",
            "Epoch 500 train loss: 0.04857442155480385 test loss: 0.04585220664739609\n",
            "Epoch 500 train loss: 0.21901676058769226 test loss: 0.21625544130802155\n",
            "Epoch 500 train loss: 0.2553395926952362 test loss: 0.2574521005153656\n",
            "Epoch 500 train loss: 0.6164589524269104 test loss: 0.6167439222335815\n",
            "Epoch 500 train loss: 0.100727379322052 test loss: 0.09667456150054932\n",
            "Epoch 500 train loss: 0.1947624683380127 test loss: 0.18354514241218567\n",
            "Epoch 500 train loss: 0.4359935522079468 test loss: 0.437798410654068\n",
            "Epoch 500 train loss: 0.16335263848304749 test loss: 0.1680581420660019\n",
            "Epoch 500 train loss: 0.4049093723297119 test loss: 0.3892214596271515\n",
            "Epoch 500 train loss: 0.09278551489114761 test loss: 0.09858845919370651\n",
            "Epoch 500 train loss: 0.3275711238384247 test loss: 0.3264888525009155\n",
            "Epoch 500 train loss: 0.10427030175924301 test loss: 0.1031426414847374\n",
            "Epoch 500 train loss: 0.08675388246774673 test loss: 0.08427053689956665\n",
            "Epoch 500 train loss: 0.28479665517807007 test loss: 0.297073632478714\n",
            "Epoch 500 train loss: 0.08795066922903061 test loss: 0.09262684732675552\n",
            "Epoch 500 train loss: 0.20146366953849792 test loss: 0.19730772078037262\n",
            "Epoch 500 train loss: 0.2013208419084549 test loss: 0.20757679641246796\n",
            "Epoch 500 train loss: 0.04565945640206337 test loss: 0.045935265719890594\n",
            "Epoch 500 train loss: 0.21511827409267426 test loss: 0.23798896372318268\n",
            "Epoch 500 train loss: 0.18430262804031372 test loss: 0.17985320091247559\n",
            "Epoch 500 train loss: 1.1407428979873657 test loss: 1.3702753782272339\n",
            "Epoch 500 train loss: 0.09126035869121552 test loss: 0.10819142311811447\n",
            "Epoch 500 train loss: 0.36422744393348694 test loss: 0.3545897901058197\n",
            "Epoch 500 train loss: 0.226938396692276 test loss: 0.23765119910240173\n",
            "Epoch 500 train loss: 0.054387424141168594 test loss: 0.056887757033109665\n",
            "Epoch 500 train loss: 0.05917109176516533 test loss: 0.06531725078821182\n",
            "Epoch 500 train loss: 0.11488643288612366 test loss: 0.11270052194595337\n",
            "Epoch 500 train loss: 0.043033868074417114 test loss: 0.04270986467599869\n",
            "Epoch 500 train loss: 0.04491303488612175 test loss: 0.04669841378927231\n",
            "Epoch 500 train loss: 0.11818490922451019 test loss: 0.12313177436590195\n",
            "Epoch 500 train loss: 0.10916132479906082 test loss: 0.10421347618103027\n",
            "Epoch 500 train loss: 0.04964565485715866 test loss: 0.0457427054643631\n",
            "Epoch 500 train loss: 0.0562274195253849 test loss: 0.05540541186928749\n",
            "Epoch 500 train loss: 0.05742068588733673 test loss: 0.05724921077489853\n",
            "Epoch 500 train loss: 0.4324153661727905 test loss: 0.42507028579711914\n",
            "Saving model \n",
            "\n",
            "Done\n",
            "Epoch 510 train loss: 0.3340187668800354 test loss: 0.3312315344810486\n",
            "Epoch 510 train loss: 0.033573344349861145 test loss: 0.03652074933052063\n",
            "Epoch 510 train loss: 0.36094704270362854 test loss: 0.35039860010147095\n",
            "Epoch 510 train loss: 0.015235740691423416 test loss: 0.017288945615291595\n",
            "Epoch 510 train loss: 0.6976572871208191 test loss: 0.6894988417625427\n",
            "Epoch 510 train loss: 0.1297624409198761 test loss: 0.12326781451702118\n",
            "Epoch 510 train loss: 0.12213025242090225 test loss: 0.12833747267723083\n",
            "Epoch 510 train loss: 0.2984791696071625 test loss: 0.3034522235393524\n",
            "Epoch 510 train loss: 0.24468345940113068 test loss: 0.26789480447769165\n",
            "Epoch 510 train loss: 0.05739959329366684 test loss: 0.06071827933192253\n",
            "Epoch 510 train loss: 0.10732907801866531 test loss: 0.10979574173688889\n",
            "Epoch 510 train loss: 0.10118655860424042 test loss: 0.10200139880180359\n",
            "Epoch 510 train loss: 0.36202263832092285 test loss: 0.3623689115047455\n",
            "Epoch 510 train loss: 0.04639972001314163 test loss: 0.04973942041397095\n",
            "Epoch 510 train loss: 0.2196425348520279 test loss: 0.244989275932312\n",
            "Epoch 510 train loss: 0.0496157743036747 test loss: 0.05611255019903183\n",
            "Epoch 510 train loss: 0.2127983123064041 test loss: 0.2295243889093399\n",
            "Epoch 510 train loss: 0.2473287433385849 test loss: 0.24579551815986633\n",
            "Epoch 510 train loss: 0.1340792030096054 test loss: 0.1289057731628418\n",
            "Epoch 510 train loss: 0.13306047022342682 test loss: 0.12640555202960968\n",
            "Epoch 510 train loss: 0.14320553839206696 test loss: 0.1408635973930359\n",
            "Epoch 510 train loss: 0.3808039724826813 test loss: 0.37969306111335754\n",
            "Epoch 510 train loss: 0.14077481627464294 test loss: 0.1321200579404831\n",
            "Epoch 510 train loss: 0.3180176019668579 test loss: 0.31505659222602844\n",
            "Epoch 510 train loss: 0.08301951736211777 test loss: 0.08341190963983536\n",
            "Epoch 510 train loss: 0.18988105654716492 test loss: 0.19708915054798126\n",
            "Epoch 510 train loss: 0.10873383283615112 test loss: 0.09869975596666336\n",
            "Epoch 510 train loss: 0.07025390863418579 test loss: 0.07194387912750244\n",
            "Epoch 510 train loss: 0.20790427923202515 test loss: 0.21826493740081787\n",
            "Epoch 510 train loss: 0.18277288973331451 test loss: 0.18950006365776062\n",
            "Epoch 510 train loss: 0.2212817221879959 test loss: 0.21853342652320862\n",
            "Epoch 510 train loss: 0.10547223687171936 test loss: 0.11404907703399658\n",
            "Epoch 510 train loss: 0.07591167092323303 test loss: 0.07678178697824478\n",
            "Epoch 510 train loss: 0.23150062561035156 test loss: 0.2771132290363312\n",
            "Epoch 510 train loss: 0.15709589421749115 test loss: 0.1542404592037201\n",
            "Epoch 510 train loss: 0.13809408247470856 test loss: 0.10758980363607407\n",
            "Epoch 510 train loss: 0.16741636395454407 test loss: 0.22474989295005798\n",
            "Epoch 510 train loss: 0.22539111971855164 test loss: 0.22653287649154663\n",
            "Epoch 510 train loss: 0.2324441373348236 test loss: 0.237010657787323\n",
            "Epoch 510 train loss: 0.04820213466882706 test loss: 0.04854762181639671\n",
            "Epoch 510 train loss: 0.05161306634545326 test loss: 0.05130407214164734\n",
            "Epoch 510 train loss: 0.08488475531339645 test loss: 0.08612756431102753\n",
            "Epoch 510 train loss: 0.06864234060049057 test loss: 0.06194908916950226\n",
            "Epoch 510 train loss: 0.03658854588866234 test loss: 0.03389488160610199\n",
            "Epoch 510 train loss: 0.1299753040075302 test loss: 0.12238559126853943\n",
            "Epoch 510 train loss: 0.07649415731430054 test loss: 0.07611457258462906\n",
            "Epoch 510 train loss: 0.068100206553936 test loss: 0.06781969219446182\n",
            "Epoch 510 train loss: 0.050802912563085556 test loss: 0.04828779026865959\n",
            "Epoch 510 train loss: 0.04647244140505791 test loss: 0.04678431898355484\n",
            "Epoch 510 train loss: 0.43505242466926575 test loss: 0.42927566170692444\n",
            "Epoch 520 train loss: 0.27775081992149353 test loss: 0.2719007432460785\n",
            "Epoch 520 train loss: 0.07106514275074005 test loss: 0.06398798525333405\n",
            "Epoch 520 train loss: 1.0331743955612183 test loss: 1.1033082008361816\n",
            "Epoch 520 train loss: 0.09890002757310867 test loss: 0.10197817534208298\n",
            "Epoch 520 train loss: 0.5880426168441772 test loss: 0.5894702672958374\n",
            "Epoch 520 train loss: 0.14689217507839203 test loss: 0.14463691413402557\n",
            "Epoch 520 train loss: 0.17566803097724915 test loss: 0.17191508412361145\n",
            "Epoch 520 train loss: 0.5483784675598145 test loss: 0.5533044934272766\n",
            "Epoch 520 train loss: 0.22229425609111786 test loss: 0.23220719397068024\n",
            "Epoch 520 train loss: 0.14224287867546082 test loss: 0.1440623253583908\n",
            "Epoch 520 train loss: 0.2310151755809784 test loss: 0.23456494510173798\n",
            "Epoch 520 train loss: 0.1820612996816635 test loss: 0.17697367072105408\n",
            "Epoch 520 train loss: 1.167892336845398 test loss: 1.2891305685043335\n",
            "Epoch 520 train loss: 0.3283928334712982 test loss: 0.3139721751213074\n",
            "Epoch 520 train loss: 0.15081840753555298 test loss: 0.170429989695549\n",
            "Epoch 520 train loss: 0.0566844567656517 test loss: 0.06247127428650856\n",
            "Epoch 520 train loss: 0.3513890504837036 test loss: 0.3433195948600769\n",
            "Epoch 520 train loss: 0.20328010618686676 test loss: 0.2073264867067337\n",
            "Epoch 520 train loss: 0.9443591833114624 test loss: 0.9290809035301208\n",
            "Epoch 520 train loss: 0.21461933851242065 test loss: 0.20854130387306213\n",
            "Epoch 520 train loss: 0.5364377498626709 test loss: 0.5283073782920837\n",
            "Epoch 520 train loss: 0.5197886228561401 test loss: 0.5142693519592285\n",
            "Epoch 520 train loss: 0.6268535852432251 test loss: 0.6513382792472839\n",
            "Epoch 520 train loss: 0.5147854089736938 test loss: 0.508861243724823\n",
            "Epoch 520 train loss: 0.29657989740371704 test loss: 0.2971978485584259\n",
            "Epoch 520 train loss: 0.30129316449165344 test loss: 0.29605624079704285\n",
            "Epoch 520 train loss: 0.22158850729465485 test loss: 0.22661671042442322\n",
            "Epoch 520 train loss: 0.3043140470981598 test loss: 0.3134838938713074\n",
            "Epoch 520 train loss: 0.2072637379169464 test loss: 0.18756452202796936\n",
            "Epoch 520 train loss: 0.3654213547706604 test loss: 0.36335229873657227\n",
            "Epoch 520 train loss: 0.28173890709877014 test loss: 0.2835487127304077\n",
            "Epoch 520 train loss: 0.9977060556411743 test loss: 0.9913433194160461\n",
            "Epoch 520 train loss: 0.24186201393604279 test loss: 0.24389417469501495\n",
            "Epoch 520 train loss: 0.39431899785995483 test loss: 0.4429638385772705\n",
            "Epoch 520 train loss: 0.4120246171951294 test loss: 0.40189194679260254\n",
            "Epoch 520 train loss: 0.11721791326999664 test loss: 0.0879410058259964\n",
            "Epoch 520 train loss: 0.27798840403556824 test loss: 0.2801876366138458\n",
            "Epoch 520 train loss: 0.5119975209236145 test loss: 0.4985966384410858\n",
            "Epoch 520 train loss: 0.2757832705974579 test loss: 0.2749403417110443\n",
            "Epoch 520 train loss: 0.11023242771625519 test loss: 0.11030366271734238\n",
            "Epoch 520 train loss: 0.20823335647583008 test loss: 0.2013794332742691\n",
            "Epoch 520 train loss: 0.1696547120809555 test loss: 0.1605817973613739\n",
            "Epoch 520 train loss: 0.14185740053653717 test loss: 0.13470794260501862\n",
            "Epoch 520 train loss: 0.2984776198863983 test loss: 0.2971443831920624\n",
            "Epoch 520 train loss: 0.20620663464069366 test loss: 0.2125987559556961\n",
            "Epoch 520 train loss: 0.2385236918926239 test loss: 0.2376323789358139\n",
            "Epoch 520 train loss: 0.09875126928091049 test loss: 0.09714868664741516\n",
            "Epoch 520 train loss: 0.1040988564491272 test loss: 0.10688149183988571\n",
            "Epoch 520 train loss: 0.22657069563865662 test loss: 0.2082095742225647\n",
            "Epoch 520 train loss: 0.8197526931762695 test loss: 0.819157063961029\n",
            "Epoch 530 train loss: 0.1525953859090805 test loss: 0.150285542011261\n",
            "Epoch 530 train loss: 0.0421769954264164 test loss: 0.04249086603522301\n",
            "Epoch 530 train loss: 0.37891682982444763 test loss: 0.38566848635673523\n",
            "Epoch 530 train loss: 0.02013610117137432 test loss: 0.02754998579621315\n",
            "Epoch 530 train loss: 0.4823078215122223 test loss: 0.47434917092323303\n",
            "Epoch 530 train loss: 0.10302600264549255 test loss: 0.10403527319431305\n",
            "Epoch 530 train loss: 0.07103532552719116 test loss: 0.07182295620441437\n",
            "Epoch 530 train loss: 0.2599638104438782 test loss: 0.26620399951934814\n",
            "Epoch 530 train loss: 0.17627741396427155 test loss: 0.1706286370754242\n",
            "Epoch 530 train loss: 0.06012723967432976 test loss: 0.062399014830589294\n",
            "Epoch 530 train loss: 0.08037381619215012 test loss: 0.07975491136312485\n",
            "Epoch 530 train loss: 0.053924255073070526 test loss: 0.05169244483113289\n",
            "Epoch 530 train loss: 0.2699557840824127 test loss: 0.27148163318634033\n",
            "Epoch 530 train loss: 0.0373871773481369 test loss: 0.03885296359658241\n",
            "Epoch 530 train loss: 0.0886315330862999 test loss: 0.0872376337647438\n",
            "Epoch 530 train loss: 0.03311985358595848 test loss: 0.03601835295557976\n",
            "Epoch 530 train loss: 0.19846060872077942 test loss: 0.19553273916244507\n",
            "Epoch 530 train loss: 0.21157371997833252 test loss: 0.20982548594474792\n",
            "Epoch 530 train loss: 0.15688827633857727 test loss: 0.16527719795703888\n",
            "Epoch 530 train loss: 0.1130075603723526 test loss: 0.10953235626220703\n",
            "Epoch 530 train loss: 0.08925578743219376 test loss: 0.09267362207174301\n",
            "Epoch 530 train loss: 0.3494234085083008 test loss: 0.3575155735015869\n",
            "Epoch 530 train loss: 0.13803067803382874 test loss: 0.1334102898836136\n",
            "Epoch 530 train loss: 0.2536124289035797 test loss: 0.25127145648002625\n",
            "Epoch 530 train loss: 0.06591077893972397 test loss: 0.06439191848039627\n",
            "Epoch 530 train loss: 0.1993611752986908 test loss: 0.2020929753780365\n",
            "Epoch 530 train loss: 0.07556138187646866 test loss: 0.07852833718061447\n",
            "Epoch 530 train loss: 0.06274504959583282 test loss: 0.06293900310993195\n",
            "Epoch 530 train loss: 0.09593796730041504 test loss: 0.106537826359272\n",
            "Epoch 530 train loss: 0.06388451159000397 test loss: 0.06343363225460052\n",
            "Epoch 530 train loss: 0.11916254460811615 test loss: 0.11571599543094635\n",
            "Epoch 530 train loss: 0.09663176536560059 test loss: 0.10351329296827316\n",
            "Epoch 530 train loss: 0.05273602157831192 test loss: 0.05509944260120392\n",
            "Epoch 530 train loss: 0.14989051222801208 test loss: 0.15051694214344025\n",
            "Epoch 530 train loss: 0.18208087980747223 test loss: 0.1879042685031891\n",
            "Epoch 530 train loss: 0.07295004278421402 test loss: 0.0659952312707901\n",
            "Epoch 530 train loss: 0.11422447115182877 test loss: 0.12076076865196228\n",
            "Epoch 530 train loss: 0.1703721433877945 test loss: 0.1771714985370636\n",
            "Epoch 530 train loss: 0.14535152912139893 test loss: 0.1434953361749649\n",
            "Epoch 530 train loss: 0.04416392743587494 test loss: 0.04374364763498306\n",
            "Epoch 530 train loss: 0.13550488650798798 test loss: 0.13542819023132324\n",
            "Epoch 530 train loss: 0.042274799197912216 test loss: 0.042078133672475815\n",
            "Epoch 530 train loss: 0.037585217505693436 test loss: 0.037731051445007324\n",
            "Epoch 530 train loss: 0.0199282206594944 test loss: 0.041717611253261566\n",
            "Epoch 530 train loss: 0.07213924080133438 test loss: 0.07900281250476837\n",
            "Epoch 530 train loss: 0.07147465646266937 test loss: 0.06525679677724838\n",
            "Epoch 530 train loss: 0.04673278331756592 test loss: 0.041256967931985855\n",
            "Epoch 530 train loss: 0.048234906047582626 test loss: 0.04746817424893379\n",
            "Epoch 530 train loss: 0.12742550671100616 test loss: 0.12570498883724213\n",
            "Epoch 530 train loss: 0.4183659851551056 test loss: 0.4210492670536041\n",
            "Epoch 540 train loss: 0.319909930229187 test loss: 0.3186728358268738\n",
            "Epoch 540 train loss: 0.026086561381816864 test loss: 0.026640042662620544\n",
            "Epoch 540 train loss: 0.34122249484062195 test loss: 0.34713980555534363\n",
            "Epoch 540 train loss: 0.02075027860701084 test loss: 0.024337738752365112\n",
            "Epoch 540 train loss: 0.32604968547821045 test loss: 0.32033810019493103\n",
            "Epoch 540 train loss: 0.1370062232017517 test loss: 0.13294561207294464\n",
            "Epoch 540 train loss: 0.09104415029287338 test loss: 0.08564548939466476\n",
            "Epoch 540 train loss: 0.24561277031898499 test loss: 0.24453963339328766\n",
            "Epoch 540 train loss: 0.19813455641269684 test loss: 0.19610358774662018\n",
            "Epoch 540 train loss: 0.03690638020634651 test loss: 0.03786593675613403\n",
            "Epoch 540 train loss: 0.07084470242261887 test loss: 0.07450580596923828\n",
            "Epoch 540 train loss: 0.06484244018793106 test loss: 0.06431363523006439\n",
            "Epoch 540 train loss: 0.31346628069877625 test loss: 0.31452351808547974\n",
            "Epoch 540 train loss: 0.04247787594795227 test loss: 0.042710013687610626\n",
            "Epoch 540 train loss: 0.07451824098825455 test loss: 0.08319084346294403\n",
            "Epoch 540 train loss: 0.107540562748909 test loss: 0.10459236800670624\n",
            "Epoch 540 train loss: 0.1875341236591339 test loss: 0.19566790759563446\n",
            "Epoch 540 train loss: 0.19449670612812042 test loss: 0.19682294130325317\n",
            "Epoch 540 train loss: 0.10782932490110397 test loss: 0.10241411626338959\n",
            "Epoch 540 train loss: 0.09269513189792633 test loss: 0.0918644517660141\n",
            "Epoch 540 train loss: 0.0839286521077156 test loss: 0.08372793346643448\n",
            "Epoch 540 train loss: 0.29487794637680054 test loss: 0.28765490651130676\n",
            "Epoch 540 train loss: 0.10523054003715515 test loss: 0.10702872276306152\n",
            "Epoch 540 train loss: 0.23162655532360077 test loss: 0.2332797646522522\n",
            "Epoch 540 train loss: 0.11421215534210205 test loss: 0.10400170832872391\n",
            "Epoch 540 train loss: 0.0941692665219307 test loss: 0.09231141954660416\n",
            "Epoch 540 train loss: 0.09994987398386002 test loss: 0.09476136416196823\n",
            "Epoch 540 train loss: 0.0611693449318409 test loss: 0.06404706090688705\n",
            "Epoch 540 train loss: 0.17457818984985352 test loss: 0.18938031792640686\n",
            "Epoch 540 train loss: 0.06123288348317146 test loss: 0.06149839982390404\n",
            "Epoch 540 train loss: 0.11562768369913101 test loss: 0.1034349575638771\n",
            "Epoch 540 train loss: 0.10741446167230606 test loss: 0.09895632416009903\n",
            "Epoch 540 train loss: 0.060985926538705826 test loss: 0.060331836342811584\n",
            "Epoch 540 train loss: 0.16821186244487762 test loss: 0.17372636497020721\n",
            "Epoch 540 train loss: 0.22413617372512817 test loss: 0.2338641732931137\n",
            "Epoch 540 train loss: 0.07429363578557968 test loss: 0.04940938204526901\n",
            "Epoch 540 train loss: 0.11605200916528702 test loss: 0.12029798328876495\n",
            "Epoch 540 train loss: 0.18526582419872284 test loss: 0.18297570943832397\n",
            "Epoch 540 train loss: 0.15386782586574554 test loss: 0.15681198239326477\n",
            "Epoch 540 train loss: 0.05020852014422417 test loss: 0.052018288522958755\n",
            "Epoch 540 train loss: 0.06701524555683136 test loss: 0.06328033655881882\n",
            "Epoch 540 train loss: 0.05364030599594116 test loss: 0.0546240396797657\n",
            "Epoch 540 train loss: 0.047544073313474655 test loss: 0.049326684325933456\n",
            "Epoch 540 train loss: 0.026678921654820442 test loss: 0.027331845834851265\n",
            "Epoch 540 train loss: 0.09798822551965714 test loss: 0.0969369113445282\n",
            "Epoch 540 train loss: 0.0477486290037632 test loss: 0.051367390900850296\n",
            "Epoch 540 train loss: 0.038187943398952484 test loss: 0.037998851388692856\n",
            "Epoch 540 train loss: 0.06114770844578743 test loss: 0.05523315817117691\n",
            "Epoch 540 train loss: 0.04110049456357956 test loss: 0.04027115926146507\n",
            "Epoch 540 train loss: 0.3597674071788788 test loss: 0.36071327328681946\n",
            "Epoch 550 train loss: 0.21090610325336456 test loss: 0.2028403878211975\n",
            "Epoch 550 train loss: 0.03399001434445381 test loss: 0.03616867959499359\n",
            "Epoch 550 train loss: 0.35545048117637634 test loss: 0.34820032119750977\n",
            "Epoch 550 train loss: 0.02764243073761463 test loss: 0.03071543574333191\n",
            "Epoch 550 train loss: 0.30247804522514343 test loss: 0.3014705777168274\n",
            "Epoch 550 train loss: 0.1555483192205429 test loss: 0.16156339645385742\n",
            "Epoch 550 train loss: 0.07996959239244461 test loss: 0.08057936280965805\n",
            "Epoch 550 train loss: 0.1373397409915924 test loss: 0.1419067233800888\n",
            "Epoch 550 train loss: 0.11092186719179153 test loss: 0.10463392734527588\n",
            "Epoch 550 train loss: 0.10151601582765579 test loss: 0.10714319348335266\n",
            "Epoch 550 train loss: 0.06713885068893433 test loss: 0.06682790815830231\n",
            "Epoch 550 train loss: 0.7712560296058655 test loss: 0.5845425128936768\n",
            "Epoch 550 train loss: 0.2343156784772873 test loss: 0.2389456331729889\n",
            "Epoch 550 train loss: 0.060383230447769165 test loss: 0.05983375012874603\n",
            "Epoch 550 train loss: 0.1711011677980423 test loss: 0.18644773960113525\n",
            "Epoch 550 train loss: 0.09695058315992355 test loss: 0.10153912752866745\n",
            "Epoch 550 train loss: 0.21591565012931824 test loss: 0.21451188623905182\n",
            "Epoch 550 train loss: 0.19996638596057892 test loss: 0.20129886269569397\n",
            "Epoch 550 train loss: 0.13338714838027954 test loss: 0.12069268524646759\n",
            "Epoch 550 train loss: 0.08825594186782837 test loss: 0.09127281606197357\n",
            "Epoch 550 train loss: 0.12569913268089294 test loss: 0.12209399789571762\n",
            "Epoch 550 train loss: 0.30375438928604126 test loss: 0.3057304918766022\n",
            "Epoch 550 train loss: 0.11294267326593399 test loss: 0.11030400544404984\n",
            "Epoch 550 train loss: 0.32166609168052673 test loss: 0.33099067211151123\n",
            "Epoch 550 train loss: 0.18950684368610382 test loss: 0.18873997032642365\n",
            "Epoch 550 train loss: 0.11599414795637131 test loss: 0.11429141461849213\n",
            "Epoch 550 train loss: 0.1567753702402115 test loss: 0.15442080795764923\n",
            "Epoch 550 train loss: 0.0598728284239769 test loss: 0.058465320616960526\n",
            "Epoch 550 train loss: 0.270620733499527 test loss: 0.34151217341423035\n",
            "Epoch 550 train loss: 0.0933346375823021 test loss: 0.0962354987859726\n",
            "Epoch 550 train loss: 0.1615910828113556 test loss: 0.15193839371204376\n",
            "Epoch 550 train loss: 0.10564596951007843 test loss: 0.10287681967020035\n",
            "Epoch 550 train loss: 0.07546360790729523 test loss: 0.07800915837287903\n",
            "Epoch 550 train loss: 0.14399191737174988 test loss: 0.13816705346107483\n",
            "Epoch 550 train loss: 0.21504676342010498 test loss: 0.22622238099575043\n",
            "Epoch 550 train loss: 0.16223573684692383 test loss: 0.1673579216003418\n",
            "Epoch 550 train loss: 0.15198786556720734 test loss: 0.14986906945705414\n",
            "Epoch 550 train loss: 0.6208440661430359 test loss: 0.6478208303451538\n",
            "Epoch 550 train loss: 0.15953782200813293 test loss: 0.1519811451435089\n",
            "Epoch 550 train loss: 0.0642155110836029 test loss: 0.06426435708999634\n",
            "Epoch 550 train loss: 0.08236411213874817 test loss: 0.08223537355661392\n",
            "Epoch 550 train loss: 0.08716528862714767 test loss: 0.08793552219867706\n",
            "Epoch 550 train loss: 0.03917759656906128 test loss: 0.04051540419459343\n",
            "Epoch 550 train loss: 0.04506427049636841 test loss: 0.05911872163414955\n",
            "Epoch 550 train loss: 0.13350048661231995 test loss: 0.13271008431911469\n",
            "Epoch 550 train loss: 0.1325460821390152 test loss: 0.1346156746149063\n",
            "Epoch 550 train loss: 0.044516388326883316 test loss: 0.05198776349425316\n",
            "Epoch 550 train loss: 0.0716886818408966 test loss: 0.07023033499717712\n",
            "Epoch 550 train loss: 0.09258785098791122 test loss: 0.08899356424808502\n",
            "Epoch 550 train loss: 0.2594355046749115 test loss: 0.2649189531803131\n",
            "Epoch 560 train loss: 0.21007609367370605 test loss: 0.20690223574638367\n",
            "Epoch 560 train loss: 0.03256821632385254 test loss: 0.03159159794449806\n",
            "Epoch 560 train loss: 0.3167978525161743 test loss: 0.32210785150527954\n",
            "Epoch 560 train loss: 0.02389337681233883 test loss: 0.07143373042345047\n",
            "Epoch 560 train loss: 0.31989985704421997 test loss: 0.32000643014907837\n",
            "Epoch 560 train loss: 0.1362333446741104 test loss: 0.14535728096961975\n",
            "Epoch 560 train loss: 0.0638432726264 test loss: 0.0600232370197773\n",
            "Epoch 560 train loss: 0.15361474454402924 test loss: 0.1494801640510559\n",
            "Epoch 560 train loss: 0.2648056149482727 test loss: 0.254829466342926\n",
            "Epoch 560 train loss: 0.034614622592926025 test loss: 0.03526720032095909\n",
            "Epoch 560 train loss: 0.0929042249917984 test loss: 0.08864124864339828\n",
            "Epoch 560 train loss: 0.1640176922082901 test loss: 0.15032923221588135\n",
            "Epoch 560 train loss: 0.2838124632835388 test loss: 0.28086236119270325\n",
            "Epoch 560 train loss: 0.10199558734893799 test loss: 0.10369003564119339\n",
            "Epoch 560 train loss: 0.20352350175380707 test loss: 0.21894803643226624\n",
            "Epoch 560 train loss: 0.06128854677081108 test loss: 0.05908004194498062\n",
            "Epoch 560 train loss: 0.1498735249042511 test loss: 0.14488191902637482\n",
            "Epoch 560 train loss: 0.19504165649414062 test loss: 0.1922178417444229\n",
            "Epoch 560 train loss: 0.08921167254447937 test loss: 0.08300866186618805\n",
            "Epoch 560 train loss: 0.09046654403209686 test loss: 0.08964772522449493\n",
            "Epoch 560 train loss: 0.0986606553196907 test loss: 0.09982334077358246\n",
            "Epoch 560 train loss: 0.37026870250701904 test loss: 0.3680424988269806\n",
            "Epoch 560 train loss: 0.10650888830423355 test loss: 0.10961040109395981\n",
            "Epoch 560 train loss: 0.17427606880664825 test loss: 0.17327362298965454\n",
            "Epoch 560 train loss: 0.10191237926483154 test loss: 0.10348539799451828\n",
            "Epoch 560 train loss: 0.12471293658018112 test loss: 0.12575723230838776\n",
            "Epoch 560 train loss: 0.06706065684556961 test loss: 0.062215205281972885\n",
            "Epoch 560 train loss: 0.08194062113761902 test loss: 0.07608360797166824\n",
            "Epoch 560 train loss: 0.25213557481765747 test loss: 0.3297160863876343\n",
            "Epoch 560 train loss: 0.04145938903093338 test loss: 0.040420204401016235\n",
            "Epoch 560 train loss: 0.0972210094332695 test loss: 0.10042461007833481\n",
            "Epoch 560 train loss: 0.09901177883148193 test loss: 0.10439642518758774\n",
            "Epoch 560 train loss: 0.04852203652262688 test loss: 0.05092518404126167\n",
            "Epoch 560 train loss: 0.10635776817798615 test loss: 0.10830863565206528\n",
            "Epoch 560 train loss: 0.27704697847366333 test loss: 0.27320191264152527\n",
            "Epoch 560 train loss: 0.23100635409355164 test loss: 0.22883735597133636\n",
            "Epoch 560 train loss: 0.09649042785167694 test loss: 0.09887923300266266\n",
            "Epoch 560 train loss: 0.1893184334039688 test loss: 0.18247288465499878\n",
            "Epoch 560 train loss: 0.1774483472108841 test loss: 0.18266896903514862\n",
            "Epoch 560 train loss: 0.04692816361784935 test loss: 0.05084409937262535\n",
            "Epoch 560 train loss: 0.036903444677591324 test loss: 0.03538508340716362\n",
            "Epoch 560 train loss: 0.09387781471014023 test loss: 0.09805355966091156\n",
            "Epoch 560 train loss: 0.04775311425328255 test loss: 0.04356899484992027\n",
            "Epoch 560 train loss: 0.0517883263528347 test loss: 0.11421039700508118\n",
            "Epoch 560 train loss: 0.053815629333257675 test loss: 0.05669163167476654\n",
            "Epoch 560 train loss: 0.09408403933048248 test loss: 0.0980416089296341\n",
            "Epoch 560 train loss: 0.05536163970828056 test loss: 0.053728725761175156\n",
            "Epoch 560 train loss: 0.04225916042923927 test loss: 0.046173032373189926\n",
            "Epoch 560 train loss: 0.04447906091809273 test loss: 0.04257940128445625\n",
            "Epoch 560 train loss: 0.2663695812225342 test loss: 0.27245190739631653\n",
            "Epoch 570 train loss: 0.10504176467657089 test loss: 0.1083136647939682\n",
            "Epoch 570 train loss: 0.023664703592658043 test loss: 0.023829324170947075\n",
            "Epoch 570 train loss: 0.30873653292655945 test loss: 0.30442920327186584\n",
            "Epoch 570 train loss: 0.021470552310347557 test loss: 0.02508767507970333\n",
            "Epoch 570 train loss: 0.1601286083459854 test loss: 0.1589600294828415\n",
            "Epoch 570 train loss: 0.07888183742761612 test loss: 0.08073553442955017\n",
            "Epoch 570 train loss: 0.06358333677053452 test loss: 0.06421325355768204\n",
            "Epoch 570 train loss: 0.1549561768770218 test loss: 0.15567050874233246\n",
            "Epoch 570 train loss: 0.24279142916202545 test loss: 0.23797135055065155\n",
            "Epoch 570 train loss: 0.056489359587430954 test loss: 0.0628235787153244\n",
            "Epoch 570 train loss: 0.07054898142814636 test loss: 0.06946425139904022\n",
            "Epoch 570 train loss: 0.06434988975524902 test loss: 0.05907712131738663\n",
            "Epoch 570 train loss: 0.23179028928279877 test loss: 0.22535976767539978\n",
            "Epoch 570 train loss: 0.05388341099023819 test loss: 0.052094630897045135\n",
            "Epoch 570 train loss: 0.14626021683216095 test loss: 0.17416971921920776\n",
            "Epoch 570 train loss: 0.05448933318257332 test loss: 0.05949173867702484\n",
            "Epoch 570 train loss: 0.13662613928318024 test loss: 0.13846002519130707\n",
            "Epoch 570 train loss: 0.1728125959634781 test loss: 0.17173753678798676\n",
            "Epoch 570 train loss: 0.08063066005706787 test loss: 0.08339834213256836\n",
            "Epoch 570 train loss: 0.07315514236688614 test loss: 0.07415452599525452\n",
            "Epoch 570 train loss: 0.0706067681312561 test loss: 0.06770920008420944\n",
            "Epoch 570 train loss: 0.2779199779033661 test loss: 0.27500656247138977\n",
            "Epoch 570 train loss: 0.07551493495702744 test loss: 0.07383476942777634\n",
            "Epoch 570 train loss: 0.146059051156044 test loss: 0.15084956586360931\n",
            "Epoch 570 train loss: 0.031514331698417664 test loss: 0.03235187381505966\n",
            "Epoch 570 train loss: 0.09868711978197098 test loss: 0.10233661532402039\n",
            "Epoch 570 train loss: 0.06970157474279404 test loss: 0.06967265903949738\n",
            "Epoch 570 train loss: 0.06303423643112183 test loss: 0.0587800107896328\n",
            "Epoch 570 train loss: 0.23727206885814667 test loss: 0.31061050295829773\n",
            "Epoch 570 train loss: 0.05537371337413788 test loss: 0.05724484473466873\n",
            "Epoch 570 train loss: 0.0795898362994194 test loss: 0.08204086124897003\n",
            "Epoch 570 train loss: 0.0767025500535965 test loss: 0.07445013523101807\n",
            "Epoch 570 train loss: 0.05471046268939972 test loss: 0.054147228598594666\n",
            "Epoch 570 train loss: 0.07850310951471329 test loss: 0.07790286093950272\n",
            "Epoch 570 train loss: 0.20509961247444153 test loss: 0.20396418869495392\n",
            "Epoch 570 train loss: 0.18040978908538818 test loss: 0.17739681899547577\n",
            "Epoch 570 train loss: 0.09089017659425735 test loss: 0.0951245129108429\n",
            "Epoch 570 train loss: 0.18651950359344482 test loss: 0.1829887181520462\n",
            "Epoch 570 train loss: 0.1160440593957901 test loss: 0.1142643466591835\n",
            "Epoch 570 train loss: 0.04568333923816681 test loss: 0.05033407360315323\n",
            "Epoch 570 train loss: 0.04081796482205391 test loss: 0.04096635803580284\n",
            "Epoch 570 train loss: 0.06077933683991432 test loss: 0.06446538865566254\n",
            "Epoch 570 train loss: 0.038381338119506836 test loss: 0.03546411171555519\n",
            "Epoch 570 train loss: 0.02411186695098877 test loss: 0.02901436761021614\n",
            "Epoch 570 train loss: 0.08841665089130402 test loss: 0.09358946979045868\n",
            "Epoch 570 train loss: 0.04241413250565529 test loss: 0.04261540248990059\n",
            "Epoch 570 train loss: 0.040174566209316254 test loss: 0.03623414412140846\n",
            "Epoch 570 train loss: 0.042753640562295914 test loss: 0.04174800589680672\n",
            "Epoch 570 train loss: 0.030312350019812584 test loss: 0.03050331585109234\n",
            "Epoch 570 train loss: 0.20586687326431274 test loss: 0.2050504833459854\n",
            "Epoch 580 train loss: 0.2981571853160858 test loss: 0.2974555790424347\n",
            "Epoch 580 train loss: 0.035236869007349014 test loss: 0.03181803226470947\n",
            "Epoch 580 train loss: 0.3869028389453888 test loss: 0.3868884742259979\n",
            "Epoch 580 train loss: 0.028399640694260597 test loss: 0.058727581053972244\n",
            "Epoch 580 train loss: 0.1771940439939499 test loss: 0.18565626442432404\n",
            "Epoch 580 train loss: 0.10628360509872437 test loss: 0.11010193079710007\n",
            "Epoch 580 train loss: 0.08798962831497192 test loss: 0.0892258957028389\n",
            "Epoch 580 train loss: 0.14980287849903107 test loss: 0.14733867347240448\n",
            "Epoch 580 train loss: 0.2629041373729706 test loss: 0.2435629665851593\n",
            "Epoch 580 train loss: 0.08861547708511353 test loss: 0.09355780482292175\n",
            "Epoch 580 train loss: 0.10205990821123123 test loss: 0.0917290523648262\n",
            "Epoch 580 train loss: 0.06806366890668869 test loss: 0.06458788365125656\n",
            "Epoch 580 train loss: 0.36724522709846497 test loss: 0.37842023372650146\n",
            "Epoch 580 train loss: 0.06602997332811356 test loss: 0.06332803517580032\n",
            "Epoch 580 train loss: 0.09623601287603378 test loss: 0.13751260936260223\n",
            "Epoch 580 train loss: 0.03229933977127075 test loss: 0.030949130654335022\n",
            "Epoch 580 train loss: 0.23732544481754303 test loss: 0.23502741754055023\n",
            "Epoch 580 train loss: 0.18167878687381744 test loss: 0.17948874831199646\n",
            "Epoch 580 train loss: 0.09169355779886246 test loss: 0.09408149123191833\n",
            "Epoch 580 train loss: 0.11672966927289963 test loss: 0.11285290867090225\n",
            "Epoch 580 train loss: 0.06252127885818481 test loss: 0.05924889072775841\n",
            "Epoch 580 train loss: 0.4405635595321655 test loss: 0.46166419982910156\n",
            "Epoch 580 train loss: 0.10098933428525925 test loss: 0.09215371310710907\n",
            "Epoch 580 train loss: 0.15148331224918365 test loss: 0.1508105993270874\n",
            "Epoch 580 train loss: 0.1899128556251526 test loss: 0.19676335155963898\n",
            "Epoch 580 train loss: 0.08774027228355408 test loss: 0.09684447199106216\n",
            "Epoch 580 train loss: 0.07663442939519882 test loss: 0.0703524574637413\n",
            "Epoch 580 train loss: 0.07030951231718063 test loss: 0.07878109067678452\n",
            "Epoch 580 train loss: 0.17281675338745117 test loss: 0.2629997730255127\n",
            "Epoch 580 train loss: 0.06505190581083298 test loss: 0.07432174682617188\n",
            "Epoch 580 train loss: 0.13682329654693604 test loss: 0.13848023116588593\n",
            "Epoch 580 train loss: 0.3491147756576538 test loss: 0.3384580910205841\n",
            "Epoch 580 train loss: 0.11510372906923294 test loss: 0.11641961336135864\n",
            "Epoch 580 train loss: 0.35534200072288513 test loss: 0.3699759244918823\n",
            "Epoch 580 train loss: 0.22913187742233276 test loss: 0.22549818456172943\n",
            "Epoch 580 train loss: 0.271740198135376 test loss: 0.2911294400691986\n",
            "Epoch 580 train loss: 0.13450711965560913 test loss: 0.1755695790052414\n",
            "Epoch 580 train loss: 0.14750225841999054 test loss: 0.14299412071704865\n",
            "Epoch 580 train loss: 0.1329052448272705 test loss: 0.1393464356660843\n",
            "Epoch 580 train loss: 0.041752856224775314 test loss: 0.04121050983667374\n",
            "Epoch 580 train loss: 0.1578681766986847 test loss: 0.15636859834194183\n",
            "Epoch 580 train loss: 0.18364004790782928 test loss: 0.19753727316856384\n",
            "Epoch 580 train loss: 0.0636245608329773 test loss: 0.0633026584982872\n",
            "Epoch 580 train loss: 0.04718225449323654 test loss: 0.06681147962808609\n",
            "Epoch 580 train loss: 0.0975184366106987 test loss: 0.10175476223230362\n",
            "Epoch 580 train loss: 0.48107287287712097 test loss: 0.47601211071014404\n",
            "Epoch 580 train loss: 0.058169614523649216 test loss: 0.05713679641485214\n",
            "Epoch 580 train loss: 0.08951766788959503 test loss: 0.08441273868083954\n",
            "Epoch 580 train loss: 0.12137317657470703 test loss: 0.12879036366939545\n",
            "Epoch 580 train loss: 0.17069341242313385 test loss: 0.17517484724521637\n",
            "Epoch 590 train loss: 0.1259484887123108 test loss: 0.12525181472301483\n",
            "Epoch 590 train loss: 0.040734246373176575 test loss: 0.03958125039935112\n",
            "Epoch 590 train loss: 0.36996638774871826 test loss: 0.3850537836551666\n",
            "Epoch 590 train loss: 0.02882622927427292 test loss: 0.027748748660087585\n",
            "Epoch 590 train loss: 0.12405364215373993 test loss: 0.12279509752988815\n",
            "Epoch 590 train loss: 0.11724001169204712 test loss: 0.1217828243970871\n",
            "Epoch 590 train loss: 0.08030060678720474 test loss: 0.06663017719984055\n",
            "Epoch 590 train loss: 0.11944981664419174 test loss: 0.12019208073616028\n",
            "Epoch 590 train loss: 0.1539931744337082 test loss: 0.1487489938735962\n",
            "Epoch 590 train loss: 0.09690447151660919 test loss: 0.0955565869808197\n",
            "Epoch 590 train loss: 0.07066222280263901 test loss: 0.06985849142074585\n",
            "Epoch 590 train loss: 0.05546250566840172 test loss: 0.06041666865348816\n",
            "Epoch 590 train loss: 0.21677936613559723 test loss: 0.21805554628372192\n",
            "Epoch 590 train loss: 0.07281012088060379 test loss: 0.07268671691417694\n",
            "Epoch 590 train loss: 0.08598372340202332 test loss: 0.0989309623837471\n",
            "Epoch 590 train loss: 0.028819924220442772 test loss: 0.03304776921868324\n",
            "Epoch 590 train loss: 0.11832965910434723 test loss: 0.13037830591201782\n",
            "Epoch 590 train loss: 0.1816457211971283 test loss: 0.1880423128604889\n",
            "Epoch 590 train loss: 0.12072424590587616 test loss: 0.1054341197013855\n",
            "Epoch 590 train loss: 0.0691971629858017 test loss: 0.07297192513942719\n",
            "Epoch 590 train loss: 0.1066371351480484 test loss: 0.10625044256448746\n",
            "Epoch 590 train loss: 0.5012282729148865 test loss: 0.499070942401886\n",
            "Epoch 590 train loss: 0.14327655732631683 test loss: 0.134905606508255\n",
            "Epoch 590 train loss: 0.18248756229877472 test loss: 0.18724524974822998\n",
            "Epoch 590 train loss: 0.041308581829071045 test loss: 0.04128279164433479\n",
            "Epoch 590 train loss: 0.09608040004968643 test loss: 0.09841665625572205\n",
            "Epoch 590 train loss: 0.07404283434152603 test loss: 0.07690341025590897\n",
            "Epoch 590 train loss: 0.16704121232032776 test loss: 0.16384732723236084\n",
            "Epoch 590 train loss: 0.0888681411743164 test loss: 0.16998600959777832\n",
            "Epoch 590 train loss: 0.02935958467423916 test loss: 0.02782808057963848\n",
            "Epoch 590 train loss: 0.10135960578918457 test loss: 0.1071266382932663\n",
            "Epoch 590 train loss: 0.0848751962184906 test loss: 0.08384917676448822\n",
            "Epoch 590 train loss: 0.0744975134730339 test loss: 0.07207826524972916\n",
            "Epoch 590 train loss: 0.11354342848062515 test loss: 0.11431068927049637\n",
            "Epoch 590 train loss: 0.1852695643901825 test loss: 0.19921241700649261\n",
            "Epoch 590 train loss: 0.23369695246219635 test loss: 0.23904575407505035\n",
            "Epoch 590 train loss: 0.23095178604125977 test loss: 0.24462199211120605\n",
            "Epoch 590 train loss: 0.17949539422988892 test loss: 0.18282848596572876\n",
            "Epoch 590 train loss: 0.11997918039560318 test loss: 0.1270197480916977\n",
            "Epoch 590 train loss: 0.03622889891266823 test loss: 0.035817623138427734\n",
            "Epoch 590 train loss: 0.09272937476634979 test loss: 0.08565401285886765\n",
            "Epoch 590 train loss: 0.06677671521902084 test loss: 0.06379645317792892\n",
            "Epoch 590 train loss: 0.022008685395121574 test loss: 0.022029433399438858\n",
            "Epoch 590 train loss: 0.0237193051725626 test loss: 0.027380796149373055\n",
            "Epoch 590 train loss: 0.10353409498929977 test loss: 0.09953450411558151\n",
            "Epoch 590 train loss: 0.04459180682897568 test loss: 0.04695582017302513\n",
            "Epoch 590 train loss: 0.03829837962985039 test loss: 0.03867173567414284\n",
            "Epoch 590 train loss: 0.04505784437060356 test loss: 0.03959061577916145\n",
            "Epoch 590 train loss: 0.034418024122714996 test loss: 0.03417644649744034\n",
            "Epoch 590 train loss: 0.14989615976810455 test loss: 0.15334424376487732\n",
            "CPU times: user 6h 15min 58s, sys: 9min 25s, total: 6h 25min 24s\n",
            "Wall time: 6h 25min 47s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3KahlpCLy8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_save_name = 'ForcePredictor2.pt'\n",
        "# path = F\"/content/gdrive/My Drive/PhD/PhD/lstm/{model_save_name}\" \n",
        "# torch.save(model.state_dict(), path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JQbLodYJ40g",
        "colab_type": "code",
        "outputId": "142b42d5-56f4-43b2-e9e2-2018f9b7ebd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Saving model\")\n",
        "model_num = model_number()\n",
        "model_save_name = f'model_params{model_num}_vlast.pt'\n",
        "torch.save({\n",
        "   # 'epoch': epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss':loss,},\n",
        "    f\"/content/drive/My Drive/PhD/PhD/lstm/{model_save_name}\" \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "u9wQYHgS-zQJ",
        "colab_type": "code",
        "outputId": "2f6a06dd-0ded-4592-db7b-3b11f31e01d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(train_hist, label=\"Training loss\")\n",
        "plt.plot(test_hist, label=\"Test loss\")\n",
        "#plt.ylim((0, 5))\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnwajR7M-zQO",
        "colab_type": "text"
      },
      "source": [
        "RL Controller Predictor"
      ]
    }
  ]
}